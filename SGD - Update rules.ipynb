{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis distance to get adagrad??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer by layer - parameter dependence?\n",
    "\n",
    "State-of-the-art research with optimisation, do something funky with the learning rates on SGD and hope it converges faster/for less.\n",
    "\n",
    "What if we learn layer by layer? (how is this related to curriculum learning?) \n",
    "\n",
    "Could save computations by not having to calculate derivatives for some layers?\n",
    "* Initially, nope. You need to calculate all derivatives anyway... Backprop.\n",
    "* Later, yea. \n",
    "\n",
    "So are we talking about varying the learning rate across different layers.\n",
    "\n",
    "How should it vary? \n",
    "If we freeze layers then it would save computations. So we would want some sort of \n",
    "\n",
    "> Learn the easy stuff first and then build on that.\n",
    "\n",
    "When should the net think about retraining/checking its lower level parameters?\n",
    "Could make the assumption. I have learnt the basic filters/... only need to learn to compose them correctly?\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Should all layers be treated equally? Or should some recieve more training than others. I.e. the integral of learning rate over training steps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
