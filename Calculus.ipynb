{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, a quick review of this as I dont know as much as I thought I did... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite difference\n",
    "\n",
    "Can derive from Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total derivative\n",
    "\n",
    "\\begin{align*}\n",
    "df(h,x,\\theta) &= \\frac{\\partial f}{\\partial h} dh+  \\frac{\\partial f}{\\partial x}dx +  \\frac{\\partial f}{\\partial \\theta}d\\theta \\\\\n",
    "\\end{align*}\n",
    "\n",
    "* Does this make sense? Why sum? Need some pics/intuition.\n",
    "\n",
    "### Directional derivatives\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobians\n",
    "\n",
    "Does $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$. Proof!\n",
    "\n",
    "\n",
    "$H(f(x)) = J(\\nabla f(x))$\n",
    "but $J = \\nabla f(x)$ ? so therefore $H(f(x)) = J(J(f(x)) = \\nabla(\\nabla(f(x))$\n",
    "\n",
    "\n",
    "\n",
    "### Matrix - scalar\n",
    "$$ \n",
    "\\begin{align}\n",
    "f(X) :&= \\mathbb{R}^{m\\times n} \\rightarrow \\mathbb{R} \\\\\n",
    "f'(X) :&= \\mathbb{R}^{m\\times n} \\rightarrow \\mathbb{R}^{m \\times n} \\tag{Jacobian.} \\\\\n",
    "f''(X) :&= \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{(m\\times n)\\times (m\\times n)} \\tag{Hessian} \\\\\n",
    "&= \\mathbb{R}^{mn\\times mn} \\tag{Flattened into 2d} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "or is it $(mxm)x(nxn)$??\n",
    "\n",
    "Jacobian/Hessian should be \n",
    "\n",
    "\n",
    "> _Desiderata: The derivative of a matrix–matrix function should be a matrix, so that we can directly use it to compute the Hessian._\n",
    "\n",
    "### Matrix - matrix\n",
    "\n",
    "\n",
    "$$\\frac {\\partial F}{\\partial X} := \\frac{\\partial vec(F^T)}{\\partial vec^T(X^T)}$$\n",
    "\n",
    "> _Desiderata: The derivative of a matrix-matrix function should be a matrix, so that a convenient chain-rule can be established._\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Note that the matrix–matrix derivative of a scalar–matrix function is not the same as the scalar–matrix derivative.\n",
    "\n",
    "$\\frac {\\partial F}{\\partial X} = \\frac{\\partial vec(F^T)}{\\partial vec^T(X^T)}$\n",
    "\n",
    "http://researcher.watson.ibm.com/researcher/files/us-pederao/ADTalk.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessians\n",
    "\n",
    "Why do we need to compute the hessian? $\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}$? I though we were only interested in $\\frac{\\partial^2 f}{\\partial x_1^2 }$. Aka the diagonal of the hessian, which should be easier to invert/calculate? How can we use $\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}$ to update our weights?\n",
    "\n",
    "Does $\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} = \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}$??\n",
    "\n",
    "\n",
    "$${\\mathbf  H}={\\begin{bmatrix}{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}$$\n",
    "\n",
    "shape = $(n \\times n)$ so for your typical NN it is $(1,000,000 \\times 1,000,000)$ giving roughly $10^{12}$ elements... Finding the inverse of a matrix is $\\mathcal O (n^3)$ (??) so $\\approx 10^{36}$ operations?\n",
    "\n",
    "If it even exists... (when doesnt it?)\n",
    "\n",
    "## Hessian chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&f:\\mathbb R^m \\rightarrow \\mathbb R^n \\\\\n",
    "&g:\\mathbb R^n \\rightarrow \\mathbb R^p \\\\\n",
    "y &= g(f(x)) \\\\\n",
    "\\nabla_x g(f(x)) &= \\nabla_{f(x)} g(f(x)) \\nabla_xf(x) \\tag{Chain rule} \\\\\n",
    "&\\text{(p x m) = (p x n) x (n x m)}\\\\\n",
    "\\nabla_x \\Big(\\nabla_x g(f(x))\\Big) &= \\nabla_x\\Big(\\nabla_{f(x)} g(f(x))\\Big) \\nabla_xf(x) +  \\nabla_{f(x)} g(f(x)) \\nabla_x \\Big(\\nabla_xf(x)\\Big) \\tag{Product rule}\\\\\n",
    "&= \\nabla_xf(x)^T\\nabla^2_{f(x)} g(f(x)) \\nabla_xf(x) +  \\nabla_{f(x)} g(f(x)) \\nabla^2_xf(x) \\tag{Chain rule}\\\\\n",
    "&\\text{(p x m x m) = (m x n)(n x p x n)(n x m) + (p x n)(n x m x m)}\\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{f(x)} g(f(x))= \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial g}{\\partial f} & \\dots & \\frac{\\partial g}{\\partial f}\\\\\n",
    "\\vdots &\\ddots&\\vdots \\\\\n",
    "\\frac{\\partial g}{\\partial f} & \\dots & \\frac{\\partial g}{\\partial f}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reshaping\n",
    "(why?) \n",
    "Why are we afraid of tensors? \n",
    "We can use they analytically?\n",
    "\n",
    "$$\n",
    "H_f:= \\begin{bmatrix}\\nabla^2_xf_1(x) \\\\ \\vdots \\\\ \\nabla^2_xf_i(x) \\\\ \\vdots \\\\ \\nabla^2_xf_n(x) \\\\ \\end{bmatrix} =  \\begin{bmatrix} H_{f_1} \\\\ \\vdots \\\\ H_{f_i} \\\\ \\vdots \\\\ H_{f_n} \\\\ \\end{bmatrix} \\text{(nm x m)} \\quad\n",
    "\\;H_g:= \\begin{bmatrix}\\nabla^2_{f(x)} g_1(f(x)) \\\\ \\vdots \\\\ \\nabla^2_{f(x)} g_j(f(x)) \\\\ \\vdots \\\\ \\nabla^2_{f(x)} g_p(f(x)) \\\\ \\end{bmatrix} = \\begin{bmatrix} H_{g_1} \\\\ \\vdots \\\\ H_{g_j} \\\\ \\vdots \\\\ H_{g_p} \\\\ \\end{bmatrix} =  \\text{(pn x n)} \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\nabla f\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "http://math.stackexchange.com/questions/174270/what-exactly-is-the-difference-between-a-derivative-and-a-total-derivative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
