{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should the stability of a classifier be defined? How should the stability of a regressor be defined? Anything else?\n",
    "\n",
    "Given $f: X \\rightarrow Y$\n",
    "\n",
    "* we can replace an element in X and get little to no change in Y. $f(\\hat{x}) \\sim f(x)$\n",
    "* we can permute X with $\\epsilon$ and get little to no change in Y. $f(x+\\epsilon) \\sim f(x)$\n",
    "* Like dropout? $\\hat{f}(x) \\sim f(x)$. Where we are premuting the function rather than the data.\n",
    "    * So we could drop a small amount of parameters,\n",
    "    * Or permute the parameters with $\\epsilon$.\n",
    "\n",
    "So pretty much $\\frac{dy}{dx} \\mid_x \\approx 0$?\n",
    "\n",
    "So we want these points to be in 'attractors' ? Does that also imply we want other places to be unstable?\n",
    "\n",
    "Imagine we are classifying MNIST. We would want 11 different attractors? One for each category, and another for being uncertain? \n",
    "* How do you design systems with attractors?\n",
    "* What is a good way to visualise a CNN as having 10 attractors?\n",
    "\n",
    "\n",
    "### Attractors\n",
    "\n",
    "(what is the definition?)\n",
    "How does this work in non (time) evolving systems?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
