{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "%matplotlib inline\n",
    "\n",
    "### The mnist data set\n",
    "from fuel.datasets import MNIST\n",
    "def get_data(datatype):\n",
    "    mnist = MNIST((datatype,))\n",
    "    state = mnist.open()\n",
    "    im,_ = mnist.get_data(state=state, request=[i for i in range(mnist.num_examples)] )\n",
    "    return im.reshape(im.shape[0],28*28)/255\n",
    "data = get_data('train')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "### Derivation\n",
    "\n",
    "* Find the covariance matrix - an empirical approximation.\n",
    "* Decompose it into SVD \n",
    "    * but with a smaller ???\n",
    "        * No just pick largest scaling values and corresponding rotations ? rotatio columns = eigenvectors of xTx?\n",
    "* Pick principle components (/eigenvectors?).\n",
    "$$\n",
    "\\begin{align}\n",
    " \\mathbf{Q}_{\\mathbf{XY}} = \\frac{1}{n-1} \\mathbf{M}_{\\mathbf{X}}^T \\mathbf{M}_{\\mathbf{Y}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Resources\n",
    "* https://en.wikipedia.org/wiki/Covariance_matrix\n",
    "* http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf\n",
    "* http://mengnote.blogspot.co.nz/2013/05/an-intuitive-explanation-of-pca.html\n",
    "* [Principle components, minor components and linear networks - Oja](https://users.ics.aalto.fi/oja/Oja92.pdf)\n",
    "* [Mixtures of Probabilistic Principal Component Analysers](http://www.miketipping.com/papers/met-mppca.pdf)\n",
    "\n",
    "### Covariance\n",
    "Why? We want to find how our dimensions vary with each other one. So we need to find how change in one dimension is correlated with the change in others. Another way of saying this is ..\n",
    "\n",
    "$$\n",
    "\\text{Co-Variance} (X) = X^T \\cdot X \\\\\n",
    "$$\n",
    "So let's pick a simple example to play with, a (4,3) matrix of data samples/observations and features/dimensions.\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "x_{1,1} & x_{2,1} & x_{3,1} \\\\\n",
    "x_{1,2} & x_{2,2} & x_{3,2} \\\\\n",
    "x_{1,3} & x_{2,3} & x_{3,3} \\\\\n",
    "x_{1,4} & x_{2,4} & x_{3,4} \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} \\\\\n",
    "x_{2,1} & x_{2,2} & x_{2,3} & x_{2,4} \\\\\n",
    "x_{3,1} & x_{3,2} & x_{3,3} & x_{3,4} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}\n",
    "x_{1,1}x_{1,1} + x_{2,1}x_{2,1} + x_{3,1}x_{3,1} & x_{1,1}x_{1,2} + x_{2,1}x_{2,2} + x_{3,1}x_{3,2} & x_{1,1}x_{1,3} + x_{2,1}x_{2,3} + x_{3,1}x_{3,3} & x_{1,1}x_{1,4} + x_{2,1}x_{2,4} + x_{3,1}x_{3,4} \\\\\n",
    "x_{1,2}x_{1,1} + x_{2,2}x_{2,1} + x_{3,2}x_{3,1} & x_{1,2}x_{1,2} + x_{2,2}x_{2,2} + x_{3,2}x_{3,2} & x_{1,2}x_{1,3} + x_{2,2}x_{2,3} + x_{3,2}x_{3,3} & x_{1,2}x_{1,4} + x_{2,2}x_{2,4} + x_{3,2}x_{3,4} \\\\\n",
    "x_{1,3}x_{1,1} + x_{2,3}x_{2,1} + x_{3,3}x_{3,1} & x_{1,3}x_{1,2} + x_{2,3}x_{2,2} + x_{3,3}x_{3,2} & x_{1,3}x_{1,3} + x_{2,3}x_{2,3} + x_{3,3}x_{3,3} & x_{1,3}x_{1,4} + x_{2,3}x_{2,4} + x_{3,3}x_{3,4} \\\\\n",
    "x_{1,4}x_{1,1} + x_{2,4}x_{2,1} + x_{3,4}x_{3,1} & x_{1,4}x_{1,2} + x_{2,4}x_{2,2} + x_{3,4}x_{3,2} & x_{1,4}x_{1,3} + x_{2,4}x_{2,3} + x_{3,4}x_{3,3} & x_{1,4}x_{1,4} + x_{2,4}x_{2,4} + x_{3,4}x_{3,4} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "$$\n",
    "Alternatively, we can view this matrix as the product of columns of X. Where $c_i$ indicates the ith column of X.\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "c_1 \\cdot c_1 & c_1 \\cdot c_2 & c_1 \\cdot c_3 & c_1 \\cdot c_4 \\\\\n",
    "c_2 \\cdot c_1 & c_2 \\cdot c_2 & c_2 \\cdot c_3 & c_2 \\cdot c_4 \\\\\n",
    "c_3 \\cdot c_1 & c_3 \\cdot c_2 & c_3 \\cdot c_3 & c_3 \\cdot c_4 \\\\\n",
    "c_4 \\cdot c_1 & c_4 \\cdot c_2 & c_4 \\cdot c_3 & c_4 \\cdot c_4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So what do entries of the covariance matrix really tell us? \n",
    "* Along the **diagonal** we have the sum of the squared values for a given column of X.\n",
    "    * E.g. ```CoVar(X)[0,0]``` is $x_{1,1}x_{1,1} + x_{2,1}x_{2,1} + x_{3,1}x_{3,1} = c_1 \\cdot c_1$\n",
    "* The rest of the elements (**off-diagonal**) are the dot product of two different columns of X.\n",
    "    * E.g. ```CoVar(X)[3,0]``` is $x_{1,4}x_{1,1} + x_{2,4}x_{2,1} + x_{3,4}x_{3,1} = c_4\\cdot c_1$\n",
    "    * Which is equivalent to the dot product of column 1 with column 4. \n",
    "    \n",
    "What does the dot product of two vectors mean?\n",
    "From the definition of variance we have \n",
    "\n",
    "Assuming the columns have been whitened (which we will come to later) lets explore the products of some columns.\n",
    "\n",
    "Now comes the complicated part. $(-) \\times (-) = (+), (+) \\times (+) = (+), (-) \\times (+) = (-)$ We can see that if the two column entries have the same sign, aka are in some binary sense correlated, then they will result in a position value. If not, negative. From this, it is easily seen that the closer the two columns are to being the same, the higher the correlation (remember the columns are whitened, so increasing the value of a column will not necessarily result in higher correlation).\n",
    "\n",
    "The key to this is the whitening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    return (x-np.mean(x,0).reshape((1,x.shape[1]))) / np.sqrt(np.var(x,0).reshape((1,x.shape[1]))**2 + 1e-6)\n",
    "    #return (x-np.mean(x,1).reshape((x.shape[0]),1)) / np.sqrt(np.var(x,1).reshape((x.shape[0],1))**2 + 1e-8)\n",
    "ims = whiten(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000635718471861\n",
      "0.000634716982959\n"
     ]
    }
   ],
   "source": [
    "my_covar = np.dot(ims.T , ims)*(1/(166*ims.shape[0])) #haha, why 166?\n",
    "their_covar = np.cov(data.T)\n",
    "print(np.mean(my_covar[100,0:784]))\n",
    "print(np.mean(their_covar[100,0:784]))\n",
    "#hmm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1843968"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "Is ?\n",
    "\n",
    "> * The columns of V (right-singular vectors) are eigenvectors of M∗M.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of MM∗.\n",
    "* The non-zero elements of Σ (non-zero singular values) are the square roots of the non-zero eigenvalues of M∗M or MM∗.\n",
    "\n",
    "Show/prove? this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principle component variance =  5.38127233108\n"
     ]
    }
   ],
   "source": [
    "### Using numpy - covariance and SVD\n",
    "U, S, V = np.linalg.svd(their_covar, full_matrices=True)\n",
    "print('Principle component variance = ',np.max(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principle component variance =  5.38073420384\n"
     ]
    }
   ],
   "source": [
    "### Using sklearn's PCA\n",
    "pca = PCA(n_components=5,whiten=True)\n",
    "pca.fit(data)\n",
    "print('Principle component variance = ',pca.explained_variance_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check how well our model fits to training data vs test data vs noise\n",
    "print(pca.score(train_ims))\n",
    "print(pca.score(test_ims))\n",
    "print(pca.score(np.random.random(test_ims.shape)))\n",
    "#how can you have positive log-likelihoods?\n",
    "\n",
    "test_score = pca.score_samples(test_ims)\n",
    "noise_score = pca.score_samples(np.random.random(test_ims.shape))\n",
    "print(np.min(test_score))\n",
    "print(np.max(noise_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kpca = KernelPCA(kernel='sigmoid')\n",
    "# kpca.fit(train_ims)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
