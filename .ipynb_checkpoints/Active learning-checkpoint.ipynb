{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries as reinforcement learning??\n",
    "\n",
    "* Query = action\n",
    "* Set of queries = policy\n",
    "* etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data, oracle   # oracle(x) = data\n",
    "patterns = cluster(data) #some unsupervised method to find patterns, clustering?\n",
    "\n",
    "#initialise a NN based on what we know from the patterns <- how???\n",
    "model = NeuralNet(patterns)  #generate a NN based on the patterns found\n",
    "\n",
    "import ActiveLearner\n",
    "NN_depth, NN_breadth = ActiveLearner(self, patterns) #given its past experience and contextual info\n",
    "\n",
    "while not target_accuracy: #and suitable sample size?\n",
    "    \n",
    "    for b in breadth:\n",
    "        # pick a leaf (aka query) to explore\n",
    "        Leaf = NN_breadth.choose_likely_leaf() #random query biased by experience and context\n",
    "        queries.append(Leaf) #canidate queries\n",
    "        \n",
    "        #explore leaf with MCTS\n",
    "        for m in MCTS:\n",
    "            \n",
    "            #pick actions randomly to depth d, thus making a policy\n",
    "            for d in depth: \n",
    "                query_policy.append(MC.random_query()) #could do better than random?, recycle NN_breadth?? \n",
    "            \n",
    "            #evaluate the policy.\n",
    "            leaf_policy_values.append(NN_depth.predict_value(query_policy)) \n",
    "            #aka - predict the accuracy of a net trained with the query - result pair\n",
    "        \n",
    "        #average value across all explored policies for the leaf - could be better than average?\n",
    "        values.append( mean(leaf_policy_values) )\n",
    "    \n",
    "    #ask query that is predicted to (on average) give the greatest accuracy\n",
    "    q = queries(argmax(values))\n",
    "    r = oracle.query(q) \n",
    "    count+= 1\n",
    "    \n",
    "    #train our model and test it\n",
    "    model.train(q,r) #back prop. but also need some way to change topology??\n",
    "    if model.test() > tol:  \n",
    "        target_accuracy = True\n",
    "        #but how can we actually test it without labeled data?!? \n",
    "        #we need a decent sample? \n",
    "        #or can it just be assumed/inferred from the patterns in the clusters and a validation?\n",
    "        \n",
    "#loss = some function of the number of queries and the complexity of the solution? which function?!?\n",
    "loss = func(count,model.complexity) \n",
    "ActiveLearner.reinforce(loss) #How? Need to figure out this algorithm properly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Thoughts and questions\n",
    "\n",
    "\n",
    "How can unsupervised techniques like clustering help us decide;\n",
    "* what queries to ask?\n",
    "* what solutions are likely? (topology of the net)\n",
    "Also, how can we intelligently change the topology of a network based on training pairs?\n",
    "\n",
    "\n",
    "*****\n",
    "\n",
    "\n",
    "* Correct results can only ever infer hypotheses (with probabilities??) \n",
    "    * Although, we can also use occams razor to help us choose a solution/explanation...\n",
    "* So, one query 'rules out'/falsifies a subset of potential weights/solutions/explanations, iff it is wrong. \n",
    "    * So traditional ML does not use the information given by mistakes to its fullest potential? \n",
    "    * Noise <- causes problems.\n",
    "        * You can also make mistakes due to noise... \n",
    "        * But we can use probabilities to try to figure out if mistakes are generated by noise or by error\n",
    "            * But, we would have to make an assumption about the noise, is there a way around this??\n",
    "* The remaining weight space is/are the plasuible hypotheses.\n",
    "    * How can we cut this weight space down as quickly as possible?\n",
    "        * Make errors. \n",
    "            * But how would it choose which inputs it wants to (try to) falisify? (or what queries it wants to ask)\n",
    "                * Queries are biased on past experience (like the breadth net from AlphaGO)\n",
    "            * What sort of input-output pair gives the most information? The largest set of weights/explanations we can remove\n",
    "        * This reduces the computational load for inference. as it would have to search a smaller weight space\n",
    "    * Weight space increases exponentially with the size/complexity of the net.\n",
    "        * So a good init is important. Start small, but not too small as the solution may be missed.\n",
    "        * How do we know what size/complexity to make it? \n",
    "            * We need to be able to approximate the complexity from the queries we ask\n",
    "\n",
    "*****\n",
    "\n",
    "Assuming there is a causal relation, is it always possible to deduce the relation from falsifications? (this assumes the past will be the same as the present?? but we can just define a function as being a function of the past?) \n",
    "\n",
    "What is the maximum amount of information that can be gained from a single mistake?\n",
    "Can the solution always be improved by asking more queries??? Idealy no, but with real data, with noise, probably??\n",
    "\n",
    "Minimise the amount of queries made to the teacher (aka. What is the minimum amount of experiments I need to run to learn this function/explain this phenomena?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ideal active learner?\n",
    "\n",
    "import MNSIT, categories #MNIST = data, categories = set of possible queries/actions\n",
    "\n",
    "Patterns = Cluster(data) #this should give us ten different clusters? hopefully\n",
    "\n",
    "Policy = ActiveLearner(Patterns,categories) #the policy should be\n",
    "#Test each category with n-1 trials. Could even infer some categories half way through? By similarities?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to use queries to not only validate clusters as being linked to a category, but also help us decide on the position of the hyperplane seperating two classes, as its position may be ambiguious within a certain range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Simpler version\n",
    "# we only want to try to learn things we (would?) have got wrong\n",
    "\n",
    "CNN = ConvolutionalNetwork(shape)\n",
    "Active = ActiveLearner(shape)\n",
    "\n",
    "Active.feed(data) #give the active learner a reference to the data\n",
    "#stores them in some sort of interesting represenation?\n",
    "\n",
    "accuracy = CNN.test() \n",
    "\n",
    "while not finished:\n",
    "    selected_datapoint = Active.query(CNN) #picks randomly to start???\n",
    "    image, label = data(selected_datapoint)\n",
    "    \n",
    "    # Train the CNN\n",
    "    guess = CNN.forward(image)\n",
    "    Loss = CrossEntropy(label, guess)\n",
    "    CNN.train(Loss)\n",
    "    \n",
    "    # testing all the time like this uses more omputations\n",
    "    current_accuracy = CNN.test() #test on validation set?\n",
    "    \n",
    "    # Train the Active learner\n",
    "    dAccuracy = accuracy - current_accuracy #some sort of finite difference to approximate the gradient?\n",
    "    accuracy = current_accuracy\n",
    "    Active.train(selected_datapoint,daccuracy) #hmm, david reckoned this was reinforcement, but here we have a gradient\n",
    "    \n",
    "    \n",
    "class ActiveLearner():\n",
    "    def __init__(self,shape,):\n",
    "        self.shape = shape\n",
    "        self.weights = np.random.standard_normal(shape)\n",
    "        \n",
    "    def feed(self,data):\n",
    "        #store in an embedding to allow us to find patterns in the data\n",
    "        #to help inform decisions about which datapoint to pick next???\n",
    "        \n",
    "        self.dataset = data \n",
    "        self.\n",
    "        \n",
    "    def query(self, ): \n",
    "        #what information should we be making this decision based on?\n",
    "        #the past history of images, guesses and labels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search and research\n",
    "\n",
    "Very little on machine learning and meta ---\n",
    "Related to;\n",
    "* Tree searches?\n",
    "* \n",
    "\n",
    "Meta- cognition, memory, reasoning\n",
    "Active learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
