{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Why bother? The loss function gies us something to optimise.\n",
    "\n",
    "Loss should = How close it was to getting it right versus how close it was to getting it wrong? This excludes the possibility that two classifications are similar? (as we are trying to widen the distance between each representation)\n",
    "\n",
    "We want to use as much information as possible.\n",
    "For example: our classifier gives us classication values for 3 different possibilities - [1.4,-2.1,4.2]. \n",
    "\n",
    "### Examples\n",
    "\n",
    "Some examples of different loss strategies for different applications\n",
    "\n",
    "* SVM classification - $\\ell(y) = \\max(0, 1-t \\cdot y)$\n",
    "* Softmax - cross entropy -log likelihood ...\n",
    "* Squared error\n",
    "* ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "maybe I should document some of the different types of activation fucntion and include graphs, info about gradients, ... why?\n",
    "\n",
    "Examples\n",
    "\n",
    "* RELU\n",
    "* Leaky\n",
    "* Tanh\n",
    "* Sigmoid\n",
    "* Exp(-phi)\n",
    "* ?? Maxout, elu, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation update rules\n",
    "\n",
    "\n",
    "### Examples\n",
    "Stocastic grad descent\n",
    "Adagrad\n",
    "Momentum (Nesterov)\n",
    "RMSprop\n",
    "Adam\n",
    "\n",
    "Second order optimization methods ?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "* Normalising\n",
    "* mean = 0\n",
    "* whitening\n",
    "* PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting points\n",
    "\n",
    "* Make sure thatyou can overfit verysmall portion of the training data. ->This means that the net should have enough complexity to fit the data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* l5 - p35 - mean 0, all positive input to a node???\n",
    "* l5 - p60 - decreasing stdev of weight. does this have any significance?\n",
    "* what does vanilla mean?!?\n",
    "* regularisation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
