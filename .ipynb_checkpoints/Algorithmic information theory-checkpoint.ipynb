{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Definition\n",
    "\n",
    "\n",
    "### Motivation\n",
    "\n",
    "What am I interested in AIT? \n",
    "\n",
    "I see it as a way to quantify the 'elegance' of functions. The smaller a function (or explanaion) is the less complex it is and the more general (how sure am I about this, is there some sort of proof?). So, what if we could optimise neural networks for their elegance?\n",
    "\n",
    "Unfortunately, kolmogorov complexity cannot be calculated, but can we come up with another metric for the complexity of neural networks?\n",
    "\n",
    "Now, it has already been shown that some NNs have displayed interesting abilities to generalise features. For example, image recognition CNNs learning to recognise handwriting, faces, ...\n",
    "\n",
    "I think that this somehow ties back to energy. Where the brain is naturally lazy/efficient and is always trying to generalise but is constrained by ???. T wo competing processes. Need to balance them right, what ratio, between specificity and generality (energy entropy - information entropy?)\n",
    "\n",
    "\n",
    "### Dropout\n",
    "\n",
    "I see dropout as a hack to achieve this goal. It forces the representations/fratures to be as simple as possible ... explain how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The information content or complexity of an object can be measured by the length of its shortest description. For instance the string _\"0101010101010101010101010101010101010101010101010101010101010101\"_ has the short description \"32 repetitions of '01'\", while _\"1100100001100001110111101110110011111010010000100101011110010110\"_\n",
    "presumably has no simple description other than writing down the string itself.\n",
    "\n",
    "No. This seems quite unfair, we are compring two different langauges.\n",
    "\n",
    "In binary we have 2 states, 0,1. Thus each gives $-p_i log(p_i) = -0.5log_2(0.5) = 1$ bit of information, but in english, each letter can be a,b,c...z or punctuation. Thus a single letter gives us $-\\frac{1}{30}log_{2}(\\frac{1}{30}) = 5$ bits of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Energy efficiency (is a natural optimiser?)\n",
    "\n",
    "\n",
    "* What if we counted how many nodes fired and also optimised (minima) for fewer activations?\n",
    "* What would happen if we measued node outputs and minimised (multi-objective) them?\n",
    "\n",
    "So now we are optimising for both energy efficiency and accuracy. (a side note, what else could be worth optimising for?) Is energy efficiency some sort of analouge for AIG or better representations...? And can we prove that?\n",
    "\n",
    "\n",
    "### A metric to minimise\n",
    "\n",
    "What is the relation to turing machines? Because there is a (universal) language that all functions can be described in it is possible how much of that language is needed for certain functions. Aka algorithmic information theory. How does this apply to neural networks? What language can we use to decribe them and their calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://en.wikipedia.org/wiki/Information_theory\n",
    "http://www.scholarpedia.org/article/Algorithmic_information_theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "* What does this have to do with omega, genetic algorithms, ... randomness, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
