{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In novelty detection (my problem) we want our classifier to draw a tight (decision) boundary around the data it has seen (but not so tight that it cant find patterns within our data). This tight boundary should help us classify/distinguish the novelties that lie outside our boundary (or off our manifold, if you prefer) and the data within/on it. The problem I have been facing is that my novelty detectors over generalise and classify novelties as being part of the original dataset. Our current solution to this problem is to use an adversarial generator that learns/is a model of novelties (an infinitely large class that we really have no idea about -- fundamentally, I dont think this makes sense). We can then use this model to help shape/refine our boundaries/manifold (aka throw noise, crappy and eventually plausible, yet fake, images at your model and use them to help tighen the boundary). \n",
    "\n",
    "What I really want is a way to control how a network generalises (where it generalises and how well). So that I can tell my classifier to draw tight boundaries around my data without needing to train on noise, or more generally, without needing to have a model of novelties. I think this is related to adversarial examples, stability and transfer learning (which is the converse problem ??). Any ideas, pointers or thoughts?\n",
    "\n",
    "## Adversarial examples\n",
    "\n",
    "Potential explanations?\n",
    "* the classification manifold passes close to itself in some dimensions and a small change can bump us over to (this would imply we need to add some sort of regularisation?) \n",
    "* we should expect this behaviour?\n",
    "* if we know the structure of the CNN then we can design an input that will break it. We can get inputs to cancel them selves out at certain layers, ... ??? (have anyone analysed the activation patterns of aversarially generated images vs normal ones? are they processed differently? what properties do they have?)\n",
    "* What sorts of architectures does this effect? Isnt this what sparsity is suposed to combat??\n",
    "\n",
    "\n",
    "## Manifold interpretation\n",
    "\n",
    "* What does this look like for multiple classes? Where do you add decision boundaries? Decision boundaries become surfaces, volumes, manifolds??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
