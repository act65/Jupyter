{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Taylor approximations.\n",
    "\n",
    "We have some function, say $y = sin(x)$ and we want to approximate it within some neighborhood located around $a$. \n",
    "\n",
    "We can do this (very poorly) by doing a linear approximation, $f(x) = mx + c$. So we know $a$, the neighborhood that we want to approimate y around. So $f$ should be the same as $y(a)$,  so $f(a) = y(a) = 0$, and also the gradient should also be the same, $df/dx(a) = dy/dx(a) = 1$.\n",
    "\n",
    "So given these two relations we can solve for $m$ and $c$.\n",
    "\n",
    "$$\n",
    "f(a) = mx + c = 1 \\\\\n",
    "df/dx = m = 0 \\\\\n",
    "$$\n",
    "\n",
    "Therefore $f(x) = 1$. We could continue this process, assuming we have access to higher order gradients, to improve the accuracy of our approximation. So using $d^2y/dx$ we could fit a polynomial of order 2 to the second derivative.\n",
    "\n",
    "$$\n",
    "f(a) = ax^2 + bx + c \\\\\n",
    "df/dx = 2ax + b \\\\\n",
    "d^2f/dx^2 = 2a \\\\\n",
    "$$\n",
    "\n",
    "Which ends up giving\n",
    "$$f(x) \\approx f(a)+{\\frac {f'(a)}{1!}}(x-a)+{\\frac {f''(a)}{2!}}(x-a)^{2}+{\\frac {f'''(a)}{3!}}(x-a)^{3}+\\cdots$$\n",
    "\n",
    "\n",
    "\n",
    "<!-- what if we learn each a? or could even just average the backpropagated grads? but how could we learn the higher order grads? estimates from the many first order ones?!? \n",
    "what about predicting $f''(a)$ -->\n",
    "\n",
    "\n",
    "#### ML\n",
    "\n",
    "In machine learning, we have access to multiple neighborhoods where we can evaluate, y,x (the training examples), dy/dx (and even higher order derivatives if we are willing to pay the computational price). But surely storing all the neighborhoods is not idea, which ones are the best and how should they be used?\n",
    "\n",
    "Questions. Can we collect/represent/approximate higher order gradients in some efficient manner? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nta(x): # neural taylor approximation\n",
    "    # a set of function approximators.\n",
    "    nets = [net for _ in range(N)]  # could just be lambda x: tf.matmul(x, w)\n",
    "    \n",
    "    # predict/lookup values of a depending on x. \n",
    "    # could just be a nearest neighbor...\n",
    "    # the neighborhood we are going to use to approximate f(x) around. \n",
    "    # preferably at x but we may not have good estimates for the gradients at, or close to x. (this seems close related to generalisation??!)\n",
    "    neighborhoods = [net(x) for net in nets]\n",
    "    \n",
    "    # evaluate the gradients within this neighborhood\n",
    "    # could use;\n",
    "    # - the grads of the fn approximators\n",
    "    # - local variables to average/collect gradients\n",
    "    # - more fn approximators?!\n",
    "    grads = [net(a) for a in neighborhoods]  # inits a new net for each c. \n",
    "    # is there any intelligent sharing that can be done here?\n",
    "    \n",
    "    # take a linear combination of ...?!!?!\n",
    "    y = tf.reduce_sum([((x-a)^i)*grad(a)/factorial(i) for i, a, grad in zip(range(N), neighborhoods, grads)])\n",
    "    \n",
    "    return y\n",
    "\n",
    "nta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first order neural taylor expansion gives a piecewise linear approximation. This interpretation implies that;\n",
    "\n",
    "we want to learn;\n",
    "* the best (finite) set of neighborhoods that allow use to 'cover' the input distribution of data. (the first layer)\n",
    "* how to combine predictions of f(x) from the different neighborhoods. (the second layer)\n",
    "\n",
    "Cool. So 1 hidden layer neural networks roughly do a 1st order taylor approximation. What do 2 layer neural networks do? And what would a 2nd order neural taylor approximation look like?\n",
    "\n",
    "\n",
    "\n",
    "## Problems.\n",
    "\n",
    "the nth order derivative (e.g. hessian) is large. and each higher order moment/derivative requires more space, scaling exponentially.\n",
    "\n",
    "solution. find compact/sparse representations, factorise their representations.\n",
    "\n",
    "could do;\n",
    "* a rank decomposition. $x^TWy = B^T(Ax \\circ By)$\n",
    "* local decomposition (related somehow to a conv) $x^TWy = M\\hat\\Sigma : M \\in \\mathbb R^{o\\times(k\\times d)}, \\hat\\Sigma is diagonal$\n",
    "\n",
    "what about circular convolutions? they seem somehow relevant here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Correlation.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "cov(x, y) &= \\mathbb E \\big[(X - \\mathbb E [X]) (Y - \\mathbb E [Y]) \\big] \\\\\n",
    "cov(x, x) &= \\mathbb E \\big[(x - \\mathbb E [x]) (X - \\mathbb E [x]) \\big] \\\\\n",
    "&= \\mathbb E[xx] - \\mathbb E[x]\\mathbb E [x] \\\\\n",
    "&= \\mathbb E[x^2] - \\mathbb E[x]^2 \\\\\n",
    "&\\approx X - \\sum_i x_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### nth order moments\n",
    "\n",
    "\n",
    "\n",
    "#### relation to derivatives\n",
    "\n",
    "what relation is there to statistical moments of different orders? if we rearrange the we get (roughly)\n",
    "$$\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)(x−a)^2\\\\\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)\\Sigma_{xx}^{@a}\\\\\n",
    "$$\n",
    "\n",
    "so the hessian picks out a linear combination of the covariance (no, not covariance, correlation) values?!?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Resources\n",
    "\n",
    "* http://www.math.smith.edu/~rhaas/m114-00/chp4taylor.pdf\n",
    "* http://mathinsight.org/taylor_polynomial_multivariable_examples\n",
    "* 3blue1brown..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
