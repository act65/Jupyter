{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Taylor approximations.\n",
    "\n",
    "We have some function, say $y = sin(x)$ and we want to approximate it within some neighborhood located around $a$. \n",
    "\n",
    "We can do this (very poorly) by doing a linear approximation, $f(x) = mx + c$. So we know $a$, the neighborhood that we want to approimate y around. So $f$ should be the same as $y(a)$,  so $f(a) = y(a) = 0$, and also the gradient should also be the same, $df/dx(a) = dy/dx(a) = 1$.\n",
    "\n",
    "So given these two relations we can solve for $m$ and $c$.\n",
    "\n",
    "$$\n",
    "f(a) = mx + c = 1 \\\\\n",
    "df/dx = m = 0 \\\\\n",
    "$$\n",
    "\n",
    "Therefore $f(x) = 1$. We could continue this process, assuming we have access to higher order gradients, to improve the accuracy of our approximation. So using $d^2y/dx$ we could fit a polynomial of order 2 to the second derivative.\n",
    "\n",
    "$$\n",
    "f(a) = ax^2 + bx + c \\\\\n",
    "df/dx = 2ax + b \\\\\n",
    "d^2f/dx^2 = 2a \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "#### ML\n",
    "\n",
    "In machine learning, we have access to multiple neighborhoods where we can evaluate, y,x (the training examples), dy/dx (and even higher order derivatives if we are willing to pay the computational price). \n",
    "\n",
    "Questions. Can we collect/represent/approximate higher order gradients in some efficient manner? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$f(x) \\approx f(a)+{\\frac {f'(a)}{1!}}(x-a)+{\\frac {f''(a)}{2!}}(x-a)^{2}+{\\frac {f'''(a)}{3!}}(x-a)^{3}+\\cdots$$\n",
    "\n",
    "what if we learn each a? or could even just average the backpropagated grads? but how could we learn the higher order grads? estimates from the many first order ones?!? \n",
    "what about predicting $f''(a)$\n",
    "\n",
    "what are we actually doing here? we would use a network to learn/predict ??? and then take a \n",
    "\n",
    "$$f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}(x−a)^THf(a)(x−a)$$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "what relation is there to statistical moments of different orders? if we rearrange the we get (roughly)\n",
    "$$\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)(x−a)^2\\\\\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)\\Sigma_{xx}^{@a}\\\\\n",
    "$$\n",
    "\n",
    "so the hessian picks out a linear combination of the covariance (no, not covariance, correlation) values?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Correlation.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "cov(x, y) &= \\mathbb E \\big[(X - \\mathbb E [X]) (Y - \\mathbb E [Y]) \\big] \\\\\n",
    "cov(x, x) &= \\mathbb E \\big[(x - \\mathbb E [x]) (X - \\mathbb E [x]) \\big] \\\\\n",
    "&= \\mathbb E[xx] - \\mathbb E[x]\\mathbb E [x] \\\\\n",
    "&= \\mathbb E[x^2] - \\mathbb E[x]^2 \\\\\n",
    "&\\approx X - \\sum_i x_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### nth order moments\n",
    "\n",
    "\n",
    "\n",
    "#### relation to derivatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def factorial(x):\n",
    "    result = 1\n",
    "    for i in xrange(2, x + 1):\n",
    "        result *= i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.random_normal([1, 10])\n",
    "N = 5\n",
    "nonlin = tf.identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def net(x):\n",
    "    W = tf.random_normal([10, 1])\n",
    "    b = tf.random_normal([1])\n",
    "    return nonlin(tf.matmul(x, W) + b)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nta(x): # neural taylor approximation\n",
    "    # a set of function approximators.\n",
    "    nets = [net for _ in range(N)]\n",
    "    \n",
    "    # predict/lookup values of a depending on x. \n",
    "    # could just be a nearest neighbor...\n",
    "    # the neighborhood we are going to use to approximate f(x) around. \n",
    "    # preferably at x but we may not have good estimates for the gradients at, or close to x. (this seems close related to generalisation??!)\n",
    "    neighborhoods = [net(x) for net in nets]\n",
    "    \n",
    "    # evaluate the gradients within this neighborhood\n",
    "    # could use;\n",
    "    # - the grads of the fn approximators\n",
    "    # - local variables to average/collect gradients\n",
    "    # - more fn approximators?!\n",
    "    grads = [net(a) for a in neighborhoods]  # inits a new net for each c. \n",
    "    # is there any intelligent sharing that can be done here?\n",
    "    \n",
    "    # take a linear combination of ...?!!?!\n",
    "    y = tf.reduce_sum([((x-a)^i)*grad(a)/factorial(i) for i, a, grad in zip(range(N), neighborhoods, grads)])\n",
    "    \n",
    "    return y\n",
    "# this will be a piecewise linear approximation in the 1st order case!!??!?!??!?\n",
    "# with each linear approximation be in the neighborhood, a.\n",
    "\n",
    "nta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Problems.\n",
    "\n",
    "the nth order derivative (e.g. hessian) is large. and each higher order moment/derivative requires more space, scaling exponentially.\n",
    "\n",
    "solution. find compact/sparse representations, factorise their representations.\n",
    "\n",
    "could do;\n",
    "* a rank decomposition. $x^TWy = B^T(Ax \\circ By)$\n",
    "* local decomposition (related somehow to a conv) $x^TWy = M\\hat\\Sigma : M \\in \\mathbb R^{o\\times(k\\times d)}, \\hat\\Sigma is diagonal$\n",
    "\n",
    "what about circular convolutions? they seem somehow relevant here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Resources\n",
    "\n",
    "* http://www.math.smith.edu/~rhaas/m114-00/chp4taylor.pdf\n",
    "* http://mathinsight.org/taylor_polynomial_multivariable_examples\n",
    "* 3blue1brown..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
