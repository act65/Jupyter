{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Taylor approximations.\n",
    "\n",
    "We have some function, say $y = sin(x)$ and we want to approximate it within some neighborhood located around $a$. \n",
    "\n",
    "We can do this (very poorly) by doing a linear approximation, $f(x) = mx + c$. We know $a$, the neighborhood that we want to approximate y around. So, the approximation $f$ should be the same as the true function at a, $f(a) = y(a) = 0$, and the gradient should also be the same, $df/dx(a) = dy/dx(a) = 1$.\n",
    "\n",
    "Given these two requirements we can solve for $m$ and $c$.\n",
    "\n",
    "$$\n",
    "f_a(x) = mx + c = 1 \\\\\n",
    "\\frac{df_a}{dx} = m = 0 \\\\\n",
    "$$\n",
    "\n",
    "Therefore $f(x) = 1\\cdot x = x$. We could continue this process, assuming we have access to higher order gradients, to improve the accuracy of our approximation. So using $\\frac{d^2y}{dx^2}$ we could fit a polynomial of order 2 to the second derivative.\n",
    "\n",
    "$$\n",
    "f(a) = ax^2 + bx + c \\\\\n",
    "df/dx = 2ax + b \\\\\n",
    "d^2f/dx^2 = 2a \\\\\n",
    "$$\n",
    "\n",
    "Continuing further gives the taylor approximation, \n",
    "$$f(x) \\approx f(a)+{\\frac {f'(a)}{1!}}(x-a)+{\\frac {f''(a)}{2!}}(x-a)^{2}+{\\frac {f'''(a)}{3!}}(x-a)^{3}+\\cdots$$\n",
    "\n",
    "<!-- insert joke... in the hood-->\n",
    "\n",
    "<!-- what if we learn each a? or could even just average the backpropagated grads? but how could we learn the higher order grads? estimates from the many first order ones?!? \n",
    "what about predicting $f''(a)$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural taylor approximations\n",
    "\n",
    "In a machine learning setting, we have access to multiple neighborhoods where we can evaluate, $y,x$ (the training examples), $dy/dx$ (and even higher order derivatives if we are willing to pay the computational price). But storing all the neighborhoods is not ideal (it would require linear memory in the size of the dataset). So which neighborhoods are the best and how should they be used? Could we learn which neighborhoods where the best!?\n",
    "\n",
    "<!-- Can we collect/represent/approximate higher order gradients in some efficient manner? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nta(x): # neural taylor approximation\n",
    "    # a set of function approximators. in this case linear. y = mx + c\n",
    "    # M is the number of neighborhoods we want to use\n",
    "    # N is the order of our approximation\n",
    "    nets = [net for _ in range(M*N)]  # could just be lambda x: tf.matmul(x, w)\n",
    "    \n",
    "    # predict/lookup values of `a` depending on `x`. \n",
    "    # could just be a nearest neighbor...\n",
    "    # this is a neighborhood we are going to use to approximate f(x) at. \n",
    "    neighborhoods = [net(x) for net in nets]\n",
    "    # NOTE preferably a = x but we may not have good estimates for the gradients at, or close to x. \n",
    "    # so we need to generalise!\n",
    "    \n",
    "    # evaluate the gradients within this neighborhood\n",
    "    # could use;\n",
    "    # - local variables to average/collect gradients (aka weights in final layer)\n",
    "    # - the grads of the fn approximators\n",
    "    # - accumulate the gradients w.r.t the error to find the best set of directions to remember\n",
    "    grads = [net(a) for a in neighborhoods]  # inits a new net for each c. \n",
    "    # NOTE is there any intelligent sharing that can be done here?\n",
    "    \n",
    "    # take a linear combination of our estimates.\n",
    "    # weight the estimates upon how 'good' the neighborhood, a, will be at x. \n",
    "    y = tf.reduce_sum([((x-a)^i)*grad(a, i)/factorial(i) for a, grad in zip(neighborhoods, grads) for i in range(N)])\n",
    "    \n",
    "    return y\n",
    "\n",
    "nta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this first order neural taylor approximation gives a piecewise linear approximation. This interpretation implies that;\n",
    "\n",
    "we want to learn;\n",
    "* the best (finite) set of neighborhoods that 'cover' the input distribution of data. (the first layer)\n",
    "* how to combine predictions of f(x) from the different neighborhoods. (the second layer)\n",
    "<img src=\"neuraltaylor.png\" alt=\"pic\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "This may remind you of a 1 hidden layer neural network! __TODO__ make this more rigorous.\n",
    "\n",
    "Cool. So 1 hidden layer neural networks do a piecewise 1st order taylor approximation. \n",
    "\n",
    "_From this perspective;_\n",
    "* _it would make more sense to use a bounded ramp function? min(max(0, x), 10)? as we want neighborhoods. otherwise we need to cancel out the other values of the ramp) -- actually the grad of sigmoid seems ideal...?_\n",
    "* _Width is the number of local approximators. And depth would be the ability to have non-linear weightings of the local approximation. I.e. we can repreat it a few times, recycle it over a few different areas, put half of it over there... _\n",
    "* _maybe it would be nice to have some prior on the distribution of neighborhoods?_\n",
    "* _as the dimensionality of x increases. there is exponentially more space that is 'far away' from a neighborhood._\n",
    "\n",
    "\n",
    "Cool, given these awesome results the next questions that follow are;\n",
    "* _What do 2 hidden layer neural networks do?_\n",
    "* _What would a 2nd order neural taylor approximation look like?_\n",
    "* _Can correlations of correlations help us learn?_\n",
    "\n",
    "Ultimately, I would like to investigate whether these are all the same thing, and if not, why not.\n",
    "\n",
    "<!-- Such that we are doing a piecewise _(cts would be better than piecewise)_ combination of, say, hessians. Switching out the right hessian (or higher order derivatives) depending on the neighborhood of the data. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order neural taylor approximations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layer neural networks\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= A\\rho(B\\rho(Cx)) \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix} \\rho \\big(\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix} \\rho \\big(\n",
    "\\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "\\end{bmatrix} x \\big) \\big)\\\\\n",
    "&= cbf \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order neural nets\n",
    "\n",
    "Aaand finally. The real goal.\n",
    "\n",
    "Second order networks. How can we train them, what do they learn, how do they generalise?\n",
    "\n",
    "$$\n",
    "f(x) = x\\mathbf Ax^T + Bx + c \\\\\n",
    "\\frac{\\partial f}{\\partial x} = x\\mathbf A + B \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x^2} = \\mathbf A \\\\\n",
    "$$\n",
    "aka A == the hessian!?\n",
    "\n",
    "$$\n",
    "\\mathcal L = \\parallel y - f(x)\\parallel_2^2 \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\mathbf A} = \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial B} = \\frac{\\partial \\mathcal L}{\\partial f} \\frac{\\partial f}{\\partial B}  = ?? (I \\circ x)\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial c} =  \\frac{\\partial \\mathcal L}{\\partial f} \\frac{\\partial f}{\\partial c} = ??(I)\\\\\n",
    "$$\n",
    "\n",
    "What about a non-linearity? The function is already non-linear. But maybe we want to ?!?!? choose which neighborhood to use to approximate, ...\n",
    "\n",
    "What if we nest multiple? Like a deep neural net?\n",
    "$$\n",
    "y = f_2(f_1(x)) \\\\\n",
    "y = (x\\mathbf A_1 x^T +B_1x +c_1) \\mathbf A_2 (x\\mathbf A_1 x^T +B_1x +c_1) + B_2(x\\mathbf A_1 x^T +B_1x +c_1) + c_2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems.\n",
    "\n",
    "the nth order derivative (e.g. hessian) is large. and each higher order moment/derivative requires more space, scaling exponentially.\n",
    "\n",
    "solution. find compact/sparse representations, factorise their representations.\n",
    "\n",
    "could do;\n",
    "* a rank decomposition. $x^TWy = B^T(Ax \\circ By)$\n",
    "* local decomposition (related somehow to a conv) $x^TWy = M\\hat\\Sigma : M \\in \\mathbb R^{o\\times(k\\times d)}, \\hat\\Sigma is diagonal$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf A_{i, :, :}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & \\dots & & & \\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} &\\dots& & \\\\\n",
    "\\dots &   a_{3,2} & a_{3,3} & a_{3,4} &\\dots& \\\\\n",
    "      &     \\dots & a_{4,3} & a_{4,4} & a_{4,5} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "but is symmetric so we dont need upper triangular?\n",
    "\n",
    "\n",
    "what about circular convolutions? they seem somehow relevant here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Resources\n",
    "\n",
    "* http://www.math.smith.edu/~rhaas/m114-00/chp4taylor.pdf\n",
    "* http://mathinsight.org/taylor_polynomial_multivariable_examples\n",
    "* 3blue1brown..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
