{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Taylor approximations.\n",
    "\n",
    "We have some function, say $y = sin(x)$ and we want to approximate it within some neighborhood located around $a$. \n",
    "\n",
    "We can do this (very poorly) by doing a linear approximation, $f(x) = mx + c$. We know $a$, the neighborhood that we want to approximate y around. So, the approximation $f$ should be the same as the true function at a, $f(a) = y(a) = 0$, and the gradient should also be the same, $df/dx(a) = dy/dx(a) = 1$.\n",
    "\n",
    "Given these two relations we can solve for $m$ and $c$.\n",
    "\n",
    "$$\n",
    "f(a) = mx + c = 1 \\\\\n",
    "df/dx = m = 0 \\\\\n",
    "$$\n",
    "\n",
    "Therefore $f(x) = 1$. We could continue this process, assuming we have access to higher order gradients, to improve the accuracy of our approximation. So using $d^2y/dx$ we could fit a polynomial of order 2 to the second derivative.\n",
    "\n",
    "$$\n",
    "f(a) = ax^2 + bx + c \\\\\n",
    "df/dx = 2ax + b \\\\\n",
    "d^2f/dx^2 = 2a \\\\\n",
    "$$\n",
    "\n",
    "Continuing further gives the taylor approximation, \n",
    "$$f(x) \\approx f(a)+{\\frac {f'(a)}{1!}}(x-a)+{\\frac {f''(a)}{2!}}(x-a)^{2}+{\\frac {f'''(a)}{3!}}(x-a)^{3}+\\cdots$$\n",
    "\n",
    "<!-- insert joke... in the hood-->\n",
    "\n",
    "<!-- what if we learn each a? or could even just average the backpropagated grads? but how could we learn the higher order grads? estimates from the many first order ones?!? \n",
    "what about predicting $f''(a)$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural taylor approximations\n",
    "\n",
    "In a machine learning setting, we have access to multiple neighborhoods where we can evaluate, $y,x$ (the training examples), $dy/dx$ (and even higher order derivatives if we are willing to pay the computational price). But storing all the neighborhoods is not ideal, which ones are the best and how should they be used? \n",
    "\n",
    "<!-- Can we collect/represent/approximate higher order gradients in some efficient manner? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nta(x): # neural taylor approximation\n",
    "    # a set of function approximators. in this case linear. y = mx + c\n",
    "    # M is the number of neighborhoods we want to use\n",
    "    # N is the order of our approximation\n",
    "    nets = [net for _ in range(M*N)]  # could just be lambda x: tf.matmul(x, w)\n",
    "    \n",
    "    # predict/lookup values of `a` depending on `x`. \n",
    "    # could just be a nearest neighbor...\n",
    "    # this is the neighborhood we are going to use to approximate f(x) around. \n",
    "    # preferably at x but we may not have good estimates for the gradients at, or close to x. (generalisation!)\n",
    "    neighborhoods = [net(x) for net in nets]\n",
    "    \n",
    "    # evaluate the gradients within this neighborhood\n",
    "    # could use;\n",
    "    # - local variables to average/collect gradients (aka weights in final layer)\n",
    "    # - the grads of the fn approximators\n",
    "    grads = [net(a) for a in neighborhoods]  # inits a new net for each c. \n",
    "    # is there any intelligent sharing that can be done here?\n",
    "    \n",
    "    # take a linear combination of \n",
    "    y = tf.reduce_sum([((x-a)^i)*grad(a, i)/factorial(i) for a, grad in zip(neighborhoods, grads) for i in range(N)])\n",
    "    \n",
    "    return y\n",
    "\n",
    "nta(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first order neural taylor expansion gives a piecewise linear approximation. This interpretation implies that;\n",
    "\n",
    "we want to learn;\n",
    "* the best (finite) set of neighborhoods that allow use to 'cover' the input distribution of data. (the first layer)\n",
    "* how to combine predictions of f(x) from the different neighborhoods. (the second layer)\n",
    "<img src=\"neuraltaylor.png\" alt=\"pic\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "Cool. So 1 hidden layer neural networks do a piecewise 1st order taylor approximation (__TODO__ proof...). Can this be generalised to higher order taylor approximations? Such that we are doing a piecewise _(cts would be better than piecewise)_ combination of, say, hessians. Switching out the right hessian (or higher order derivatives) depending on the neighborhood of the data.\n",
    "\n",
    "_From this perspective;_\n",
    "* _it would make more sense to use a bounded ramp function? min(max(0, x), 10)? as we want neighborhoods. otherwise we need to cancel out the other values of the ramp) -- actually the grad of sigmoid seems ideal...?_\n",
    "* _Width is the number of local approximators. And depth would be the ability to have non-linear weightings of the local approximation. I.e. we can repreat it a few times, recycle it over a few different areas, put half of it over there... _\n",
    "* _maybe it would be nice to have some prior on the distribution of neighborhoods?_\n",
    "* _as the dimensionality of x increases. there is exponentially more space that is 'far away' from a neighborhood._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The relation between correlation and derivatives.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "cov(x, y) &= \\mathbb E \\big[(X - \\mathbb E [X]) (Y - \\mathbb E [Y]) \\big] \\\\\n",
    "cov(x, x) &= \\mathbb E \\big[(x - \\mathbb E [x]) (X - \\mathbb E [x]) \\big] \\\\\n",
    "&= \\mathbb E[xx] - \\mathbb E[x]\\mathbb E [x] \\\\\n",
    "&= \\mathbb E[x^2] - \\mathbb E[x]^2 \\\\\n",
    "&\\approx X - \\sum_i x_i^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "cov(y, x) = \n",
    "\\begin{bmatrix}\n",
    "var(y_1, x_1) & \\dots & var(y_1, x_m)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "var(y_n, x_1) & \\dots & var(y_n, x_m)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_m}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\dots & \\frac{\\partial y_n}{\\partial x_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Best linear approximation is $y = \\mathcal J x$?\n",
    "\n",
    "\n",
    "#### nth order moments\n",
    "\n",
    "\n",
    "\n",
    "#### relation to derivatives\n",
    "\n",
    "what relation is there to statistical moments of different orders? if we rearrange the we get (roughly)\n",
    "$$\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)(x−a)^2\\\\\n",
    "f(x)\\approx f(a)+Df(a)(x−a)+\\frac{1}{2}Hf(a)\\Sigma_{xx}^{@a}\\\\\n",
    "$$\n",
    "\n",
    "so the hessian picks out a linear combination of the covariance (no, not covariance, correlation) values?!?\n",
    "\n",
    "Hessians\n",
    "\n",
    "what was that equation david showed us relating the hessian with a loss fn!?\n",
    "\n",
    "derivatives are linearisations. are linear operators and can be represented in matrix form.\n",
    "\n",
    "\n",
    "#### Baldi et al. 2012\n",
    "\n",
    "What do the weight matrices learn? In the case of $y = ABx, B = (A^*A)^{-1}A^* \\Sigma_{yx}\\Sigma_{xx}^{-1}$. Where $A(A^*A)^{-1}A^*$ is just some low rank projection, and in the case where rank of A and B is of the same rank as the data, then we get $A(A^*A)^{-1}A^* = I$, therefore $y = ABx =  \\Sigma_{yx}\\Sigma_{xx}^{-1}x$\n",
    "\n",
    "*** \n",
    "\n",
    "Therefore, $\\mathcal J = \\Sigma_{yx}\\Sigma_{xx}^{-1}$ ... in which cases?!\n",
    "\n",
    "So this means that we are learning a factorisation of the jacobian?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, given these awesome results the next questions that follow are;\n",
    "* _What do 2 hidden layer neural networks do?_\n",
    "* _What would a 2nd order neural taylor approximation look like?_\n",
    "* _Can correlations of correlations help us learn?_\n",
    "\n",
    "Ultimately, I would like to investigate whether these are all the same thing, and if not, why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order neural taylor approximations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layer neural networks\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= A\\rho(B\\rho(Cx)) \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix} \\rho \\big(\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix} \\rho \\big(\n",
    "\\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "\\end{bmatrix} x \\big) \\big)\\\\\n",
    "&= cbf \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order neural nets\n",
    "\n",
    "Aaand finally. The real goal.\n",
    "\n",
    "Second order networks. How can we train them, what do they learn, how do they generalise?\n",
    "\n",
    "$$\n",
    "f(x) = x\\mathbf Ax^T + Bx + c \\\\\n",
    "\\frac{\\partial f}{\\partial x} = x\\mathbf A + B \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x^2} = \\mathbf A \\\\\n",
    "$$\n",
    "aka A == the hessian!?\n",
    "\n",
    "$$\n",
    "\\mathcal L = \\parallel y - f(x)\\parallel_2^2 \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\mathbf A} = \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial B} = \\frac{\\partial \\mathcal L}{\\partial f} \\frac{\\partial f}{\\partial B}  = ?? (I \\circ x)\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial c} =  \\frac{\\partial \\mathcal L}{\\partial f} \\frac{\\partial f}{\\partial c} = ??(I)\\\\\n",
    "$$\n",
    "\n",
    "What about a non-linearity? The function is already non-linear. But maybe we want to ?!?!? choose which neighborhood to use to approximate, ...\n",
    "\n",
    "What if we nest multiple? Like a deep neural net?\n",
    "$$\n",
    "y = f_2(f_1(x)) \\\\\n",
    "y = (x\\mathbf A_1 x^T +B_1x +c_1) \\mathbf A_2 (x\\mathbf A_1 x^T +B_1x +c_1) + B_2(x\\mathbf A_1 x^T +B_1x +c_1) + c_2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems.\n",
    "\n",
    "the nth order derivative (e.g. hessian) is large. and each higher order moment/derivative requires more space, scaling exponentially.\n",
    "\n",
    "solution. find compact/sparse representations, factorise their representations.\n",
    "\n",
    "could do;\n",
    "* a rank decomposition. $x^TWy = B^T(Ax \\circ By)$\n",
    "* local decomposition (related somehow to a conv) $x^TWy = M\\hat\\Sigma : M \\in \\mathbb R^{o\\times(k\\times d)}, \\hat\\Sigma is diagonal$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf A_{i, :, :}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & \\dots & & & \\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} &\\dots& & \\\\\n",
    "\\dots &   a_{3,2} & a_{3,3} & a_{3,4} &\\dots& \\\\\n",
    "      &     \\dots & a_{4,3} & a_{4,4} & a_{4,5} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "but is symmetric so we dont need upper triangular?\n",
    "\n",
    "\n",
    "what about circular convolutions? they seem somehow relevant here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Resources\n",
    "\n",
    "* http://www.math.smith.edu/~rhaas/m114-00/chp4taylor.pdf\n",
    "* http://mathinsight.org/taylor_polynomial_multivariable_examples\n",
    "* 3blue1brown..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
