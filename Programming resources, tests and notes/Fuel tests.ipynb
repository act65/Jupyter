{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST(('train','test'))\n",
    "state = mnist.open()\n",
    "im,labels = mnist.get_data(state=state, request=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1, 28, 28)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(im.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1, 28, 28) (50, 1)\n"
     ]
    }
   ],
   "source": [
    "from fuel.schemes import ShuffledScheme\n",
    "scheme = ShuffledScheme(examples=mnist.num_examples, batch_size=50)\n",
    "for request in scheme.get_request_iterator():\n",
    "    data = mnist.get_data(state=state, request=request)\n",
    "print(data[0].shape, data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import CIFAR10\n",
    "cifar = CIFAR10(('train','test'))\n",
    "state = cifar.open()\n",
    "im,labels = cifar.get_data(state=state, request=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3, 32, 32)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(im.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_str = 'this this is a test'\n",
    "words = Counter(test_str.split())\n",
    "min_freq = 1\n",
    "# vocab = {v:i[0] for v,i in enumerate(words.items()) if i[1] >= min_freq}\n",
    "vocab = {i[0]:v for v,i in enumerate(words.items()) if i[1] >= min_freq}\n",
    "vocab_size = len(vocab)\n",
    "vocab['<S>'] = vocab_size\n",
    "vocab['</S>'] = vocab_size+1\n",
    "vocab['<UNK>'] = vocab_size+2\n",
    "# vocab[vocab_size+2] = '<S>'\n",
    "# vocab[vocab_size+1] = '</S>'\n",
    "# vocab[vocab_size+2] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "1-billion-word/training-monolingual.tokenized.shuffled/news.en-00001-of-00100 not found in Fuel's data path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b21b678f27f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneBillionWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moneword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneBillionWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/act65/anaconda/lib/python3.5/site-packages/fuel-0.2.0-py3.5-macosx-10.5-x86_64.egg/fuel/datasets/billion.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, which_set, which_partitions, dictionary, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;34m'1-billion-word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training-monolingual.tokenized.shuffled'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 'news.en-{:05d}-of-00100'.format(partition)))\n\u001b[0;32m---> 52\u001b[0;31m                 for partition in which_partitions]\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             if not all(partition in range(50)\n",
      "\u001b[0;32m/Users/act65/anaconda/lib/python3.5/site-packages/fuel-0.2.0-py3.5-macosx-10.5-x86_64.egg/fuel/datasets/billion.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;34m'1-billion-word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training-monolingual.tokenized.shuffled'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 'news.en-{:05d}-of-00100'.format(partition)))\n\u001b[0;32m---> 52\u001b[0;31m                 for partition in which_partitions]\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             if not all(partition in range(50)\n",
      "\u001b[0;32m/Users/act65/anaconda/lib/python3.5/site-packages/fuel-0.2.0-py3.5-macosx-10.5-x86_64.egg/fuel/utils/__init__.py\u001b[0m in \u001b[0;36mfind_in_data_path\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found in Fuel's data path\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: 1-billion-word/training-monolingual.tokenized.shuffled/news.en-00001-of-00100 not found in Fuel's data path"
     ]
    }
   ],
   "source": [
    "from fuel.datasets import OneBillionWord\n",
    "oneword = OneBillionWord('training', [1],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OneBillionWord in module fuel.datasets.billion:\n",
      "\n",
      "class OneBillionWord(fuel.datasets.text.TextFile)\n",
      " |  Google's One Billion Word benchmark.\n",
      " |  \n",
      " |  This monolingual corpus contains 829,250,940 tokens (including sentence\n",
      " |  boundary markers). The data is split into 100 partitions, one of which\n",
      " |  is the held-out set. This held-out set is further divided into 50\n",
      " |  partitions. More information about the dataset can be found in\n",
      " |  [CMSG14].\n",
      " |  \n",
      " |  .. [CSMG14] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, and\n",
      " |     Thorsten Brants, *One Billion Word Benchmark for Measuring Progress\n",
      " |     in Statistical Language Modeling*, `arXiv:1312.3005 [cs.CL]\n",
      " |     <http://arxiv.org/abs/1312.3005>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  which_set : 'training' or 'heldout'\n",
      " |      Which dataset to load.\n",
      " |  which_partitions : list of ints\n",
      " |      For the training set, valid values must lie in [1, 99]. For the\n",
      " |      heldout set they must be in [0, 49].\n",
      " |  vocabulary : dict\n",
      " |      A dictionary mapping tokens to integers. This dictionary is\n",
      " |      expected to contain the tokens ``<S>``, ``</S>`` and ``<UNK>``,\n",
      " |      representing \"start of sentence\", \"end of sentence\", and\n",
      " |      \"out-of-vocabulary\" (OoV). The latter will be used whenever a token\n",
      " |      cannot be found in the vocabulary.\n",
      " |  preprocess : function, optional\n",
      " |      A function that takes a string (a sentence including new line) as\n",
      " |      an input and returns a modified string. A useful function to pass\n",
      " |      could be ``str.lower``.\n",
      " |  \n",
      " |  See :class:`TextFile` for remaining keyword arguments.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneBillionWord\n",
      " |      fuel.datasets.text.TextFile\n",
      " |      fuel.datasets.base.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, which_set, which_partitions, dictionary, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fuel.datasets.text.TextFile:\n",
      " |  \n",
      " |  get_data(self, state=None, request=None)\n",
      " |      Request data from the dataset.\n",
      " |      \n",
      " |      .. todo::\n",
      " |      \n",
      " |         A way for the dataset to communicate which kind of requests it\n",
      " |         accepts, and a way to communicate what kind of request is being\n",
      " |         sent when supporting multiple.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state : object, optional\n",
      " |          The state as returned by the :meth:`open` method. The dataset\n",
      " |          can use this to e.g. interact with files when needed.\n",
      " |      request : object, optional\n",
      " |          If supported, the request for a particular part of the data\n",
      " |          e.g. the number of examples to return, or the indices of a\n",
      " |          particular minibatch of examples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple\n",
      " |          A tuple of data matching the order of :attr:`sources`.\n",
      " |  \n",
      " |  open(self)\n",
      " |      Return the state if the dataset requires one.\n",
      " |      \n",
      " |      Datasets which e.g. read files from disks require open file\n",
      " |      handlers, and this sort of stateful information should be handled\n",
      " |      by the data stream.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      state : object\n",
      " |          An object representing the state of a dataset.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from fuel.datasets.text.TextFile:\n",
      " |  \n",
      " |  example_iteration_scheme = None\n",
      " |  \n",
      " |  provides_sources = ('features',)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from fuel.datasets.base.Dataset:\n",
      " |  \n",
      " |  apply_default_transformers(self, stream)\n",
      " |      Applies default transformers to a stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      stream : :class:`~.streams.AbstractDataStream`\n",
      " |          A data stream.\n",
      " |  \n",
      " |  close(self, state)\n",
      " |      Cleanly close the dataset e.g. close file handles.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state : object\n",
      " |          The current state.\n",
      " |  \n",
      " |  filter_sources(self, data)\n",
      " |      Filter the requested sources from those provided by the dataset.\n",
      " |      \n",
      " |      A dataset can be asked to provide only a subset of the sources it\n",
      " |      can provide (e.g. asking MNIST only for the features, not for the\n",
      " |      labels). A dataset can choose to use this information to e.g. only\n",
      " |      load the requested sources into memory. However, in case the\n",
      " |      performance gain of doing so would be negligible, the dataset can\n",
      " |      load all the data sources and then use this method to return only\n",
      " |      those requested.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : tuple of objects\n",
      " |          The data from all the sources i.e. should be of the same length\n",
      " |          as :attr:`provides_sources`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple\n",
      " |          A tuple of data matching :attr:`sources`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy\n",
      " |      >>> class Random(Dataset):\n",
      " |      ...     provides_sources = ('features', 'targets')\n",
      " |      ...     def get_data(self, state=None, request=None):\n",
      " |      ...         data = (numpy.random.rand(10), numpy.random.randn(3))\n",
      " |      ...         return self.filter_sources(data)\n",
      " |      >>> Random(sources=('targets',)).get_data() # doctest: +SKIP\n",
      " |      (array([-1.82436737,  0.08265948,  0.63206168]),)\n",
      " |  \n",
      " |  get_example_stream(self)\n",
      " |  \n",
      " |  next_epoch(self, state)\n",
      " |      Switches the dataset state to the next epoch.\n",
      " |      \n",
      " |      The default implementation for this method is to reset the state.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state : object\n",
      " |          The current state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      state : object\n",
      " |          The state for the next epoch.\n",
      " |  \n",
      " |  reset(self, state)\n",
      " |      Resets the state.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      state : object\n",
      " |          The current state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      state : object\n",
      " |          A reset state.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default implementation closes the state and opens a new one. A\n",
      " |      more efficient implementation (e.g. using ``file.seek(0)`` instead\n",
      " |      of closing and re-opening the file) can override the default one in\n",
      " |      derived classes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from fuel.datasets.base.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  sources\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from fuel.datasets.base.Dataset:\n",
      " |  \n",
      " |  default_transformers = ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(OneBillionWord)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
