{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and goals\n",
    "\n",
    "Hunch. Resnets, fractal nets, ... ??? sculpt gradients to aid learning. \n",
    "We want to find a rigorous method of analysing gradients and use this to investigate why some networks learn better features than others.\n",
    "\n",
    "* How should gradients be used to correct errors and to learn?\n",
    "* What fundamental problems are there with using gradients to learn?\n",
    "    * Local minima\n",
    "    * Vanishing/exploding\n",
    "    * Only 1st order gradients are computationally feasible (?)\n",
    "    * ???\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs and PCA ... Baldi et al. 1989/2012\n",
    "\n",
    "### Summary\n",
    "Goals: \n",
    "1. Show that each critical point of $E(A,B)$ is some combination of principle components of $\\Sigma$, which are the eigen vectors of the covariance matrix of our data. (is that true?)\n",
    "* Show that the minium loss is achieved when the largest eigenvalues/leading eigenvectors are chosen/learnt.\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(A,B) = \\sum_t \\parallel y^t - ABx^t \\parallel \\tag{a} \\\\\n",
    "A^T(AB - \\Sigma_{YX}\\Sigma^{-1}_{XX}) = 0 \\\\\n",
    "S = AB - P_A \\Sigma_{YX}\\Sigma^{-1}_{XX} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "To show (1) they set A and solve for B given $E(A,B)$ is at a critical point (aka that the gradient of loss is zero) -- and vice versa. Using this result we find that AB is equal to the projection of the covariance YX and the inverse of covariance XX into a subspace of A (using a clever argument to show that $S = 0$, as S is both in the subspace of A and orthogonal to A). It follows that $\\Sigma$ is invariant to the columns of A, which is (roughly) the definition of an eigen vector. \n",
    "\n",
    "To show (2) we use a unitary change of coordinates, where the covariance XX is diagonal, to allow some nice rearranging. Because AB is a projection matrix we know that we can drop the aquared term, thus we can cancel we can simplity to the trace of $tr(\\Sigma) - tr(AB\\Sigma)$. Then using pertubation analysis of ...\n",
    "\n",
    "### Questions and notes\n",
    "\n",
    "* We did second order optimisation without computing the hessian. Interesting!\n",
    "* SVD proof of projection?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions ... Saxe et al.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Goals:\n",
    "1. Describe the dynamics/trajectories of parameter updates\n",
    "* Investigate orthogonal weight initialisations\n",
    "* \n",
    "\n",
    "Results:\n",
    "1. \n",
    "\n",
    "\n",
    "> _Our fundamental goal is to understand the dynamics of learning as a function of the input statistics $\\Sigma_{XX}$ and input-output statistics $\\Sigma_{YX}$._\n",
    "\n",
    "Hmm. So this is the goal of all learning algorithms?\n",
    "\n",
    "> _To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where $\\Sigma_{XX} = I$. This assumption will hold exactly for whitened input data, a widely used preprocessing step._\n",
    "\n",
    "\n",
    "$\\Sigma_{YX} = U_{mxm}S_{mxn}V_{nxn}^T =\\sum^{n}_{i=1} s_iu_iv_i^T$\n",
    "\n",
    "But the indicies dont work out?? as there are m lots of $u_i$ vectors in U, but we are only looking at n of them. So either $n>m$ and $u_n$ doesnt exist, or $n<m$ and we have not used all of them. And u_i.v_i doesnt work?!? Oh... we are talking about the covariance matrix, so m = n... Wait are we?\n",
    "\n",
    "\n",
    "* $u_i$ reflect the independent modes of variation in the output,\n",
    "    * so U and $\\Sigma_{YX}$ are also decorrelated??\n",
    "* $v_i$ reflect the independent modes of variation in the input,\n",
    "* $s_i$ are the singular values -- which mean __?\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Why can we call the loss function an energy function? Can I do this for any loss function? What makes this one special?\n",
    "* How can we transform to cts time?\n",
    "* What about cross entropy loss with softmax? What changes??\n",
    "* Dynamical isometry\n",
    "* Invariant manifold: where d/dt = 0 for some/all variables/parameters.\n",
    "* How much work (path integral of trajectory in the vector field) is done for different initialisations?\n",
    "* $C = a^2 - b^2 \\implies a = \\pm b$. What does this mean? Solutions are symmetric?!? It's a circle?!?\n",
    "* The learning rate is $\\mathcal{O} (t/s)$. But this seems to imply that we converge to stronger modes faster. So if we multiplied these modes in the data by some scalar we could learn faster? Also, A hunch. The stronger the mode, the faster we learn it, but the further we have to go?!?\n",
    "* $a^{\\alpha},b^{\\alpha}$. These represent the eigen vectors from the input layer and the output modes from into the final layer. What happened to the middle layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance = 1 (Glorot et al.)\n",
    "\n",
    "Summary:\n",
    "Investigate \n",
    "\n",
    "* How does variance actually help? It doesnt actually constrain the values. We can still get var = 1 with vanishing/exploding gradients.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random ideas\n",
    "\n",
    "\n",
    "### Orthogonal weights and rotating updates\n",
    "\n",
    "Initialise the weights to be orthonormal, then only update the weights using rotations. Why? As we dont want to bother learning orthogonal weights, the principle components (and normalising is just nice). As orthogonal weights are optimal (in non-linear systems as well????)\n",
    "\n",
    "```python\n",
    "weights = random.init(shape,'orthonormal')\n",
    "\n",
    "for batch in iter(batches):\n",
    "    grads = dL(weights,batch)\n",
    "    weights = rotate(weights,grads)\n",
    "```\n",
    "\n",
    "Forget about whether the loss function will impose this condition. Just add it ourselves.\n",
    "\n",
    "When is this a good/bad idea?\n",
    "* What if we cant whiten our data (is this even possible?)? I.e. if two variables are not separable? (well they probably wouldnt help us much anyway??)\n",
    "* \n",
    "\n",
    "### Attention modulated gradients\n",
    "\n",
    "Backrpop algol is wrong? Should be able to explain away residual prediction errors? Instead we just keep propagating them backward.\n",
    "\n",
    "Based on the predictive processing.\n",
    "Loss is dependent on amount of uncertainty.\n",
    "\n",
    "Inputs: \n",
    "* Prediction errors/residuals (which is like ??? surprise?) \n",
    "* Current hypotheses (which is like active learning)\n",
    "\n",
    "$f:X \\times Y \\rightarrow Z$\n",
    "\n",
    "$A(x,y) = ??$\n",
    "\n",
    "### GANs\n",
    "\n",
    "The discriminator net learns relevent features for discrimination (as expected). However, when we backprop through these, they sculpt the gradients. Thus ...\n",
    "\n",
    "TODO\n",
    "* Compare gradients. GAN v AE\n",
    "* Prove something about a linear GAN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
