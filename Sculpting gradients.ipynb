{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and goals\n",
    "\n",
    "Hunch. Resnets, fractal nets, ... ??? sculpt gradients to aid learning. \n",
    "We want to find a rigorous method of analysing gradients and use this to investigate why some networks learn better features than others.\n",
    "\n",
    "* How should gradients be used to correct errors and to learn?\n",
    "* What fundamental problems are there with using gradients to learn?\n",
    "    * Local minima\n",
    "    * Vanishing/exploding\n",
    "    * Only 1st order gradients are computationally feasible (?)\n",
    "    * ???\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear gradients\n",
    "\n",
    "So, an AE is trying to maximise the variance each middle node sees across the dataset. But they also interefere with eachother, competitively? Why do they learn this??? More variance captures more 'information'? Easier to separate?\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Does linear imply convexity?? Yes and no.(?)\n",
    "* How does the update rule imply we will learn the principle components?\n",
    "* If we initialised every weight to be the same, would it end up learning the same components?\n",
    "* Why does it learn perpendicular directions? Caputres more information, but how does it know that???\n",
    "* What do the biases mean??\n",
    "* Multi layer nets? Doing PCA of PCA... if linear this doesnt make sense. But why does it make sense for non-linear layers?\n",
    "* Not having tied weights, aka having more parameters may help training as the problem will be convex wrt to each weight.\n",
    "* So for any linear network $y = ABx$ we can rewrite it as $y = Cx$ where $C = A\\cdot B$. So any linear network is really just a one layer net? Or do we actually get something from depth in linear networks?\n",
    "* Relation to least squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs and PCA ... Baldi et al. 1989/2012\n",
    "\n",
    "### Summary\n",
    "Goals: \n",
    "1. Show that each critical point of $E(A,B)$ is some combination of principle components of $\\Sigma$, which are the eigen vectors of the covariance matrix of our data. (is that true?)\n",
    "* Show that the minium loss is achieved when the largest eigenvalues/leading eigenvectors are chosen/learnt.\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(A,B) = \\sum_t \\parallel y^t - ABx^t \\parallel \\tag{a} \\\\\n",
    "A^T(AB - \\Sigma_{YX}\\Sigma^{-1}_{XX}) = 0 \\\\\n",
    "S = AB - P_A \\Sigma_{YX}\\Sigma^{-1}_{XX} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "To show (1) they set A and solve for B given $E(A,B)$ is at a critical point (aka that the gradient of loss is zero) -- and vice versa. Using this result we find that AB is equal to the projection of the covariance YX and the inverse of covariance XX into a subspace of A (using a clever argument to show that $S = 0$, as S is both in the subspace of A and orthogonal to A). It follows that $\\Sigma$ is invariant to the columns of A, which is (roughly) the definition of an eigen vector. \n",
    "\n",
    "To show (2) we use a unitary change of coordinates, where the covariance XX is diagonal, to allow some nice rearranging. Because AB is a projection matrix we know that we can drop the aquared term, thus we can cancel we can simplity to the trace of $tr(\\Sigma) - tr(AB\\Sigma)$. Then using pertubation analysis of ...\n",
    "\n",
    "### Questions and notes\n",
    "\n",
    "* We did second order optimisation without computing the hessian. Interesting!\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions ... Saxe et al.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Goals:\n",
    "1. Describe the dynamics/trajectories of parameter updates\n",
    "* Investigate orthogonal weight initialisations\n",
    "* \n",
    "\n",
    "Results:\n",
    "1. \n",
    "\n",
    "\n",
    "> _Our fundamental goal is to understand the dynamics of learning as a function of the input statistics $\\Sigma_{XX}$ and input-output statistics $\\Sigma_{YX}$._\n",
    "\n",
    "Hmm. So this is the goal of all learning algorithms?\n",
    "\n",
    "> _To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where $\\Sigma_{XX} = I$. This assumption will hold exactly for whitened input data, a widely used preprocessing step._\n",
    "\n",
    "\n",
    "$\\Sigma_{YX} = U_{mxm}S_{mxn}V_{nxn}^T =\\sum^{n}_{i=1} s_iu_iv_i^T$\n",
    "\n",
    "But the indicies dont work out?? as there are m lots of $u_i$ vectors in U, but we are only looking at n of them. So either $n>m$ and $u_n$ doesnt exist, or $n<m$ and we have not used all of them. And u_i.v_i doesnt work?!? Oh... we are talking about the covariance matrix, so m = n... Wait are we?\n",
    "\n",
    "\n",
    "* $u_i$ reflect the independent modes of variation in the output,\n",
    "    * so U and $\\Sigma_{YX}$ are also decorrelated??\n",
    "* $v_i$ reflect the independent modes of variation in the input,\n",
    "* $s_i$ are the singular values -- which mean __?\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Why can we call the loss function an energy function? Can I do this for any loss function? What makes this one special?\n",
    "* What about cross entropy loss with softmax? What changes??\n",
    "* Dynamical isometry\n",
    "* Invariant manifold: where d/dt = 0 for some/all variables/parameters.\n",
    "* How much work (path integral of trajectory in the vector field) is done for different initialisations?\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random ideas\n",
    "\n",
    "\n",
    "### Orthogonal weights and rotating updates\n",
    "\n",
    "Initialise the weights to be orthonormal, then only update the weights using rotations. Why? As we dont want to bother learning orthogonal weights, the principle components (and normalising is just nice). As orthogonal weights are optimal (in non-linear systems as well????)\n",
    "\n",
    "```python\n",
    "weights = random.init(shape,'orthonormal')\n",
    "\n",
    "for batch in iter(batches):\n",
    "    grads = dL(weights,batch)\n",
    "    weights = rotate(weights,grads)\n",
    "```\n",
    "\n",
    "Forget about whether the loss function will impose this condition. Just add it ourselves.\n",
    "\n",
    "When is this a good/bad idea?\n",
    "* What if we cant whiten our data (is this even possible?)? I.e. if two variables are not separable? (well they probably wouldnt help us much anyway??)\n",
    "* \n",
    "\n",
    "### Attention modulated gradients\n",
    "\n",
    "Backrpop algol is wrong? Should be able to explain away residual prediction errors? Instead we just keep propagating them backward.\n",
    "\n",
    "Based on the predictive processing.\n",
    "Loss is dependent on amount of uncertainty.\n",
    "\n",
    "Inputs: \n",
    "* Prediction errors/residuals (which is like ??? surprise?) \n",
    "* Current hypotheses (which is like active learning)\n",
    "\n",
    "$f:X \\times Y \\rightarrow Z$\n",
    "\n",
    "$A(x,y) = ??$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions, thoughts and notes\n",
    "\n",
    "* Relation to game theory? Global optima = nash equilibrium, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
