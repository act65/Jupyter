{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation and goals\n",
    "\n",
    "Hunch. Resnets, fractal nets, ... ??? sculpt gradients to aid learning. \n",
    "We want to find a rigorous method of analysing gradients and use this to investigate why some networks learn better features than others.\n",
    "\n",
    "* How should gradients be used to correct errors and to learn?\n",
    "* What fundamental problems are there with using gradients to learn?\n",
    "    * Local minima\n",
    "    * Vanishing/exploding\n",
    "    * Only 1st order gradients are computationally feasible (?)\n",
    "    * ???\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear gradients\n",
    "\n",
    "So, an AE is trying to maximise the variance each middle node sees across the dataset. But they also interefere with eachother, competitively? Why do they learn this??? More variance captures more 'information'? Easier to separate?\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Does linear imply convexity?? Yes and no.(?)\n",
    "* How does the update rule imply we will learn the principle components?\n",
    "* If we initialised every weight to be the same, would it end up learning the same components?\n",
    "* Why does it learn perpendicular directions? Caputres more information, but how does it know that???\n",
    "* What do the biases mean??\n",
    "* Multi layer nets? Doing PCA of PCA... if linear this doesnt make sense. But why does it make sense for non-linear layers?\n",
    "* Not having tied weights, aka having more parameters may help training as the problem will be convex wrt to each weight.\n",
    "* So for any linear network $y = ABx$ we can rewrite it as $y = Cx$ where $C = A\\cdot B$. So any linear network is really just a one layer net? Or do we actually get something from depth in linear networks?\n",
    "* Relation to least squares?\n",
    "* \n",
    "\n",
    "\n",
    "##### Let's go through a smple 2d case (weights tied).\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{c}   y_1\\\\ y_2 \\\\ \\end{array} \\right]\n",
    "= \\begin{bmatrix} \n",
    "W_1 \\\\ \n",
    "W_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "W_1 & W_2 \\\\ \n",
    "\\end{bmatrix} \\left[ \\begin{array}{c}  x_1\\\\ x_2\\\\ \\end{array} \\right] \\\\\n",
    "= \\left[ \\begin{array}{c}  W_1(W_1x_1 + W_2x_2)   \\\\ W_2(W_1x_1 + W_2x_2) \\\\ \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= (\\textbf{x}-\\textbf{y})^2\\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "(x_1 - W_1(W_1x_1 + W_2x_2) )^2  \\\\ \n",
    "(x_2 - W_2(W_1x_1 + W_2x_2) )^2 \\\\ \n",
    "\\end{array} \\right] \\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "(x_1 - W_1^2x_1 - W_1W_2x_2)^2  \\\\ \n",
    "(x_2 - W_2W_1x_1 - W_2^2x_2)^2 \\\\ \\end{array} \\right] \\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "  \\\\ \n",
    " \\\\ \\end{array} \\right] \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs and PCA ... Baldi et al.\n",
    "\n",
    "where does _\"without local minima\"_ come into it?\n",
    "***\n",
    "Ok. I am confused about this loss function -- $E(W) = \\sum_t \\parallel y^t - Wx^t \\parallel$. So we just assume/know that the norm we are using is the 2-norm.\n",
    "\n",
    "$E(W) = \\sum_t \\parallel y^t - Wx^t \\parallel = ?? \\parallel Y - WX \\parallel$\n",
    "\n",
    "Assuming we are using the two norm, and assuming we can append all time steps - X,Y. We have  $E(W) = \\big(\\sqrt{\\sum_i (Y_i - w_iX_i^t)^2}\\big)^2 = \\sum_i (Y_i - w_iX_i^t)^2 = $\n",
    "\n",
    "***\n",
    "$$\n",
    "\\begin{align}\n",
    "E(W) &= \\sum_t \\parallel y_t - Wx_t \\parallel \\\\\n",
    "W &= \\Sigma_{YX}\\Sigma_{XX} \\tag{Is the optimal solution of E} \\\\\n",
    "&\\text{why?} \\\\\n",
    "&= \\sum_t \\parallel y_t - \\Sigma_{YX}\\Sigma_{XX}^{-1}x_t \\parallel \\tag{substituting}\\\\\n",
    "&=\\sum_t \\parallel y_t - (\\sum_i y_ix_i^T)(\\sum_j x_jx_j^T)^{-1}x_t \\parallel \\tag{more substituting}\\\\\n",
    "&= \\sum_t \\parallel y_t - (\\sum_i y_ix_i^T (x_jx_j^T)^{-1}x_t \\parallel \\tag{for i = j, as  _??}\\\\\n",
    "&= \\sum_t \\parallel y_t - (\\sum_i y_ix_i^T x_j^Tx_jx_t) \\parallel \\tag{as $(xx^{T})^{-1} = (xx^{T})^{T} = (x^{T}x)$ aka orthogonal}\\\\\n",
    "&= \\sum_t \\parallel y_t - y_tx_t^T x_t^Tx_tx_t \\parallel \\tag{i = j = t. not sure about this. move sum out of norm?} \\\\\n",
    "&= \\sum_t \\parallel y_t -  y_tx_t^T x_t^Tx_tx_t \\parallel \\tag{x^Tx} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hmm. The i=j=t step erquires your covarianve matrices to be over the entrire training dataset. Therefore new test data does not apply, not necessarily at the optimum.\n",
    "\n",
    "Wait. This doesnt show why $W = \\Sigma_{YX}\\Sigma_{XX}$ the minima, only that =0. But MSE must be greater than or equal to zero, therefore minimum.\n",
    "\n",
    "Hmm. So this is two seperate (by related??) covariance matrices we want to learn. \n",
    "***\n",
    "(supposed) fact:  E(A,B) is convex in the coefficients of B and attains its minimum for B satisfying $A^T\\Sigma_{YX} = A^T A B \\Sigma_{XX}$\n",
    "\n",
    "##### Definitions\n",
    "$y_t$ is some target, A and B are matrices (of rank p??) and $x_t$ is the input. $E(A,B) = \\sum_t \\parallel y_t - ABx_t\\parallel^2 $\n",
    "\n",
    "\\begin{align}\n",
    "E(A,B) &= \\sum_t \\parallel y_t - ABx_t\\parallel^2 \\\\\n",
    "&=\\parallel Y - ABX\\parallel^2 \\tag{stack all timesteps. assume 2-norm to move sum}\\\\\n",
    "&=(Y - ABX)^2 \\tag{not sure about this. sqrt and power cancel}\\\\\n",
    "&= (Y - ABX)^T (Y - ABX) \\tag{expand}\\\\\n",
    "&= Y^TY -Y^TABX - ABXY+(ABX)^TABX \\tag{expand} \\\\\n",
    "\\frac{\\partial E(A,B)}{\\partial B} &= \\frac{\\partial }{\\partial B} \\parallel Y - ABX\\parallel^2 = 0 \\\\\n",
    "&= Y^TY -Y^TABX - ABXY+(ABX)^TABX \\tag{$vec(PQR') = (R \\otimes P) vec Q$} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions ... Saxe et al.\n",
    "Andrew\n",
    "\n",
    "> _Our fundamental goal is to understand the dynamics of learning as a function of the input statistics $\\Sigma_{XX}$ and input-output statistics $\\Sigma_{YX}$._\n",
    "\n",
    "Hmm. So this is the goal of all learning algorithms?\n",
    "\n",
    "> _To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where $\\Sigma_{XX} = I$. This assumption will hold exactly for whitened input data, a widely used preprocessing step._\n",
    "\n",
    "\n",
    "$\\Sigma_{YX} = U_{mxm}S_{mxn}V_{nxn}^T =\\sum^{n}_{i=1} s_iu_iv_i^T$\n",
    "\n",
    "But the indicies dont work out?? as there are m lots of $u_i$ vectors in U, but we are only looking at n of them. So either $n>m$ and $u_n$ doesnt exist, or $n<m$ and we have not used all of them. And u_i.v_i doesnt work?!? Oh... we are talking about the covariance matrix, so m = n... Wait are we?\n",
    "\n",
    "\n",
    "* $u_i$ reflect the independent modes of variation in the output,\n",
    "    * so U and $\\Sigma_{YX}$ are also decorrelated??\n",
    "* $v_i$ reflect the independent modes of variation in the input,\n",
    "* $s_i$ are the singular values -- which mean __?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random ideas\n",
    "\n",
    "\n",
    "### Orthogonal weights and rotating updates\n",
    "\n",
    "Initialise the weights to be orthonormal, then only update the weights using rotations. Why? As we dont want to bother learning orthogonal weights, the principle components (and normalising is just nice). As orthogonal weights are optimal (in non-linear systems as well????)\n",
    "\n",
    "```python\n",
    "weights = random.init(shape,'orthonormal')\n",
    "\n",
    "for e in range(epochs):\n",
    "    grads = dL_dw(weights,batch)\n",
    "    \n",
    "    \n",
    "    weights = np.dot(weights,???) #must be matmul to rotate?\n",
    "\n",
    "```\n",
    "\n",
    "### Attention modulated gradients\n",
    "\n",
    "Backrpop algol is wrong? Should be able to explain away residual prediction errors? Instead we just keep propagating them backward.\n",
    "\n",
    "Based on the predictive processing.\n",
    "Loss is dependent on amount of uncertainty.\n",
    "\n",
    "Inputs: \n",
    "* Prediction errors/residuals (which is like ??? surprise?) \n",
    "* Current hypotheses (which is like active learning)\n",
    "\n",
    "$f:X \\times Y \\rightarrow Z$\n",
    "\n",
    "$A(x,y) = ??$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Questions, thoughts and notes\n",
    "\n",
    "* Relation to game theory? Global optima = nash equilibrium, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
