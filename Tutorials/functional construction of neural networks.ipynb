{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursion\n",
    "\n",
    "This seems similar to boosting. We are (recusrively) adding on an extra (residual) learner. However the only difference between resnets and gradient boosting is when/where they add them. What difference does this make? Or does it reduce down to the same thing?\n",
    "\n",
    "Let just have a play with using recusrion to define some algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda z:1/(1+np.exp(-z))\n",
    "#A simple on layer NN function to play with\n",
    "class Net():\n",
    "    count = 0\n",
    "    def __init__(self,size):\n",
    "        Net.count += 1; self.name = str(Net.count)\n",
    "        \n",
    "        self.weights = np.random.standard_normal(size)\n",
    "        self.biases = np.zeros((size[1],1))\n",
    "    def __call__(self,x):\n",
    "        return sigmoid(np.dot(self.weights,x) + self.biases)\n",
    "    def __repr__(self):\n",
    "        return 'f' + self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A recursive function that will apply a list of functions, F, to some data,\n",
    "#as according to some mapping\n",
    "def Unroll(F,x,mapping): ## this is actually just fold...\n",
    "    if len(F) == 1:\n",
    "        return mapping(x,F[0])\n",
    "    else:\n",
    "        return Unroll(F[1:],mapping(x,F[0]),mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Images/Unrollmapper.png' height = 400 width=400>\n",
    "\n",
    "* What if we swapped around the mappers and the functions?\n",
    "* It's cool how we can look at the functions as objects like this.\n",
    "\n",
    "What are these maps really doing? It gives us a way to think about how we combine variables and functions (which is really just another function). How does this help us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Interesting cases\n",
    "ff_map = lambda z,f: f(z) #a feedforward net\n",
    "skip_map = lambda z,f: f(z)+z #a Resnet\n",
    "rnn = lambda h,x: f(x) + h #a RNN\n",
    "boost = lambda z,f: f(x) + z #boosting\n",
    "\n",
    "#others?\n",
    "#mul_map = lambda f,x: x * f(x)  ???\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[f1, f2, f3, f4, f5]\n"
     ]
    }
   ],
   "source": [
    "###inits\n",
    "#some input\n",
    "x = np.random.standard_normal((3,3)) \n",
    "\n",
    "#a list of functions/layers\n",
    "F = [Net((3,3)) for i in range(5)]  # = [f_1,f_2,f_3... f_n]\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21072356  0.21826266  0.21358719]\n",
      " [ 0.21964227  0.21001793  0.21359689]\n",
      " [ 0.34807249  0.34477761  0.34626691]]\n"
     ]
    }
   ],
   "source": [
    "#A feedforward net\n",
    "print(Unroll(F,x,ff_map)) # =  f_n(... f_3(f_3(f_1(x))) ...)\n",
    "\n",
    "#=  f_3(  f_2(  f_1(y_1,y_2)  ,y_3)  ,y_4)\n",
    "#!! so if we set y_1 = the target\n",
    "#then we get  f_3(  f_2(  loss(target,out_1),  out_2)  ,out_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.20512124  0.22183679  0.91949788]\n",
      " [ 0.43095705  2.70578206  3.64931753]\n",
      " [ 1.66645688  1.53153556  2.51976068]]\n"
     ]
    }
   ],
   "source": [
    "#A ResNet\n",
    "print(Unroll(F,x,skip_map)) # = f_n( ... f_3(f_2(f_1(x) + x) + f_1(x) + x) f_2(x) + f_1(x) + x ) + ... complicated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.72497842  2.42460916  0.06777119]\n",
      " [ 2.00572831  2.576681    4.35027651]\n",
      " [ 3.89167146  0.07942172  1.81858963]]\n"
     ]
    }
   ],
   "source": [
    "#A RNN\n",
    "g = Net((3,3))\n",
    "G = [g for i in range(5)]#f is the same across time\n",
    "print(Unroll(G,x,rnn))\n",
    "\n",
    "#ahh, doesnt quite work. x should be changing with each layer\n",
    "\n",
    "#what about a multilayer RNN??\n",
    "#we are passing information in two directions now.\n",
    "#across in time and down in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.32100509  2.1610769   1.56781679]\n",
      " [ 2.06170987  3.81795911  3.43916054]\n",
      " [ 2.10083201  3.61580354  3.26092478]]\n"
     ]
    }
   ],
   "source": [
    "#Boosting\n",
    "print(Unroll(F,0,boost)) # = f_1(x_1) + f_2(x_1) + f_3(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of function outputs = \n",
      " [[ 3.32100509  2.1610769   1.56781679]\n",
      " [ 2.06170987  3.81795911  3.43916054]\n",
      " [ 2.10083201  3.61580354  3.26092478]]\n",
      "Map reduce of addition onto function outputs \n",
      " [[ 3.32100509  2.1610769   1.56781679]\n",
      " [ 2.06170987  3.81795911  3.43916054]\n",
      " [ 2.10083201  3.61580354  3.26092478]]\n"
     ]
    }
   ],
   "source": [
    "func_out = list(map(lambda f:f(x),F))\n",
    "print('Sum of function outputs = \\n',sum(func_out ))\n",
    "from functools import reduce\n",
    "print('Map reduce of addition onto function outputs \\n',reduce(lambda x,y:x+y,func_out ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward = \n",
      " [[ 3.20512124  0.22183679  0.91949788]\n",
      " [ 0.43095705  2.70578206  3.64931753]\n",
      " [ 1.66645688  1.53153556  2.51976068]]\n",
      "Backward =\n",
      " [[ 3.71010021  2.81894675  3.96976147]\n",
      " [ 2.60590452  2.75511983  1.90141416]\n",
      " [ 2.12412577  2.15911346  2.05853539]]\n"
     ]
    }
   ],
   "source": [
    "### Resnet\n",
    "#forward\n",
    "print('Forward = \\n',reduce(skip_map, [x]+F))\n",
    "#backward\n",
    "dL = np.random.random((3,3))\n",
    "print('Backward =\\n',reduce(skip_map, [dL] + list(reversed(F))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward = \n",
      " [[ 1.77422587  1.61150717  1.55036097]\n",
      " [ 2.45322177  2.15327711  1.73290924]\n",
      " [ 3.59526762  2.95965196  3.28490744]]\n"
     ]
    }
   ],
   "source": [
    "### RNN\n",
    "#forward\n",
    "rnn = lambda h,z:g(z) + h\n",
    "inputs = [np.random.random((3,3)) for i in range(5)]\n",
    "print('Forward = \\n',reduce(rnn, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick implemetation of a ResNet using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self,num):\n",
    "        self.w = 'W' + str(num)\n",
    "        self.b = 'b' + str(num)\n",
    "    def __call__(self,params,x):\n",
    "        return x + sigmoid(np.dot(x,params[self.w]) + params[self.b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_net():\n",
    "    params = {}; Net = []\n",
    "    for i in range(10):\n",
    "        params['W' + str(i)] = np.random.standard_normal((3,3))\n",
    "        params['b' + str(i)] = np.zeros((1,3))\n",
    "        Net.append(Layer(i))\n",
    "    return params,Net\n",
    "\n",
    "params,Net = generate_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(x,Net,params):\n",
    "    #the forward propagation through the net\n",
    "    output = x\n",
    "    for layer in Net:\n",
    "        output = layer(params,output)\n",
    "    return output\n",
    "\n",
    "def Loss(params,ims,labels):\n",
    "    #just calculate the squared error\n",
    "    return np.sum((labels - predict(ims,Net,params))**2)\n",
    "dL = grad(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.300333652\n",
      "79.1481480023\n",
      "66.0446403205\n",
      "55.8333736782\n",
      "23.491532155\n",
      "25.433560046\n",
      "32.0363912341\n",
      "15.0434883477\n",
      "17.7814038765\n",
      "23.4146620214\n"
     ]
    }
   ],
   "source": [
    "#some data...\n",
    "x = np.random.random((10,3))\n",
    "target = np.random.random((10,3))\n",
    "\n",
    "def update(epochs = 10,learning_rate = 0.01):\n",
    "    #for each epoch\n",
    "    for e in range(epochs):\n",
    "        #for each batch\n",
    "        for i in range(10):\n",
    "            batch = np.random.standard_normal((10,3))\n",
    "            dL_dparams = dL(params,batch,target)\n",
    "            #update each parameter\n",
    "            for key in params:\n",
    "                params[key] -= learning_rate * dL_dparams[key] #SGD\n",
    "            \n",
    "        print(Loss(params,batch,target))\n",
    "    return params\n",
    "params = update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
