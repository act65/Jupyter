{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learners need to be able to learn simple things first and then build upon them, it is easier to learn.\n",
    "\n",
    "What is the 'best' way to go from simple to complex?\n",
    "Best:= most efficient? best representation? most transferable?\n",
    "\n",
    "* Add noise (simulated annealing)\n",
    "* Mollifying/curriculum\n",
    "* Distill []()\n",
    "* Integrate nets with stochastic paths (drop-path/swapout) []()\n",
    "* Initialise []() (ID layer?)\n",
    "* Matrix decomposition/iterative []() -- this can be dont better?\n",
    "\n",
    "(alternatively, ways to compose learners/loss fns)\n",
    "take existing knowlege and build on it; (to share/transfer knowledge)\n",
    "\n",
    "Ways to decompose complex tasks into simple ones;\n",
    "* \n",
    "\n",
    "\n",
    "##### Docomposition\n",
    "\n",
    "Test on Linear decomposition\n",
    "```python\n",
    "net = Net.init(depth=1)\n",
    "while learning:\n",
    "    gnvs = opt.compute_gradients()\n",
    "    \n",
    "    for grad,var in gnvs: #for each layer\n",
    "        if norm(grad) < tol: #will need some sort of average\n",
    "            A,B = decompose(var)\n",
    "            Net.set_layer(var, A,B)\n",
    "    \n",
    "    Net.apply_gradients(gnvs)\n",
    "    \n",
    "    if valid_acc - test_acc > 0: #will need early stopping\n",
    "        #restore last\n",
    "        break\n",
    "```\n",
    "\n",
    "* When to split?\n",
    "* Learning capacity.\n",
    "* How to grow net? One at a time? Exp growing net -- Factoring all layers?\n",
    "* Non-negative matrix factorisation? Thus can get through relu if all inputs are positive\n",
    "* Which factorisation/decomposition? SVD factorisaiton?\n",
    "\n",
    "\n",
    "##### Conv decomposition\n",
    "\n",
    "\n",
    "\n",
    "##### Saddles\n",
    "\n",
    "* Does it break saddles?!?! Kawaguchi? Saddles are obstacles, which can be traversed easir in high dims?!?\n",
    "* What does this decomposition buy us?\n",
    "* How does it help us get around obstacles in/on the loss surface?\n",
    "\n",
    "\n",
    "### Thoughts/questions/notes/other\n",
    "\n",
    "* This seems deeply connected to dynamic programming?\n",
    "* The way the fractal/dense/resnets use/learn paths of different length gives compositionality of learned features. Each layer learns to do something wrt to the data, we just need to find the right set of paths through them to do something useful.\n",
    "* What about making a network that could figure out when to decompose its layers? (if error wrt to layer has reached ??? - low or high? - then decompose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
