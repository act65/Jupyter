{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* We dont know what actions to take. \n",
    "    * We dont know how they will effect the environment,\n",
    "    * and whether they will get us rewards. \n",
    "* So what we want is to know;\n",
    "    * what action we should take given some state we are in, $f:\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathcal{R}$. \n",
    "    * or equivalently, the reward associated with each state, and how to get to each of those states.$v:\\mathcal{S}\\rightarrow \\mathcal{R}$ $t:\\mathcal{S}\\times \\mathcal{A}\\rightarrow \\mathcal{S}$\n",
    "    * or ???\n",
    "\n",
    "Most algorithms (I have seen) just focus on the latter problem and assume the former (e.g. we are commonly just given the transition function). Then we just need to learn to model the reward function so we can predict which actions will give us rewards. Learning a 'value' estimation function somewhat achieves this goal by ???. \n",
    "\n",
    "If we dont assume that we know the transition function then we have. \n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "> _Playing football, I have the ball at my feet, and a defender infront of me. What should I do? So, using my past experiences I predict that the defender will not dive in, I also predict that given my position, other defenders will continue to mark my team mates and not double team me. _\n",
    "\n",
    "So what have I just done? I used my experiences to esitmate what would happen in the future (given my actions/inactions. Aka a transition function. \n",
    "\n",
    "> _Imagine. You are being tested, if you fail you die. Every 2 hours you are told good/bad. What do you do?_\n",
    "\n",
    "* What am I being tested on (some actions or all, what am I supposed to be doing)?\n",
    "* When does the test finish?\n",
    "* Will I fail if I make a mistake (am I allowed to explore)?\n",
    "* Are you allowed to change the test (online)?\n",
    "* Can/do you lie (untrust worthy rewards)?\n",
    "* \n",
    "\n",
    "### Rewards\n",
    "\n",
    "\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <th>State</th>\n",
    "    <th>Action</th>\n",
    "    <th>Reward</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> - </td>\n",
    "    <td>Eat sugar</td>\n",
    "    <td>Dopamine</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Popular product</td>\n",
    "    <td> - </td>\n",
    "    <td>Money</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td></td>\n",
    "    <td>Hard work</td>\n",
    "    <td>Consistency bias</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Subgoals and heuristics\n",
    "\n",
    "What Stanovich says about building a autonomos robot. Need it to be able to reason in real time so we program it with ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific method\n",
    "\n",
    "Reinforcement learning requires the scientific method?\n",
    "\n",
    "* Design/generate new hypotheses based on previous results (normally just something random...)\n",
    "* Test hypotheses.\n",
    "* Repeat.\n",
    "\n",
    "*****\n",
    "\n",
    "* What about maknig predictions and falsifying? Related to a predictive autoencoder?\n",
    "* Based on previous results is hard. How can we intelligently design hypotheses?\n",
    "    * What should be the goal of hypothesis design (or querying).\n",
    "        * To determine the relation between inputs (observations) and outputs (actions)? Learn a transfer function.\n",
    "        * To determine the relation between inputs (observations) and loss (reward)? Learn a value function.\n",
    "        * ???\n",
    "        \n",
    "Learning the transfer function reminds me of unsupervised learnings role in semi-supervised learning. (?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDPs\n",
    "\n",
    "$$V^{(n+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(n)}(s')]$$\n",
    "\n",
    "Hmm. The discounted value iteration at n reminds me of momentum. Oh, it also allows us to choose nothing. To stay in the same spot? But as it is discounted polynomially, $\\gamma^n$, it would force us to explore?\n",
    "\n",
    "\n",
    "Value = the maximum of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "So, we have $Q(s,a) = \\sum $\n",
    "\n",
    "Does this make sense with in continuious control??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradients\n",
    "\n",
    "```python\n",
    "for i in range(iters):\n",
    "    for run in range(batch_size):\n",
    "        actions[run,:],reward[run] = env.rollout()\n",
    "    \n",
    "    d_Loss = #so we need to be able to symbolically differentiate our loss?\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> _If you think through this process you’ll start to find a few funny properties. For example what if we made a good action in frame 50 (bouncing the ball back correctly), but then missed the ball in frame 150? If every single action is now labeled as bad (because we lost), wouldn’t that discourage the correct bounce on frame 50? You’re right - it would. However, when you consider the process over thousands/millions of games, then doing the first bounce correctly makes you slightly more likely to win down the road, so on average you’ll see more positive than negative updates for the correct bounce and your policy will end up doing the right thing._ [Karpathy](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "What a horrible algorithm... It doesnt try to (intelligently) find any patterns. Other than using basic linear statisctics using SGD enmasse.\n",
    "\n",
    "> One good idea is to “standardize” these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way we’re always encouraging and discouraging roughly half of the performed actions.\n",
    "\n",
    "there must be a smarter way to do this? i mean, 9/10ths of the decisions could be total crap, and yet we would positively update with 4/10ths of them. We really need some expectation of what a good reward is, then using this we can (more intelligently) choose which rewards to positively update.\n",
    "\n",
    "$$\n",
    "a^* = \\mathop{argmax}_a \\;\\mathbb{E}_{a \\sim p_{\\phi}(a\\mid s,h)} \\big[ v_{\\theta}(a,s,h)\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted rewards\n",
    "\n",
    "So at each time step we are given a reward, however. We dont want to weight each action the same $R_t = \\sum r_i$. So instead we let $R_t = \\sum \\gamma^i r_i$. So for typical vaules of gamma (=0.999), ... . Aka, exponential decay in the importance of later actions. \n",
    "\n",
    "But how is this ever true in the real world? I guess if we look at the decisions as a tree, then eariler decisions choose between exponentially more alternatives, thus making them more important.\n",
    "\n",
    "Doesn't this make it hard to learn later actions? What if the optimal solutions only requires one action near t=T. And the rest dont matter, this would be super hard to learn... Maybe we could adjust the decay as learning proceeds, so that initially we focus on earlier moves, but one we can do them we focus on later moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-differentiable\n",
    "\n",
    "Differentiable vs non-differentiable -- convex vs non-convex? (same sort of thing, on is easy, the other is hard... but we can transfer techniques?)\n",
    "\n",
    "* So is RL for non-differentiable problems? \n",
    "* If I could make it differentiable then it would just be supervised learning?\n",
    "* Is there a weaker form of differentiability/continuity that could allow us to 'guess' at how changes in actions would effect the loss. I guess this is what we are learning...\n",
    "\n",
    "\n",
    "The problem (at least my my first couple of attempts) is that you have to sample from some action distribution, thus it isnt differentiable. So we cannot differentiate our loss w.r.t the output action as the output action was sampled. Why cant we just use the probabilities, like supervised learning (where we sample/argmax for the label)? It should end up working out with (lots of) SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts/questions/notes\n",
    "\n",
    "* Online RL?!? Will need to update before we have 'finished'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
