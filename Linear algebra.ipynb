{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Goal: Explore the basics of Linear Algebra and its relations to: groups, representations, graphs, probability, analysis, geometry, algebra, learning, ... ???_\n",
    "\n",
    "Desiderata;\n",
    "* Intuit the geometry\n",
    "* Analyse the algorithms\n",
    "* Understand what makes linear algebra the right level of abstraction and power and why this is important.\n",
    "* How do NNs fit into this?\n",
    "\n",
    "\n",
    "> Linear algebra is the language of ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basis for representation\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix} &=\n",
    "a\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} +\n",
    "b\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\tag{linear combination of bases}\\\\\n",
    "&= a \\hat i + b \\hat j \\\\\n",
    "&= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "So the vector form is really saying that we have a linear combination of basis vectors, which in the default case are the unit vectors. The span of two vectors $\\vec u, \\vec v$  is the set of all their linear combinations $a\\vec u + b\\vec v: a,b \\in \\mathbb R$ (in R or ??). Aka what are all the points we can reach using only scalar multiplication and vector addition of basis vectors?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2  \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "a_{11}b_1 + a_{12}b_2 \\\\\n",
    "a_{21}b_1 + a_{22}b_2 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= b_1 \\vec a_{\\cdot, 1}  + b_2 \\vec a_{\\cdot, 2} \\tag{a linear combination of columns of $A$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can pick any set of basis vectors, $\\vec u, \\vec v$, as long as they are not pointing in the same direction ($\\not \\exists c: c\\vec v = \\vec u$). \n",
    "\n",
    "***\n",
    "\n",
    "* Which bases are the best?\n",
    "* What if our bases are non-linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowable (linear) transformations\n",
    "\n",
    "Needs to be additive f(u,v) = f(u) + f(v) and ?? f(c.u) = c.f(u).\n",
    "Which is the same as saying that grid lines must stay parallel and evenly spaced ??!?!\n",
    "\n",
    "The nice thing is that any transformation can be completely described by what happens to the two basis vectors.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\vec u &= a\\vec i + b\\vec j \\\\\n",
    "f(\\vec u) &= \\vec v \\\\\n",
    "&= f(a\\vec i + b\\vec j) \\\\\n",
    "&= af(\\vec i) + bf(\\vec j) \\tag{? what is his property called?}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Grid lines must stay parallel and evenly spaced. Left $\\mathcal L = \\{ f\\}$ be the set of functions that do a linear transform on their inputs\n",
    "\n",
    "$\\forall f,g \\in \\mathcal L \\exists h: f(g(\\vec u)) = h(\\vec u)$\n",
    "A unique property to linear algebra because of how we have restricted $f,g$. \n",
    "\n",
    "_Note that $f\\circ g$ is not equivalent to $h$, only equal to._\n",
    "\n",
    "\n",
    "## Dot product\n",
    "\n",
    "Is the similarity between two vectors. How much are they pointing in the same direction?\n",
    "\n",
    "$$\n",
    "a \\cdot b = \\parallel a \\parallel_2 \\parallel  b \\parallel_2 cos (\\phi) \\tag{proof?!?}\n",
    "$$\n",
    "\n",
    "\n",
    "A 2x1 linear transform (2 dims to 1) == the dot product. The equivalence of a vector and a n x 1 linear transform. Awesome duality. The dual of a vector is the linear transofrmation that it encodes. Function = representation.\n",
    "\n",
    "## Cross product\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "v_{3} \\\\\n",
    "\\end{bmatrix}\\times\n",
    "\\begin{bmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "w_{3} \\\\\n",
    "\\end{bmatrix}\n",
    "&= \\begin{bmatrix}\n",
    "p_{1} \\\\\n",
    "p_{2} \\\\\n",
    "p_{3} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "?? &= det\\Big(\\begin{bmatrix}\n",
    "x & v_{1} & w_{1}\\\\\n",
    "y & v_{2} & w_{2}\\\\\n",
    "z & v_{3} & w_{3}\\\\\n",
    "\\end{bmatrix}\\Big) \\tag{f(x) such that ?}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "p_{1} \\\\\n",
    "p_{2} \\\\\n",
    "p_{3} \\\\\n",
    "\\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "y_{2} \\\\\n",
    "z_{3} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We know that det(A(x)) is a linear function that takes x to some 1-dim value. Therefore there exists some vector p that also does this.\n",
    "\n",
    "\n",
    "## Outer product\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Matmul\n",
    "\n",
    "If $C = AB$ then we are taking the dot product of rows of A and columns of B. So each element of C is a measure of similarity between rows/columns of A/B. So, the elements of the first row of C are the similarities between the first row of A and each column of B.\n",
    "\n",
    "A matrix multiplication by a vector is simply a linear combination of the column space. This implies that $rank(A) \\leq rank(M)$ (proof?)\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "b_{11}\\vec a_{\\cdot,1} + b_{21}\\vec a_{\\cdot,2} &\n",
    " b_{12}\\vec a_{\\cdot,1} + b_{22}\\vec a_{\\cdot,2}\\\\\n",
    "\\end{bmatrix}\\tag{columns}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "a_{11}\\vec b_{1,\\cdot} + a_{12}\\vec b_{2,\\cdot} \\\\\n",
    "a_{21}\\vec b_{1,\\cdot} + a_{22}\\vec b_{2,\\cdot}\\\\\n",
    "\\end{bmatrix}\\tag{rows}\\\\\n",
    "&=\\begin{bmatrix}\n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{21} + a_{22}b_{22} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* Hmm. So which is the 'basis' vectors in this case? The rows or the columns?\n",
    "* Can we do the same for 3 matricies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change of basis\n",
    "\n",
    "Let $\\vec v = \\begin{bmatrix}v_1 \\\\v_1 \\\\\\end{bmatrix}$ be a vector. Where $v_1$ describes the amount to scale $\\hat i$ and $v_2$ describes how much to scale $\\hat j$. The implicit assumptions of this vectors are;\n",
    "\n",
    "* First coordinate is $\\hat i$.\n",
    "* Second coordinate is $\\hat j$.\n",
    "* The unit of distance = U\n",
    "* \n",
    "\n",
    "It is like we are talking different languages. But we can easily translate them. Alternatively, we each look at the world a different way.\n",
    "\n",
    "So,  let $\\vec u$ be some vector. We can describe $\\vec u$ in the canonical basis (i,j) as $\\vec u_{\\mathcal C} = \\begin{bmatrix}-4 \\\\1 \\\\\\end{bmatrix}$. But we could also describe  in a different basis. For example let $\\mathcal B: \\hat i = \\begin{bmatrix}2 \\\\1 \\\\\\end{bmatrix}, \\hat j \\begin{bmatrix}-1 \\\\1 \\\\\\end{bmatrix}$. Then $\\vec u_{\\mathcal B} = \\begin{bmatrix}\\frac{5}{3} \\\\ \\frac{1}{3} \\\\\\end{bmatrix}$. So we know that $\\vec u_{\\mathcal C} \\sim \\vec u_{\\mathcal B}$ as they are both talking about the same vector $\\vec u$.\n",
    "\n",
    "### Translating a matrix\n",
    "\n",
    "We have a transform/matrix $M$ in the canonical basis $\\mathcal C$. But we want to know how to apply it to a vector $\\vec u_{\\mathcal B}$ in a different basis $\\mathcal B$. \n",
    "\n",
    "* First we need to translate into our language\n",
    "* Apply transform in our language\n",
    "* Translate back into their langauge\n",
    "\n",
    "\\begin{align*}\n",
    "M &= \\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "M_{\\mathcal C}(\\vec u_{\\mathcal B}) &= T^{-1}M_{\\mathcal C}T\\vec u_{\\mathcal B} \\\\\n",
    "\\implies M_{\\mathcal B} &= T^{-1}M_{\\mathcal C}T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vectors and values\n",
    "\n",
    "For any vector being transform linearlly, the chance that it is mapped back onto the same span than the initial vector was on is small. But for some small set of vectors, this occurs. We call these eigenvectors.\n",
    "\n",
    "These are equivalent to the axes of a rotation ?!!?!\n",
    "\n",
    "\\begin{align*}\n",
    "A\\vec v &= \\lambda \\vec v \\tag{}\\\\\n",
    "&= (\\lambda I) \\vec v \\\\\n",
    "A\\vec v  - (\\lambda I) \\vec v &= \\textbf{0}\\\\\n",
    "\\Big(A  - (\\lambda I) \\Big) \\vec v &= \\textbf{0}\\\\\n",
    "det\\Big(A  - (\\lambda I) \\Big)  &= \\textbf{0} \\tag{either $\\vec v$ = 0 or }\\\\\n",
    "\\end{align*}\n",
    "\n",
    "If Ax = 0 then we know that A must have squished x into a lower dimension (how does this work? not necessarily true?). Therefor a sufficient condition is that det(A) = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank and Linear independence\n",
    "\n",
    "If we have two vectors, $x,y \\in \\mathbb{R}$, such that $x = [1,4], y = [2,8]$ then these vectors are _linearly dependent_ as y = 2x. Therefore, by composing x and y we can only ever get vectors on a line. However, \n",
    "\n",
    "\n",
    "* Do that proof thingy of rows = columns.\n",
    "* Random init = full rank with probability 1 ?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinant\n",
    "\n",
    "$A_{initial} =$ area within unit square defined by i,j. \n",
    "Transform the space by some M.\n",
    "$A_{final}$ area within Mi x Mj.\n",
    "Determinant = $\\frac{A_{final}}{A_{initial}}$\n",
    "\n",
    "Equivalent to the cross product?\n",
    "Also, determinant is maximised for orthogonal column space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal matrix\n",
    "\n",
    "$A^T = A^{-1}$ Proof?\n",
    "\n",
    "Why are covariance matrices orthogonal?\n",
    "\n",
    "> _\"Orthogonality and statistical independence are not synonyms.\"_ [SE](http://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)\n",
    "\n",
    "### Orthogonal projections \n",
    "\n",
    "Let x be some vector and L be a subspace such that $L = span(v) = \\{cv : c \\in \\mathbb{R}\\}$. Then the projection of x onto L, $proj_L(x) = \\frac{x\\cdot v}{v \\cdot v}v$\n",
    "\n",
    "$P_A = P_A^2$ <- proof??\n",
    "\n",
    "$P_A = A^T(A^TA)^{-1}A$\n",
    "\n",
    "\n",
    "Let U = (mxm), S = (mxn), V^T = (nxn)\n",
    "$$\n",
    "\\begin{align}\n",
    "P_A &= A(A^TA)^{-1}A^T \\\\\n",
    "M &= USV^T \\\\\n",
    "P_A &= (USV^T) ((USV^T)^T(USV^T))^{-1} (USV^T)^T \\\\\n",
    "P_A &= USV^T (VS^TU^TUSV^T)^{-1} (VS^TU^T) \\\\\n",
    "P_A &= USV^T (VS^2V^T)^{-1} (VS^TU^T) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor product\n",
    "\n",
    "> This is what we mean when we combine two independent states.\n",
    "\n",
    "## Kronecker product\n",
    "\n",
    "\n",
    "$$A\\otimes B = \n",
    "\\begin{bmatrix}\n",
    "a_{11}B & ... & a_{1n}B \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}B & ... & a_{mn}B \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```julia\n",
    "function kronecker(A::Array,B::Array)\n",
    "    n,m = size(A)\n",
    "    p,q = size(B)\n",
    "    C = zeros((n*p,q*m))\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            C[ p*(i-1)+1:p*i , q*(j-1)+1:q*j] += A[i,j].*B\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "```\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "Show that\n",
    "* $(A \\otimes B)(C \\otimes D) = AC \\otimes BD$\n",
    "* $vec(ABC) = (C^T \\otimes A ) vec(B)$\n",
    "* $AX=B \\implies (I\\otimes A)vec(X) = vec(B)$\n",
    "* $A\\otimes B = U_A\\Sigma_A V_A^T \\otimes U_B\\Sigma_B V_B^T = (U_A \\otimes U_B)(\\Sigma_A \\otimes \\Sigma_B )(V_A^T \\otimes V_B^T)$ with some reordering?!?\n",
    "* $tr(x \\otimes y) = x \\cdot y$ if x and y are the same shape????\n",
    "\n",
    "## Generalised tensor product\n",
    "\n",
    "$A\\in \\mathbb R^{n_1,\\dots,n_P}, B\\in \\mathbb R^{n_{P+1},\\dots,n_{Q+P}}, A\\otimes_g B \\in \\mathbb R^{n_1,\\dots,n_{P+Q}}$\n",
    "\n",
    "$g : \\mathbb R \\times \\mathbb R \\rightarrow \\mathbb R$ such that $\\forall a, b, c \\in \\mathbb R : g(g(a, b), c) = g(a, g(b, c))$ and $\\forall a, b ∈ \\mathbb R : g(a, b) = g(b, a)$\n",
    "\n",
    "\\begin{align*}\n",
    "(A\\otimes B)_{i_1,\\dots,i_{P+Q}} &= (a_{i_1,\\dots,i_{P}}b_{i_{P+1},\\dots,i_{P+Q}})\\\\\n",
    "(A\\otimes_g B)_{i,j} &= g(a_i,b_j) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "***\n",
    "* A matmul is a special case (?). By summing over a set of the ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vecs/vals\n",
    "\n",
    "Now that we have the covariances between variables/features of X, we can find their eigen values/vectors. Eigenvectors of a matrix, M, are directions that are not rotated when multiplied by M -- $\\mathbf{A}\\vec{x} = a\\vec{x}$. So, the eigenvectors of our covariance matrix are directions that ???\n",
    "\n",
    "Why does it even make sense that there are m eigen vectors? It seems weird that there must be directions that are invariant to rotation. Why should there be eigenvectors? It seems kinda strange.\n",
    "\n",
    "$Ax = ax \\implies (A-aI)x = 0$\n",
    "\n",
    "* Hmm. Eigen vectors are not unique??\n",
    "* What does it mean when eigen values are negative. \n",
    "    * Given small positive diagonals and large off-diagonals.\n",
    "    * Given negative diagonals. And small off-diagonals.\n",
    "* or imaginary?\n",
    "\n",
    "Why does covar matrix have positive eigen values, but symetric ones dont necessarily.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Av &= \\lambda v\\\\\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix}\\tag{let A = ...}\\\\\n",
    "\\implies 0 &= Av-\\lambda v \\\\\n",
    "&= (A-\\lambda I)v \\\\\n",
    "&= det(A-\\lambda I) \\tag{why?!?} \\\\\n",
    "0& = det(\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "-\\begin{bmatrix}\n",
    "\\lambda & 0 \\\\ \n",
    "0 & \\lambda \\\\\n",
    "\\end{bmatrix})\\\\\n",
    "0&= (a_{11}-\\lambda)(a_{22}-\\lambda) - a_{12}a_{21} \\\\\n",
    "&= a_{11}a_{22} - a_{11}\\lambda - a_{22}\\lambda + \\lambda^2  \\\\\n",
    "&= \\lambda^2 + (-a_{11} - a_{22})\\lambda + (- a_{12}a_{21})\\\\\n",
    "x&={\\frac {-b\\pm {\\sqrt {b^{2}-4ac\\ }}}{2a}}\\\\\n",
    "&={\\frac {-(-a_{11} - a_{22})\\pm {\\sqrt {(-a_{11} - a_{22})^{2}-4(- a_{12}a_{21})\\ }}}{2}}\\\\\n",
    "&={\\frac {a_{11} + a_{22}\\pm {\\sqrt {a_{11}^2 -2a_{11}a_{22}+ a_{22}^2+4 a_{12}a_{21}\\ }}}{2}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But what does this mean intuitively? What is the geomentric interpretation?\n",
    "* Well, just diagonal entries scale the vector.\n",
    "* Off-diagonal entries add in information from other dimensions.\n",
    "\n",
    "What about some matrix transforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional interpretations\n",
    "\n",
    "Eigen functions, inner functions, ...?!?\n",
    "\n",
    "## Indexing\n",
    "\n",
    "Indexing as function calls $a(11) = a_{11}$. Or as a special type class ? Where each index is a component of the vecotr and all components are independent (but maybe not independent in the other sense).\n",
    "\n",
    "Which leads to the quantum funny business. Interesting that it is the indexes getting entangled with each other, not the elements. Hmm.!\n",
    "\n",
    "\n",
    "What if we have a vector $\\vec v$ that is indexed by $\\mathbb R$? We could write this as $v(i): i \\in \\mathbb R$. Then $v + u = v(i) + u(i)$ and $2v = \\forall 2v(i)$.\n",
    "\n",
    "\n",
    "\n",
    "## Functional linear algebra \n",
    "\n",
    "\n",
    "## Functional linear transform\n",
    "\n",
    "Want a function $f:\\mathbb R^n \\rightarrow \\mathbb R^n$ that is a linear transform.\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2  \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2  \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= f(\\vec b)\n",
    "\\end{align*}\n",
    "\n",
    "We could imagine this function mapping one vector to another. But how? Does it use a look up table, or continuiously push the input to the output? Or ??\n",
    "\n",
    "\n",
    "## Functional analaysis \n",
    "\n",
    "infinite vector spaces\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive semi definite\n",
    "***\n",
    "> _\"positive definiteness is a sufficient condition for strict convexity\"_ [SE](http://math.stackexchange.com/questions/210187/relation-between-positive-definite-matrix-and-strictly-convex-function)\n",
    "\n",
    "Convexity of what? Prove!\n",
    "\n",
    "***\n",
    "Prove that a covariance matrix is always positive semi-definite.\n",
    "http://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Norms\n",
    "\n",
    "What if we want to know how big a matrix is? We need a scale to compare different vectors/matrices/tensors. How should it work? Well, it should be;\n",
    "* transitive. If $\\parallel A \\parallel > \\parallel B \\parallel$ and $\\parallel B \\parallel > \\parallel C \\parallel$ then $\\parallel A \\parallel > \\parallel C \\parallel$\n",
    "\n",
    "It's a distance metric?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\parallel v \\parallel &= \\sqrt{v_1^2 + v_2^2 ... v_d^2} \\tag{from pythagoras} \\\\\n",
    "&= \\sqrt{v\\cdot v} \\tag{} \\\\\n",
    "\\therefore \\parallel v \\parallel^2 &= v\\cdot v \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\left\\|\\mathbf {x} \\right\\|_{p}:={\\bigg (}\\sum _{i=1}^{n}\\left|x_{i}\\right|^{p}{\\bigg )}^{1/p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverses\n",
    "\n",
    "So, given a set in $\\mathbb R^d$, can we make a group under matrix multiplication? $G = (\\mathbb R, \\times)$. We need identity, $I$, and inverses, $A^{-1}$\n",
    "\n",
    "\n",
    "Invertible if A is square and $\\exists A^{-1}:I = AA^{-1} = A^{-1}A$. \n",
    "\n",
    "Rank deficient case -- https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse\n",
    "\n",
    "\n",
    "##### Proofs and questions\n",
    "\n",
    "* Singular iff det(A) = 0\n",
    "* What if $I = AA^{-1} \\neq A^{-1}A$. Hmm.\n",
    "\n",
    "### Generalised inverse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur complement\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Mx &= y\\\\\n",
    "\\begin{bmatrix}\n",
    "A & B \\\\\n",
    "C & D \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\therefore Ax_1 + Bx_2 &= y_1 \\\\\n",
    "Cx_1 + Dx_2 &= y_2 \\\\\n",
    "x_2 &= D^{-1}(d-Cx_1) \\tag{solve for xs}\\\\\n",
    "\\dots \\\\\n",
    "x_1 &= (A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\\\\n",
    "x_2 &= D^{-1}\\Big(d-C(A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\Big)\\\\\n",
    "x &= M^{-1}y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* What about a greater number of blocks? E.g. 3x3?\n",
    "\n",
    "* Let A,B,C be singular/non-invertible.\n",
    "    * Can we invert the schur complement of D in M?\n",
    "    * I.e. What is $(A - BD^{-1}C)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts\n",
    "\n",
    "* Linear algebra seems to be the langauge of linear composition. It is almost entirely about how you can compose bases, however there are restrictions of the types of composition you can do: linear only.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* https://www.khanacademy.org/math/linear-algebra/eola-topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
