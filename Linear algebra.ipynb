{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Goal: Explore Linear Algebra and its relations to deeper math: groups, representations, graphs, probability, analysis, geometry, algebras,  ... ???_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication\n",
    "\n",
    "### Inner/Dot product\n",
    "\n",
    "Is the similarity between two vectors. How much are they pointing in the same direction?\n",
    "\n",
    "$$\n",
    "a \\cdot b = \\parallel a \\parallel_2 \\parallel  b \\parallel_2 cos (\\phi) \\tag{proof?!?}\n",
    "$$\n",
    "\n",
    "### Matmul\n",
    "\n",
    "> ??? what are we doing here?\n",
    "\n",
    "If $C = AB$ then we are taking the dot product of rows of A and columns of B. So each element of C is a measure of similarity between rows/columns of A/B. So, the elements of the first row of C are the similarities between the first row of A and each column of B.\n",
    "\n",
    "\n",
    "A matrix multiplication by a vector is simply a linear combination of the column space. This implies that $rank(A) \\leq rank(M)$ (proof?)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A = M\\vec x &= \n",
    "\\begin{bmatrix}\n",
    "\\vec c_{1} & \\vec c_{2} & \\vec c_{3} & \\vec c_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2  \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "x_1\\vec c_{1} + x_2\\vec c_{2} + x_3\\vec c_{3} + x_4\\vec c_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "### Examples and play\n",
    "\n",
    "ABC = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kronecker product\n",
    "\n",
    "> This is what we mean when we combine two independent states.\n",
    "\n",
    "$$A\\otimes B = \n",
    "\\begin{bmatrix}\n",
    "a_{11}B & ... & a_{1n}B \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}B & ... & a_{mn}B \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```julia\n",
    "function kronecker(A::Array,B::Array)\n",
    "    n,m = size(A)\n",
    "    p,q = size(B)\n",
    "    C = zeros((n*p,q*m))\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            C[ p*(i-1)+1:p*i , q*(j-1)+1:q*j] += A[i,j].*B\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "```\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "Show that\n",
    "* $(A \\otimes B)(C \\otimes D) = AC \\otimes BD$\n",
    "* $vec(ABC) = (C^T \\otimes A ) vec(B)$\n",
    "* $AX=B \\implies (I\\otimes A)vec(X) = vec(B)$\n",
    "* $A\\otimes B = U_A\\Sigma_A V_A^T \\otimes U_B\\Sigma_B V_B^T = (U_A \\otimes U_B)(\\Sigma_A \\otimes \\Sigma_B )(V_A^T \\otimes V_B^T)$ with some reordering?!?\n",
    "* $tr(x \\otimes y) = x \\cdot y$ if x and y are the same shape????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vecs/vals\n",
    "\n",
    "Now that we have the covariances between variables/features of X, we can find their eigen values/vectors. Eigenvectors of a matrix, M, are directions that are not rotated when multiplied by M -- $\\mathbf{A}\\vec{x} = a\\vec{x}$. So, the eigenvectors of our covariance matrix are directions that ???\n",
    "\n",
    "Why does it even make sense that there are m eigen vectors? It seems weird that there must be directions that are invariant to rotation. Why should there be eigenvectors? It seems kinda strange.\n",
    "\n",
    "$Ax = ax \\implies (A-aI)x = 0$\n",
    "\n",
    "* Hmm. Eigen vectors are not unique??\n",
    "* What does it mean when eigen values are negative. \n",
    "    * Given small positive diagonals and large off-diagonals.\n",
    "    * Given negative diagonals. And small off-diagonals.\n",
    "* or imaginary?\n",
    "\n",
    "Why does covar matrix have positive eigen values, but symetric ones dont necessarily.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Av &= \\lambda v\\\\\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix}\\tag{let A = ...}\\\\\n",
    "\\implies 0 &= Av-\\lambda v \\\\\n",
    "&= (A-\\lambda I)v \\\\\n",
    "&= det(A-\\lambda I) \\tag{why?!?} \\\\\n",
    "0& = det(\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "-\\begin{bmatrix}\n",
    "\\lambda & 0 \\\\ \n",
    "0 & \\lambda \\\\\n",
    "\\end{bmatrix})\\\\\n",
    "0&= (a_{11}-\\lambda)(a_{22}-\\lambda) - a_{12}a_{21} \\\\\n",
    "&= a_{11}a_{22} - a_{11}\\lambda - a_{22}\\lambda + \\lambda^2  \\\\\n",
    "&= \\lambda^2 + (-a_{11} - a_{22})\\lambda + (- a_{12}a_{21})\\\\\n",
    "x&={\\frac {-b\\pm {\\sqrt {b^{2}-4ac\\ }}}{2a}}\\\\\n",
    "&={\\frac {-(-a_{11} - a_{22})\\pm {\\sqrt {(-a_{11} - a_{22})^{2}-4(- a_{12}a_{21})\\ }}}{2}}\\\\\n",
    "&={\\frac {a_{11} + a_{22}\\pm {\\sqrt {a_{11}^2 -2a_{11}a_{22}+ a_{22}^2+4 a_{12}a_{21}\\ }}}{2}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But what does this mean intuitively? What is the geomentric interpretation?\n",
    "* Well, just diagonal entries scale the vector.\n",
    "* Off-diagonal entries add in information from other dimensions.\n",
    "\n",
    "What about some matrix transforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whitening data\n",
    "\n",
    "Let; \n",
    "* E be a matrix of stacked eigenvectors\n",
    "* V be the eigenvalues\n",
    "* M be some matrix e.g. covariance\n",
    "\n",
    "We can write the diagonalized covariance as: $V = E^TME$. \n",
    "So decorrelated variables, is the set eigenvalues. Why/how does this make sense? Isnt this almost exactally what PCA is doing???\n",
    "\n",
    "Therefore if we set $y= E^Tx$ then y is a decorelated representation of x. (???)\n",
    "\n",
    "##### Questions\n",
    "* Can all matrices be decorrelated? Does this depend on rank?\n",
    "* Where do the damned eigenvectors come from? Why is it that there (must??) exist a set of orthogonal axes that each ...\n",
    "\n",
    "##### Resources\n",
    "* http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf\n",
    "* https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value decomposition\n",
    "Used as an alternative way to find eigen vectors? Why?\n",
    "\n",
    "From before. $Av = \\lambda v \\therefore V\\Lambda V^{-1} = A$\n",
    "\n",
    "##### Positive semidefinite normal\n",
    "Must matrix to be factored must be a positive semidefinite normal matrix. What garuntee do we have that our matrix, M, will satisfy this requirement? Well, we are doing SVD on a correlation matrix. A correlation matrix is symmetric by definition, and ??? (which is why we always have positive values for our principle components/eigen values??)\n",
    "\n",
    "##### M = USV\n",
    "\n",
    "> * The columns of V (right-singular vectors) are eigenvectors of M∗M.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of MM∗.\n",
    "* The non-zero elements of Σ (non-zero singular values) are the square roots of the non-zero eigenvalues of M∗M or MM∗.\n",
    "\n",
    "Show/prove? this\n",
    "\n",
    "Let Ax = ax, aka x is an eigen vector of A. Then let A = USV, then Ux = ? = xV = x? As eigen vectors are only scaled, not rotated. \n",
    "What does U and V mean? I know they are rotations, (why??) but where do they rotate us?\n",
    "\n",
    "### How to find U,S,V\n",
    "\n",
    "How do you find these?? Maybe that will shed some light on what they are.\n",
    "\n",
    "$\\mathbf {M} =\\sum _{i}\\mathbf {A} _{i}=\\sum _{i}\\sigma _{i}\\mathbf {U} _{i}\\otimes \\mathbf {V} _{i}^{\\dagger }$ where $A = u\\otimes v$.\n",
    "\n",
    "\n",
    "### Relation to eigenvalue decomposition\n",
    "\n",
    "<i>\n",
    "> Given an SVD of M, as described above, the following two relations hold:\n",
    "$$\\begin{aligned}\\mathbf {M} ^{*}\\mathbf {M} &=\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}\\,\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}=\\mathbf {V} ({\\boldsymbol {\\Sigma }}^{*}{\\boldsymbol {\\Sigma }})\\mathbf {V} ^{*}\\\\\\mathbf {M} \\mathbf {M} ^{*}&=\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}\\,\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}=\\mathbf {U} ({\\boldsymbol {\\Sigma }}{\\boldsymbol {\\Sigma }}^{*})\\mathbf {U} ^{*}\\end{aligned} $$\n",
    "The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:\n",
    "* The columns of V (right-singular vectors) are eigenvectors of $M^{-1}M$.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of $MM^{-1}$.\n",
    "* The non-zero elements of Σ (non-zero singular values) are the square roots of the non-zero eigenvalues of M∗M or MM∗.</i>\n",
    "\n",
    "Ahh. That makes some more sense?\n",
    "\n",
    "$\\therefore W = M^TM = V_M \\Sigma_M^T \\Sigma_M V_M^T = U_W\\Sigma_W V^T_W$ which means $V_M = U_W, V_M^T = V^T_W$. I guess this makes sense. As the covariance matrix is symmetric. But now I am confused as we are using the eigen vectors as rotations, but normally they only scale the matrix, in fact that is their definition.\n",
    "\n",
    "https://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf\n",
    "\n",
    "\n",
    "$$\n",
    "USV^T = \\\\\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & u_{13} & u_{14} \\\\\n",
    "u_{21} & u_{22} & u_{23} & u_{24} \\\\\n",
    "u_{31} & u_{32} & u_{33} & u_{34} \\\\\n",
    "u_{41} & u_{42} & u_{43} & u_{44} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_{11} & 0 & 0 \\\\\n",
    "0 & s_{22} & 0 \\\\\n",
    "0 & 0 & s_{33} \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23} \\\\\n",
    "v_{31} & v_{32} & v_{33} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "=\\begin{bmatrix}\n",
    "s_{11}u_{11} & s_{22}u_{12} & s_{33}u_{13} \\\\\n",
    "s_{11}u_{21} & s_{22}u_{22} & s_{33}u_{23} \\\\\n",
    "s_{11}u_{31} & s_{22}u_{32} & s_{33}u_{33} \\\\\n",
    "s_{11}u_{41} & s_{22}u_{42} & s_{33}u_{43} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23} \\\\\n",
    "v_{31} & v_{32} & v_{33} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}\n",
    "v_{11}s_{11}u_{11} + v_{21}s_{22}u_{12} + v_{31}s_{33}u_{13} & v_{12}s_{11}u_{11} + v_{22}s_{22}u_{12} + v_{32}s_{33}u_{13} & v_{13}s_{11}u_{11} + v_{23}s_{22}u_{12} + v_{33}s_{33}u_{13} \\\\\n",
    "v_{11}s_{11}u_{21} + v_{21}s_{22}u_{22} + v_{31}s_{33}u_{23} & v_{12}s_{11}u_{21} + v_{22}s_{22}u_{22} + v_{32}s_{33}u_{23} & v_{13}s_{11}u_{21} + v_{23}s_{22}u_{22} + v_{33}s_{33}u_{23} \\\\\n",
    "v_{11}s_{11}u_{31} + v_{21}s_{22}u_{32} + v_{31}s_{33}u_{33} & v_{12}s_{11}u_{31} + v_{22}s_{22}u_{32} + v_{32}s_{33}u_{33} & v_{13}s_{11}u_{31} + v_{23}s_{22}u_{32} + v_{33}s_{33}u_{33} \\\\\n",
    "v_{11}s_{11}u_{41} + v_{21}s_{22}u_{42} + v_{31}s_{33}u_{43} & v_{12}s_{11}u_{41} + v_{22}s_{22}u_{42} + v_{32}s_{33}u_{43} & v_{13}s_{11}u_{41} + v_{23}s_{22}u_{42} + v_{33}s_{33}u_{43} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}\n",
    "v_{11}s_{11}u_{11} & v_{12}s_{11}u_{11} & v_{13}s_{11}u_{11} \\\\\n",
    "v_{11}s_{11}u_{21} & v_{12}s_{11}u_{21} & v_{13}s_{11}u_{21} \\\\\n",
    "v_{11}s_{11}u_{31} & v_{12}s_{11}u_{31} & v_{13}s_{11}u_{31} \\\\\n",
    "v_{11}s_{11}u_{41} & v_{12}s_{11}u_{41} & v_{13}s_{11}u_{41} \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "v_{21}s_{22}u_{12} & v_{22}s_{22}u_{12} & v_{23}s_{22}u_{12} \\\\\n",
    "v_{21}s_{22}u_{22} & v_{22}s_{22}u_{22} & v_{23}s_{22}u_{22} \\\\\n",
    "v_{21}s_{22}u_{32} & v_{22}s_{22}u_{32} & v_{23}s_{22}u_{32} \\\\\n",
    "v_{21}s_{22}u_{42} & v_{22}s_{22}u_{42} & v_{23}s_{22}u_{42} \\\\\n",
    "\\end{bmatrix}\n",
    "+\\begin{bmatrix}\n",
    "v_{31}s_{33}u_{13} & v_{32}s_{33}u_{13} & v_{33}s_{33}u_{13} \\\\\n",
    "v_{31}s_{33}u_{23} & v_{32}s_{33}u_{23} & v_{33}s_{33}u_{23} \\\\\n",
    "v_{31}s_{33}u_{33} & v_{32}s_{33}u_{33} & v_{33}s_{33}u_{33} \\\\\n",
    "v_{31}s_{33}u_{43} & v_{32}s_{33}u_{43} & v_{33}s_{33}u_{43} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Questions\n",
    "\n",
    "* When can a matrix not be decomposed into eigenvectors? (existince)\n",
    "* Uniquness. When do \n",
    "* Why is the decomposition so popular? And so useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank and Linear independence\n",
    "\n",
    "If we have two vectors, $x,y \\in \\mathbb{R}$, such that $x = [1,4], y = [2,8]$ then these vectors are _linearly dependent_ as y = 2x. Therefore, by composing x and y we can only ever get vectors on a line. However, \n",
    "\n",
    "\n",
    "* Do that proof thingy of rows = columns.\n",
    "* Random init = full rank with probability 1 ?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection\n",
    "\n",
    "##### Projection\n",
    "Let U = (mxm), S = (mxn), V^T = (nxn)\n",
    "$$\n",
    "\\begin{align}\n",
    "P_A &= A(A^TA)^{-1}A^T \\\\\n",
    "M &= USV^T \\\\\n",
    "P_A &= (USV^T) ((USV^T)^T(USV^T))^{-1} (USV^T)^T \\\\\n",
    "P_A &= USV^T (VS^TU^TUSV^T)^{-1} (VS^TU^T) \\\\\n",
    "P_A &= USV^T (VS^2V^T)^{-1} (VS^TU^T) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal matrix\n",
    "\n",
    "$A^T = A^{-1}$ Proof?\n",
    "\n",
    "Why are covariance matrices orthogonal?\n",
    "\n",
    "> _\"Orthogonality and statistical independence are not synonyms.\"_ [SE](http://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)\n",
    "\n",
    "### Orthogonal projections \n",
    "\n",
    "Let x be some vector and L be a subspace such that $L = span(v) = \\{cv : c \\in \\mathbb{R}\\}$. Then the projection of x onto L, $proj_L(x) = \\frac{x\\cdot v}{v \\cdot v}v$\n",
    "\n",
    "$P_A = P_A^2$ <- proof??\n",
    "\n",
    "$P_A = A^T(A^TA)^{-1}A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive semi definite\n",
    "***\n",
    "> _\"positive definiteness is a sufficient condition for strict convexity\"_ [SE](http://math.stackexchange.com/questions/210187/relation-between-positive-definite-matrix-and-strictly-convex-function)\n",
    "\n",
    "Convexity of what? Prove!\n",
    "\n",
    "***\n",
    "Prove that a covariance matrix is always positive semi-definite.\n",
    "http://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Norms\n",
    "\n",
    "What if we want to know how big a matrix is? We need a scale to compare different vectors/matrices/tensors. How should it work? Well, it should be;\n",
    "* transitive. If $\\parallel A \\parallel > \\parallel B \\parallel$ and $\\parallel B \\parallel > \\parallel C \\parallel$ then $\\parallel A \\parallel > \\parallel C \\parallel$\n",
    "\n",
    "It's a distance metric?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\parallel v \\parallel &= \\sqrt{v_1^2 + v_2^2 ... v_d^2} \\tag{from pythagoras} \\\\\n",
    "&= \\sqrt{v\\cdot v} \\tag{} \\\\\n",
    "\\therefore \\parallel v \\parallel^2 &= v\\cdot v \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\left\\|\\mathbf {x} \\right\\|_{p}:={\\bigg (}\\sum _{i=1}^{n}\\left|x_{i}\\right|^{p}{\\bigg )}^{1/p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverses\n",
    "\n",
    "So, given a set in $\\mathbb R^d$, can we make a group under matrix multiplication? $G = (\\mathbb R, \\times)$. We need identity, $I$, and inverses, $A^{-1}$\n",
    "\n",
    "\n",
    "Invertible if A is square and $\\exists A^{-1}:I = AA^{-1} = A^{-1}A$. \n",
    "\n",
    "Rank deficient case -- https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse\n",
    "\n",
    "\n",
    "##### Proofs and questions\n",
    "\n",
    "* Singular iff det(A) = 0\n",
    "* What if $I = AA^{-1} \\neq A^{-1}A$\n",
    "\n",
    "### Generalised inverse\n",
    "\n",
    "\n",
    "### Generalised \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur complement\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Mx &= y\\\\\n",
    "\\begin{bmatrix}\n",
    "A & B \\\\\n",
    "C & D \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\therefore Ax_1 + Bx_2 &= y_1 \\\\\n",
    "Cx_1 + Dx_2 &= y_2 \\\\\n",
    "x_2 &= D^{-1}(d-Cx_1) \\tag{solve for xs}\\\\\n",
    "\\dots \\\\\n",
    "x_1 &= (A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\\\\n",
    "x_2 &= D^{-1}\\Big(d-C(A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\Big)\\\\\n",
    "x &= M^{-1}y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* What about a greater number of blocks? E.g. 3x3?\n",
    "\n",
    "* Let A,B,C be singular/non-invertible.\n",
    "    * Can we invert the schur complement of D in M?\n",
    "    * I.e. What is $(A - BD^{-1}C)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity matrix\n",
    "\n",
    "If $B = P^{-1}AP$ then A and B have the same;\n",
    "\n",
    "* rank,\n",
    "* multi\n",
    "... ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Subspaces?!?\n",
    "* Pictures!!!\n",
    "* Span\n",
    "* Column space\n",
    "* Range (a general idea for functions but has specific definition for vector spaces??)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
