{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Goal: Explore Linear Algebra and its relations to everything: groups, representations, graphs, probability, analysis, geometry, algebra, learning, ... ???_\n",
    "\n",
    "Desiderata;\n",
    "* Intuit the geometry\n",
    "* Analyse the algorithms (SVD, Matmul, ?)\n",
    "* Understand what makes linear algebra the right level of abstraction and power and why this is important.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basis for representation\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix} &=\n",
    "a\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} +\n",
    "b\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\tag{linear combination of bases}\\\\\n",
    "&= a \\hat i + b \\hat j \\\\\n",
    "&= \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "So the vector form is really saying that we have a linear combination of basis vectors, which in the default case are the unit vectors. The span of two vectors $\\vec u, \\vec v$  is the set of all their linear combinations $a\\vec u + b\\vec v: a,b \\in \\mathbb R$ (in R or ??). Aka what are all the points we can reach using only scalar multiplication and vector addition of basis vectors?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2  \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2  \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "a_{11}b_1 + a_{12}b_2 \\\\\n",
    "a_{21}b_1 + a_{22}b_2 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= b_1 \\vec a_{\\cdot, 1}  + b_2 \\vec a_{\\cdot, 2} \\tag{a linear combination of columns of $A$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can pick any set of basis vectors, $\\vec u, \\vec v$, as long as they are not pointing in the same direction ($\\not \\exists c: c\\vec v = \\vec u$). \n",
    "\n",
    "***\n",
    "\n",
    "* Which bases are the best?\n",
    "* What if our bases are non-linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowable (linear) transformations\n",
    "\n",
    "\\begin{align*}\n",
    "\\vec u &= a\\vec i + b\\vec j \\\\\n",
    "f(\\vec u) &= \\vec v \\\\\n",
    "&= f(a\\vec i + b\\vec j) \\\\\n",
    "&= af(\\vec i) + bf(\\vec j) \\tag{? what is his property called?}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Grid lines must stay parallel and evenly spaced. Left $\\mathcal L = \\{ f\\}$ be the set of functions that do a linear transform on their inputs\n",
    "\n",
    "$\\forall f,g \\in \\mathcal L \\exists h: f(g(\\vec u)) = h(\\vec u)$\n",
    "A unique property to linear algebra because of how we have restricted $f,g$. \n",
    "\n",
    "_Note that $f\\circ g$ is not equivalent to $h$, only equal to._\n",
    "\n",
    "\n",
    "## Outer product\n",
    "\n",
    "\n",
    "## Inner/Dot product\n",
    "\n",
    "Is the similarity between two vectors. How much are they pointing in the same direction?\n",
    "\n",
    "$$\n",
    "a \\cdot b = \\parallel a \\parallel_2 \\parallel  b \\parallel_2 cos (\\phi) \\tag{proof?!?}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Matmul\n",
    "\n",
    "If $C = AB$ then we are taking the dot product of rows of A and columns of B. So each element of C is a measure of similarity between rows/columns of A/B. So, the elements of the first row of C are the similarities between the first row of A and each column of B.\n",
    "\n",
    "A matrix multiplication by a vector is simply a linear combination of the column space. This implies that $rank(A) \\leq rank(M)$ (proof?)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A = M\\vec x &= \n",
    "\\begin{bmatrix}\n",
    "\\vec c_{1} & \\vec c_{2} & \\vec c_{3} & \\vec c_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2  \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    "x_1\\vec c_{1} + x_2\\vec c_{2} + x_3\\vec c_{3} + x_4\\vec c_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Examples and play\n",
    "\n",
    "ABC = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank and Linear independence\n",
    "\n",
    "If we have two vectors, $x,y \\in \\mathbb{R}$, such that $x = [1,4], y = [2,8]$ then these vectors are _linearly dependent_ as y = 2x. Therefore, by composing x and y we can only ever get vectors on a line. However, \n",
    "\n",
    "\n",
    "* Do that proof thingy of rows = columns.\n",
    "* Random init = full rank with probability 1 ?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal matrix\n",
    "\n",
    "$A^T = A^{-1}$ Proof?\n",
    "\n",
    "Why are covariance matrices orthogonal?\n",
    "\n",
    "> _\"Orthogonality and statistical independence are not synonyms.\"_ [SE](http://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)\n",
    "\n",
    "### Orthogonal projections \n",
    "\n",
    "Let x be some vector and L be a subspace such that $L = span(v) = \\{cv : c \\in \\mathbb{R}\\}$. Then the projection of x onto L, $proj_L(x) = \\frac{x\\cdot v}{v \\cdot v}v$\n",
    "\n",
    "$P_A = P_A^2$ <- proof??\n",
    "\n",
    "$P_A = A^T(A^TA)^{-1}A$\n",
    "\n",
    "\n",
    "Let U = (mxm), S = (mxn), V^T = (nxn)\n",
    "$$\n",
    "\\begin{align}\n",
    "P_A &= A(A^TA)^{-1}A^T \\\\\n",
    "M &= USV^T \\\\\n",
    "P_A &= (USV^T) ((USV^T)^T(USV^T))^{-1} (USV^T)^T \\\\\n",
    "P_A &= USV^T (VS^TU^TUSV^T)^{-1} (VS^TU^T) \\\\\n",
    "P_A &= USV^T (VS^2V^T)^{-1} (VS^TU^T) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Networks\n",
    "\n",
    "* CP decomposition gives a large diagonal as the central tensor. What other decompositions are there which give us nice properties of the center? How about something dense/small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Contractions\n",
    "\n",
    "A matrix multiplication is a special case.\n",
    "\n",
    "\\begin{align*}\n",
    "(A\\cdot_{1}B)_{i,j}&= \\sum_k A(i,k)B(k,j)\\tag{matmul}\\\\\n",
    "(\\mathcal A\\cdot_{k_1,\\dots,k_K})_{i_1,\\dots,i_I}&= \\sum_{k_1,\\dots,k_K} A(i_1,k)\\dots A(i_I,j)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vecotisation, Matricisation and Tensorisation\n",
    "\n",
    "What is with the fascination of matricising tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor product\n",
    "\n",
    "> This is what we mean when we combine two independent states.\n",
    "\n",
    "## Kronecker product\n",
    "\n",
    "\n",
    "$$A\\otimes B = \n",
    "\\begin{bmatrix}\n",
    "a_{11}B & ... & a_{1n}B \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}B & ... & a_{mn}B \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```julia\n",
    "function kronecker(A::Array,B::Array)\n",
    "    n,m = size(A)\n",
    "    p,q = size(B)\n",
    "    C = zeros((n*p,q*m))\n",
    "    for i in 1:n\n",
    "        for j in 1:m\n",
    "            C[ p*(i-1)+1:p*i , q*(j-1)+1:q*j] += A[i,j].*B\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end\n",
    "```\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "Show that\n",
    "* $(A \\otimes B)(C \\otimes D) = AC \\otimes BD$\n",
    "* $vec(ABC) = (C^T \\otimes A ) vec(B)$\n",
    "* $AX=B \\implies (I\\otimes A)vec(X) = vec(B)$\n",
    "* $A\\otimes B = U_A\\Sigma_A V_A^T \\otimes U_B\\Sigma_B V_B^T = (U_A \\otimes U_B)(\\Sigma_A \\otimes \\Sigma_B )(V_A^T \\otimes V_B^T)$ with some reordering?!?\n",
    "* $tr(x \\otimes y) = x \\cdot y$ if x and y are the same shape????\n",
    "\n",
    "## Generalised tensor product\n",
    "\n",
    "$A\\in \\mathbb R^{n_1,\\dots,n_P}, B\\in \\mathbb R^{n_{P+1},\\dots,n_{Q+P}}, A\\otimes_g B \\in \\mathbb R^{n_1,\\dots,n_{P+Q}}$\n",
    "\n",
    "$g : \\mathbb R \\times \\mathbb R \\rightarrow \\mathbb R$ such that $\\forall a, b, c \\in \\mathbb R : g(g(a, b), c) = g(a, g(b, c))$ and $\\forall a, b âˆˆ \\mathbb R : g(a, b) = g(b, a)$\n",
    "\n",
    "\\begin{align*}\n",
    "(A\\otimes B)_{i_1,\\dots,i_{P+Q}} &= (a_{i_1,\\dots,i_{P}}b_{i_{P+1},\\dots,i_{P+Q}})\\\\\n",
    "(A\\otimes_g B)_{i,j} &= g(a_i,b_j) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "***\n",
    "* A matmul is a special case (?). By summing over a set of the ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vecs/vals\n",
    "\n",
    "Now that we have the covariances between variables/features of X, we can find their eigen values/vectors. Eigenvectors of a matrix, M, are directions that are not rotated when multiplied by M -- $\\mathbf{A}\\vec{x} = a\\vec{x}$. So, the eigenvectors of our covariance matrix are directions that ???\n",
    "\n",
    "Why does it even make sense that there are m eigen vectors? It seems weird that there must be directions that are invariant to rotation. Why should there be eigenvectors? It seems kinda strange.\n",
    "\n",
    "$Ax = ax \\implies (A-aI)x = 0$\n",
    "\n",
    "* Hmm. Eigen vectors are not unique??\n",
    "* What does it mean when eigen values are negative. \n",
    "    * Given small positive diagonals and large off-diagonals.\n",
    "    * Given negative diagonals. And small off-diagonals.\n",
    "* or imaginary?\n",
    "\n",
    "Why does covar matrix have positive eigen values, but symetric ones dont necessarily.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Av &= \\lambda v\\\\\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "v_{11} \\\\ v_{21} \\\\\n",
    "\\end{bmatrix}\\tag{let A = ...}\\\\\n",
    "\\implies 0 &= Av-\\lambda v \\\\\n",
    "&= (A-\\lambda I)v \\\\\n",
    "&= det(A-\\lambda I) \\tag{why?!?} \\\\\n",
    "0& = det(\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "-\\begin{bmatrix}\n",
    "\\lambda & 0 \\\\ \n",
    "0 & \\lambda \\\\\n",
    "\\end{bmatrix})\\\\\n",
    "0&= (a_{11}-\\lambda)(a_{22}-\\lambda) - a_{12}a_{21} \\\\\n",
    "&= a_{11}a_{22} - a_{11}\\lambda - a_{22}\\lambda + \\lambda^2  \\\\\n",
    "&= \\lambda^2 + (-a_{11} - a_{22})\\lambda + (- a_{12}a_{21})\\\\\n",
    "x&={\\frac {-b\\pm {\\sqrt {b^{2}-4ac\\ }}}{2a}}\\\\\n",
    "&={\\frac {-(-a_{11} - a_{22})\\pm {\\sqrt {(-a_{11} - a_{22})^{2}-4(- a_{12}a_{21})\\ }}}{2}}\\\\\n",
    "&={\\frac {a_{11} + a_{22}\\pm {\\sqrt {a_{11}^2 -2a_{11}a_{22}+ a_{22}^2+4 a_{12}a_{21}\\ }}}{2}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But what does this mean intuitively? What is the geomentric interpretation?\n",
    "* Well, just diagonal entries scale the vector.\n",
    "* Off-diagonal entries add in information from other dimensions.\n",
    "\n",
    "What about some matrix transforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whitening data\n",
    "\n",
    "Let; \n",
    "* E be a matrix of stacked eigenvectors\n",
    "* V be the eigenvalues\n",
    "* M be some matrix e.g. covariance\n",
    "\n",
    "We can write the diagonalized covariance as: $V = E^TME$. \n",
    "So decorrelated variables, is the set eigenvalues. Why/how does this make sense? Isnt this almost exactally what PCA is doing???\n",
    "\n",
    "Therefore if we set $y= E^Tx$ then y is a decorelated representation of x. (???)\n",
    "\n",
    "##### Questions\n",
    "* Can all matrices be decorrelated? Does this depend on rank?\n",
    "* Where do the damned eigenvectors come from? Why is it that there (must??) exist a set of orthogonal axes that each ...\n",
    "\n",
    "##### Resources\n",
    "* http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf\n",
    "* https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value decomposition\n",
    "Used as an alternative way to find eigen vectors? Why?\n",
    "\n",
    "From before. $Av = \\lambda v \\therefore V\\Lambda V^{-1} = A$\n",
    "\n",
    "##### Positive semidefinite normal\n",
    "Must matrix to be factored must be a positive semidefinite normal matrix. What garuntee do we have that our matrix, M, will satisfy this requirement? Well, we are doing SVD on a correlation matrix. A correlation matrix is symmetric by definition, and ??? (which is why we always have positive values for our principle components/eigen values??)\n",
    "\n",
    "##### M = USV\n",
    "\n",
    "> * The columns of V (right-singular vectors) are eigenvectors of Mâˆ—M.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of MMâˆ—.\n",
    "* The non-zero elements of Î£ (non-zero singular values) are the square roots of the non-zero eigenvalues of Mâˆ—M or MMâˆ—.\n",
    "\n",
    "Show/prove? this\n",
    "\n",
    "Let Ax = ax, aka x is an eigen vector of A. Then let A = USV, then Ux = ? = xV = x? As eigen vectors are only scaled, not rotated. \n",
    "What does U and V mean? I know they are rotations, (why??) but where do they rotate us?\n",
    "\n",
    "### How to find U,S,V\n",
    "\n",
    "How do you find these?? Maybe that will shed some light on what they are.\n",
    "\n",
    "$\\mathbf {M} =\\sum _{i}\\mathbf {A} _{i}=\\sum _{i}\\sigma _{i}\\mathbf {U} _{i}\\otimes \\mathbf {V} _{i}^{\\dagger }$ where $A = u\\otimes v$.\n",
    "\n",
    "\n",
    "### Relation to eigenvalue decomposition\n",
    "\n",
    "<i>\n",
    "> Given an SVD of M, as described above, the following two relations hold:\n",
    "$$\\begin{aligned}\\mathbf {M} ^{*}\\mathbf {M} &=\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}\\,\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}=\\mathbf {V} ({\\boldsymbol {\\Sigma }}^{*}{\\boldsymbol {\\Sigma }})\\mathbf {V} ^{*}\\\\\\mathbf {M} \\mathbf {M} ^{*}&=\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}\\,\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}=\\mathbf {U} ({\\boldsymbol {\\Sigma }}{\\boldsymbol {\\Sigma }}^{*})\\mathbf {U} ^{*}\\end{aligned} $$\n",
    "The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:\n",
    "* The columns of V (right-singular vectors) are eigenvectors of $M^{-1}M$.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of $MM^{-1}$.\n",
    "* The non-zero elements of Î£ (non-zero singular values) are the square roots of the non-zero eigenvalues of Mâˆ—M or MMâˆ—.</i>\n",
    "\n",
    "Ahh. That makes some more sense?\n",
    "\n",
    "$\\therefore W = M^TM = V_M \\Sigma_M^T \\Sigma_M V_M^T = U_W\\Sigma_W V^T_W$ which means $V_M = U_W, V_M^T = V^T_W$. I guess this makes sense. As the covariance matrix is symmetric. But now I am confused as we are using the eigen vectors as rotations, but normally they only scale the matrix, in fact that is their definition.\n",
    "\n",
    "https://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf\n",
    "\n",
    "\n",
    "$$\n",
    "USV^T = \\\\\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & u_{13} & u_{14} \\\\\n",
    "u_{21} & u_{22} & u_{23} & u_{24} \\\\\n",
    "u_{31} & u_{32} & u_{33} & u_{34} \\\\\n",
    "u_{41} & u_{42} & u_{43} & u_{44} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "s_{11} & 0 & 0 \\\\\n",
    "0 & s_{22} & 0 \\\\\n",
    "0 & 0 & s_{33} \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23} \\\\\n",
    "v_{31} & v_{32} & v_{33} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "=\\begin{bmatrix}\n",
    "s_{11}u_{11} & s_{22}u_{12} & s_{33}u_{13} \\\\\n",
    "s_{11}u_{21} & s_{22}u_{22} & s_{33}u_{23} \\\\\n",
    "s_{11}u_{31} & s_{22}u_{32} & s_{33}u_{33} \\\\\n",
    "s_{11}u_{41} & s_{22}u_{42} & s_{33}u_{43} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23} \\\\\n",
    "v_{31} & v_{32} & v_{33} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}\n",
    "v_{11}s_{11}u_{11} + v_{21}s_{22}u_{12} + v_{31}s_{33}u_{13} & v_{12}s_{11}u_{11} + v_{22}s_{22}u_{12} + v_{32}s_{33}u_{13} & v_{13}s_{11}u_{11} + v_{23}s_{22}u_{12} + v_{33}s_{33}u_{13} \\\\\n",
    "v_{11}s_{11}u_{21} + v_{21}s_{22}u_{22} + v_{31}s_{33}u_{23} & v_{12}s_{11}u_{21} + v_{22}s_{22}u_{22} + v_{32}s_{33}u_{23} & v_{13}s_{11}u_{21} + v_{23}s_{22}u_{22} + v_{33}s_{33}u_{23} \\\\\n",
    "v_{11}s_{11}u_{31} + v_{21}s_{22}u_{32} + v_{31}s_{33}u_{33} & v_{12}s_{11}u_{31} + v_{22}s_{22}u_{32} + v_{32}s_{33}u_{33} & v_{13}s_{11}u_{31} + v_{23}s_{22}u_{32} + v_{33}s_{33}u_{33} \\\\\n",
    "v_{11}s_{11}u_{41} + v_{21}s_{22}u_{42} + v_{31}s_{33}u_{43} & v_{12}s_{11}u_{41} + v_{22}s_{22}u_{42} + v_{32}s_{33}u_{43} & v_{13}s_{11}u_{41} + v_{23}s_{22}u_{42} + v_{33}s_{33}u_{43} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "= \\begin{bmatrix}\n",
    "v_{11}s_{11}u_{11} & v_{12}s_{11}u_{11} & v_{13}s_{11}u_{11} \\\\\n",
    "v_{11}s_{11}u_{21} & v_{12}s_{11}u_{21} & v_{13}s_{11}u_{21} \\\\\n",
    "v_{11}s_{11}u_{31} & v_{12}s_{11}u_{31} & v_{13}s_{11}u_{31} \\\\\n",
    "v_{11}s_{11}u_{41} & v_{12}s_{11}u_{41} & v_{13}s_{11}u_{41} \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "v_{21}s_{22}u_{12} & v_{22}s_{22}u_{12} & v_{23}s_{22}u_{12} \\\\\n",
    "v_{21}s_{22}u_{22} & v_{22}s_{22}u_{22} & v_{23}s_{22}u_{22} \\\\\n",
    "v_{21}s_{22}u_{32} & v_{22}s_{22}u_{32} & v_{23}s_{22}u_{32} \\\\\n",
    "v_{21}s_{22}u_{42} & v_{22}s_{22}u_{42} & v_{23}s_{22}u_{42} \\\\\n",
    "\\end{bmatrix}\n",
    "+\\begin{bmatrix}\n",
    "v_{31}s_{33}u_{13} & v_{32}s_{33}u_{13} & v_{33}s_{33}u_{13} \\\\\n",
    "v_{31}s_{33}u_{23} & v_{32}s_{33}u_{23} & v_{33}s_{33}u_{23} \\\\\n",
    "v_{31}s_{33}u_{33} & v_{32}s_{33}u_{33} & v_{33}s_{33}u_{33} \\\\\n",
    "v_{31}s_{33}u_{43} & v_{32}s_{33}u_{43} & v_{33}s_{33}u_{43} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Questions\n",
    "\n",
    "* When can a matrix not be decomposed into eigenvectors? (existince)\n",
    "* Uniquness. When do \n",
    "* Why is the decomposition so popular? And so useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional interpretations\n",
    "\n",
    "## Indexing\n",
    "\n",
    "Indexing as function calls $a(11) = a_{11}$. Or as a special type class ? Where each index is a component of the vecotr and all components are independent (but maybe not independent in the other sense).\n",
    "\n",
    "Which leads to the quantum funny business. Interesting that it is the indexes getting entangled with each other, not the elements. Hmm.!\n",
    "\n",
    "## Functional linear algebra \n",
    "\n",
    "\n",
    "## Functional linear transform\n",
    "\n",
    "Want a function $f:\\mathbb R^n \\rightarrow \\mathbb R^n$ that is a linear transform.\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2  \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2  \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= f(\\vec b)\n",
    "\\end{align*}\n",
    "\n",
    "We could imagine this function mapping one vector to another. But how? Does it use a look up table, or continuiously push the input to the output? Or ??\n",
    "\n",
    "\n",
    "## Functional analaysis \n",
    "\n",
    "infinite vector spaces\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive semi definite\n",
    "***\n",
    "> _\"positive definiteness is a sufficient condition for strict convexity\"_ [SE](http://math.stackexchange.com/questions/210187/relation-between-positive-definite-matrix-and-strictly-convex-function)\n",
    "\n",
    "Convexity of what? Prove!\n",
    "\n",
    "***\n",
    "Prove that a covariance matrix is always positive semi-definite.\n",
    "http://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Norms\n",
    "\n",
    "What if we want to know how big a matrix is? We need a scale to compare different vectors/matrices/tensors. How should it work? Well, it should be;\n",
    "* transitive. If $\\parallel A \\parallel > \\parallel B \\parallel$ and $\\parallel B \\parallel > \\parallel C \\parallel$ then $\\parallel A \\parallel > \\parallel C \\parallel$\n",
    "\n",
    "It's a distance metric?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\parallel v \\parallel &= \\sqrt{v_1^2 + v_2^2 ... v_d^2} \\tag{from pythagoras} \\\\\n",
    "&= \\sqrt{v\\cdot v} \\tag{} \\\\\n",
    "\\therefore \\parallel v \\parallel^2 &= v\\cdot v \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\left\\|\\mathbf {x} \\right\\|_{p}:={\\bigg (}\\sum _{i=1}^{n}\\left|x_{i}\\right|^{p}{\\bigg )}^{1/p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverses\n",
    "\n",
    "So, given a set in $\\mathbb R^d$, can we make a group under matrix multiplication? $G = (\\mathbb R, \\times)$. We need identity, $I$, and inverses, $A^{-1}$\n",
    "\n",
    "\n",
    "Invertible if A is square and $\\exists A^{-1}:I = AA^{-1} = A^{-1}A$. \n",
    "\n",
    "Rank deficient case -- https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse\n",
    "\n",
    "\n",
    "##### Proofs and questions\n",
    "\n",
    "* Singular iff det(A) = 0\n",
    "* What if $I = AA^{-1} \\neq A^{-1}A$\n",
    "\n",
    "### Generalised inverse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schur complement\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Mx &= y\\\\\n",
    "\\begin{bmatrix}\n",
    "A & B \\\\\n",
    "C & D \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\therefore Ax_1 + Bx_2 &= y_1 \\\\\n",
    "Cx_1 + Dx_2 &= y_2 \\\\\n",
    "x_2 &= D^{-1}(d-Cx_1) \\tag{solve for xs}\\\\\n",
    "\\dots \\\\\n",
    "x_1 &= (A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\\\\n",
    "x_2 &= D^{-1}\\Big(d-C(A-BD^{-1}C)^{-1}(c-CD^{-1}d) \\Big)\\\\\n",
    "x &= M^{-1}y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* What about a greater number of blocks? E.g. 3x3?\n",
    "\n",
    "* Let A,B,C be singular/non-invertible.\n",
    "    * Can we invert the schur complement of D in M?\n",
    "    * I.e. What is $(A - BD^{-1}C)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariant symmetries\n",
    "\n",
    "Non-uniqueness, ... \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "c_{11} & c_{12} \\\\\n",
    "c_{21} & c_{22} \\\\\n",
    "\\end{bmatrix} &=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22} \\\\\n",
    "\\end{bmatrix}\\\\ \n",
    "&= \\begin{bmatrix}\n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "???\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured matrices\n",
    "\n",
    "* Circulant\n",
    "* Diagonal\n",
    "* Upper triangular\n",
    "* Symmetric\n",
    "* ?!?\n",
    "* Convolutional\n",
    "* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
