{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear independence\n",
    "\n",
    "If we have two vectors, $x,y \\in \\mathbb{R}$, such that $x = [1,4], y = [2,8]$ then these vectors are _linearly dependent_ as y = 2x. Therefore, by composing x and y we can only ever get vectors on a line. However, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen vecs/vals\n",
    "\n",
    "Now that we have the covariances between variables/features of X, we can find their eigen values/vectors. Eigenvectors of a matrix, M, are directions that are not rotated when multiplied by M -- $\\mathbf{A}\\vec{x} = a\\vec{x}$. So, the eigenvectors of our covariance matrix are directions that ???\n",
    "\n",
    "Why does it even make sense that there are m eigen vectors? It seems weird that there must be directions that are invariant to rotation.\n",
    "\n",
    "Why should there be eigenvectors? It seems kinda strange.\n",
    "\n",
    "$Ax = ax \\implies (A-aI)x = 0$\n",
    "\n",
    "Hmm. Eigen vectors are not unique??\n",
    "\n",
    "\n",
    "Now take whichever eigenvectors you like, but really we want the ones with the greatest eigenvalues. This is because the eigen values are the variances of our eigen vectors. As ... ???\n",
    "\n",
    "hmm. should these all be positive? - the eigen values\n",
    "\n",
    "then multiply data . selected eigenvectors. But wait a minute, arent the eigen vectos for the co-variance matrix, not the data? So really, the eigen vectors are ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whitening data\n",
    "\n",
    "Let; \n",
    "* E be a matrix of stacked eigenvectors\n",
    "* V be the eigenvalues\n",
    "* M be some matrix e.g. covariance\n",
    "\n",
    "We can write the diagonalized covariance as: $V = E^TME$. \n",
    "So decorrelated variables, is the set eigenvalues. Why/how does this make sense? Isnt this almost exactally what PCA is doing???\n",
    "\n",
    "Therefore if we set $y= E^Tx$ then y is a decorelated representation of x. (???)\n",
    "\n",
    "##### Questions\n",
    "* Can all matrices be decorrelated? Does this depend on rank?\n",
    "* Where do the damned eigenvectors come from? Why is it that there (must??) exist a set of orthogonal axes that each ...\n",
    "\n",
    "##### Resources\n",
    "* http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf\n",
    "* https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value decomposition\n",
    "Used as an alternative way to find eigen vectors? Why?\n",
    "\n",
    "##### Positive semidefinite normal\n",
    "Must matrix to be factored must be a positive semidefinite normal matrix. What garuntee do we have that our matrix, M, will satisfy this requirement? Well, we are doing SVD on a correlation matrix. A correlation matrix is symmetric by definition, and ??? (which is why we always have positive values for our principle components/eigen values??)\n",
    "\n",
    "##### M = USV\n",
    "\n",
    "> * The columns of V (right-singular vectors) are eigenvectors of M∗M.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of MM∗.\n",
    "* The non-zero elements of Σ (non-zero singular values) are the square roots of the non-zero eigenvalues of M∗M or MM∗.\n",
    "\n",
    "Show/prove? this\n",
    "\n",
    "Let Ax = ax, aka x is an eigen vector of A. Then let A = USV, then Ux = ? = xV = x? As eigen vectors are only scaled, not rotated. \n",
    "What does U and V mean? I know they are rotations, (why??) but where do they rotate us?\n",
    "\n",
    "### How to find U,S,V\n",
    "\n",
    "How do you find these?? Maybe that will shed some light on what they are.\n",
    "\n",
    "$\\mathbf {M} =\\sum _{i}\\mathbf {A} _{i}=\\sum _{i}\\sigma _{i}\\mathbf {U} _{i}\\otimes \\mathbf {V} _{i}^{\\dagger }$ where $A = u\\otimes v$.\n",
    "\n",
    "\n",
    "### Relation to eigenvalue decomposition\n",
    "\n",
    "<i>\n",
    "> Given an SVD of M, as described above, the following two relations hold:\n",
    "$$\\begin{aligned}\\mathbf {M} ^{*}\\mathbf {M} &=\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}\\,\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}=\\mathbf {V} ({\\boldsymbol {\\Sigma }}^{*}{\\boldsymbol {\\Sigma }})\\mathbf {V} ^{*}\\\\\\mathbf {M} \\mathbf {M} ^{*}&=\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}\\,\\mathbf {V} {\\boldsymbol {\\Sigma }}^{*}\\mathbf {U} ^{*}=\\mathbf {U} ({\\boldsymbol {\\Sigma }}{\\boldsymbol {\\Sigma }}^{*})\\mathbf {U} ^{*}\\end{aligned} $$\n",
    "The right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides. Consequently:\n",
    "* The columns of V (right-singular vectors) are eigenvectors of $M^{-1}M$.\n",
    "* The columns of U (left-singular vectors) are eigenvectors of $MM^{-1}$.\n",
    "* The non-zero elements of Σ (non-zero singular values) are the square roots of the non-zero eigenvalues of M∗M or MM∗.</i>\n",
    "\n",
    "Ahh. That makes some more sense?\n",
    "\n",
    "$\\therefore W = M^TM = V_M \\Sigma_M^T \\Sigma_M V_M^T = U_W\\Sigma_W V^T_W$ which means $V_M = U_W, V_M^T = V^T_W$. I guess this makes sense. As the covariance matrix is symmetric. But now I am confused as we are using the eigen vectors as rotations, but normally they only scale the matrix, in fact that is their definition.\n",
    "\n",
    "https://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank\n",
    "\n",
    "* Do that proof thingy of rows = columns.\n",
    "* Random init = full rank with probability 1 ?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal matrix\n",
    "\n",
    "$A^T = A^{-1}$ Proof?\n",
    "\n",
    "Why are covariance matrices orthogonal?\n",
    "\n",
    "> _\"Orthogonality and statistical independence are not synonyms.\"_ [SE](http://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive semi definite\n",
    "***\n",
    "> _\"positive definiteness is a sufficient condition for strict convexity\"_ [SE](http://math.stackexchange.com/questions/210187/relation-between-positive-definite-matrix-and-strictly-convex-function)\n",
    "\n",
    "Convexity of what? Prove!\n",
    "\n",
    "***\n",
    "Prove that a covariance matrix is always positive semi-definite.\n",
    "http://math.stackexchange.com/questions/114072/what-is-the-proof-that-covariance-matrices-are-always-semi-definite\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal projections \n",
    "\n",
    "Let x be some vector and L be a subspace such that $L = span(v) = \\{cv : c \\in \\mathbb{R}\\}$. Then the projection of x onto L, $proj_L(x) = \\frac{x\\cdot v}{v \\cdot v}v$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Norms\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\parallel v \\parallel &= \\sqrt{v_1^2 + v_2^2 ... v_d^2} \\tag{from pythagoras} \\\\\n",
    "&= \\sqrt{v\\cdot v} \\tag{} \\\\\n",
    "\\therefore \\parallel v \\parallel^2 &= v\\cdot v \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subspaces?!?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
