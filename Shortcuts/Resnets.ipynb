{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "A residual is the error left over when approximating a function.\n",
    "* Define $\\mathcal{H}(x)$ to be the underlying mapping we want to learn. Such that $\\mathcal{H}: X \\rightarrow Y$.\n",
    "* At any given training step we have some approximation of $h_{i}:\\lim_{i\\to\\infty} h_{i}  \\rightarrow \\mathcal{H}$\n",
    "* The residual (or error) of the approximation is given as $h(x) - y = r$\n",
    "\n",
    "However, a resnet defines the underlying mapping to be the identity.\n",
    "\n",
    "* Define $\\mathcal{H}(x) = x$.\n",
    "* We can define the residual, r, as a function of x. Giving $\\mathcal{F}(x):= r = h(x) - x$\n",
    "\n",
    "\n",
    "In practice this is rearranged and we learn the residual function instead of the underlying mapping.\n",
    "\n",
    "* $\\mathcal{H}(x) = \\mathcal{F}(x) + x$. \n",
    "So, $\\mathcal{F}(x)$ is a measure of how far away the mapping is from the identity (rather than zeros like usual).\n",
    "\n",
    "\n",
    "Next, we choose $\\mathcal{H}(x) = x$ as a special case of $\\mathcal{H}(x) = Ax$ where A = I. So, we can make this formulation more general by defining A, and initialising it as I.\n",
    "* $\\mathcal{H}(x) = Ax$\n",
    "* thus we get $\\mathcal{F}(x):= h(x) - Ax$ \n",
    "\n",
    "### Backprop derivation\n",
    "\n",
    "$$x_{l+1} = \\mathcal{F}_l(x_l|\\mathcal{W}_l) + x_l$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x_l} = \\frac{\\partial \\mathcal{L}}{ \\partial\\mathcal{F}_l}  \\frac{\\partial \\mathcal{F}_l}{ \\partial x}\n",
    "$$\n",
    "\n",
    "\n",
    "### Facilitation\n",
    "\n",
    "Oh... This is why it makes it easy to learn the identity. \n",
    "(??? so it is easier to learn zeros than ones?) Hmm, dont know about that. So this assumes that it is easy for the mapping to be pushed to zero?\n",
    "\n",
    " \n",
    "\n",
    "> If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.\n",
    "\n",
    "I like this. It is like facilitating learning. I guess this is about what biases/heuristics/priors we want to incorporate into our network. How can we reason about this at a higher level? So that a network could learn to change its structure, with the goal of facilitating learning? This is the sort of thing we would hope to learn from unsupervised learning? (!?!)\n",
    "\n",
    "So this is saying that we expect the mapping to be some linear function of x. (what if we use others?) Add\n",
    "\n",
    "### Types\n",
    "\n",
    "So why would passing information/representing features with residuals be better? (types of information?? which is best?) How how does it make sense to combine different types of information? Residual + x = y. \n",
    "\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Can we interpret these residual functions as errors? How about as uncertainty (probabilistic interpretation)?\n",
    "\n",
    "Probability uncertainty - residual error ... http://www.probabilistic-numerics.org/general/2015/01/14/UQ/\n",
    "\n",
    "As RNNs\n",
    "\n",
    "##### Ensembles\n",
    "\n",
    "I still dont get why it doesnt effect the next when you drop a layer...\n",
    "\n",
    "Because they are learning residuals. Error w.r.t. their input/output. If we drop upstream layers, this shouldnt effect the residual that this layer was correcting as the residuals are disjoint?? So they each specialise? \n",
    "\n",
    "Could compensate for others??? Why would a layer learn to correct another layer? It wouldnt/shouldnt?\n",
    "\n",
    "\n",
    "If a layer is giving good/accurate output, then downstream layers will only recieve error signals for the difference between their ... ?? So they will fit themselves to errors that have not been removed upstream in the network.\n",
    "\n",
    "\n",
    "\n",
    "### Random ideas\n",
    "\n",
    "Try with ELUs instead of ReLUs. As normally f(x) + x = linear with discontinuity + linear.\n",
    "\n",
    "##### Gating\n",
    "Does the residual passed forward include other residuals? $y = \\mathcal{F}(x,W\\{W_i\\}) + W_S x $ OR $y = \\mathcal{F}(x,W\\{W_i\\}) + W_{S_i} x_i + W_{S_{i-1}} x_{i-1}$\n",
    "\n",
    "Could used multiplicitive residuals to decide which information is sent forward through the residuals? $y =  \\mathcal{F}(x,W\\{W_i\\}) + W_{S_i} x_i \\times \\sigma(W_{S_{i-1}} x_{i-1})$. Kind of like gating the residuals, or could use the residuals to gate?\n",
    "\n",
    "##### Mappings\n",
    "\n",
    "Aka, a way to add a prior?\n",
    "\n",
    "* Inverse? \n",
    "* Rotation? \n",
    "* $H(x) = e^x$ so that large values have greater weight.\n",
    "* \n",
    "\n",
    "##### Regularisation\n",
    "What if you tried to minimise/regularise the distance from the ideal mapping? A way of biasing toward linear identities? Kinda like occams razor? \n",
    "\n",
    "### Questions\n",
    "\n",
    "* Does this change anything related to being convex in inputs???\n",
    "* How does L1/L2 regulaisation work with this? Probably not very well...?\n",
    "* Allows you to forget more!!! (like my screen recordings?)\n",
    "* How are they initialised? With identities?\n",
    "* What happens if you use adaboost with a residual net? is it like double boosting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
