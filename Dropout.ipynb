{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is really weird.\n",
    "\n",
    "### What is it really doing?\n",
    "\n",
    "\n",
    "\n",
    "##### Discouraging co-adaption?\n",
    "\n",
    "The authors of the paper cite this as the main motivation for dropout. They compare dropout to meiosis and the merging of two seperate chromosomes.\n",
    "\n",
    "\n",
    "The metaphor goes ... quote?\n",
    "\n",
    "To be honest this metaphor doesnt really make that much sense to me, as\n",
    "\n",
    "Also, I somehow missed their justification for why co-adaption is a bad thing. Sure, in over sized neural networks .. But this does not apply to all networks.\n",
    "\n",
    "In my opinion co-adaption is where neural networks get their strength and is a form of compositionally. Allowing ...\n",
    "\n",
    "##### Robust features\n",
    "\n",
    "Another justification the authors cite in their paper is that \n",
    "\n",
    "Falsification?\n",
    "\n",
    "Wouldnt this just add redundancy?\n",
    "\n",
    "\n",
    "##### Adding invariance\n",
    "\n",
    "To network topology?\n",
    "\n",
    "##### Sparsifying \n",
    "\n",
    "Making features (more) independent?\n",
    "\n",
    "\n",
    "##### Regularisation\n",
    "\n",
    "##### \n",
    "\n",
    "##### Ensemble of weak learners\n",
    "\n",
    "Related to Boosting?\n",
    "\n",
    "Our weak learners here would be the smaller nets where some proportion of the network has been ‘dropped’. Th\n",
    "\n",
    "And we average across these weak-learners. (this sounds reasonable)\n",
    "And we average across different network architectures. (this does not)\n",
    "\n",
    "### Dropout is a convolution?\n",
    "\n",
    "Recently I have been studying convolutional networks. And as a result, I am looking at everything else through that lens.\n",
    "\n",
    "\n",
    "\n",
    "### Questions and thoughts\n",
    "* Averaging over different network topologies. How does this even make sense?\n",
    "\n",
    "\n",
    "When we train a net with dropout are we convolving possible structures of our net (a large assumption)?\n",
    "\n",
    "* That a small batch of the data approximates the true data (the same assumption made for SGD anyway?)\n",
    "\n",
    "\n",
    "\n",
    "References\n",
    "\n",
    "SwapNet\n",
    "Dropout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
