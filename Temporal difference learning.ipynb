{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions. Observe. Update beliefs. Repeat.\n",
    "\n",
    "\n",
    "What problem is TD learning solving? Why it this problem important? Why is TD better than alternatives?\n",
    "\n",
    "\n",
    "### Learning to Predict by the Methods of Temporal Differences (Sutton)\n",
    "\n",
    "Good paper. Build on each method as a special case of the last. \n",
    "\n",
    "***\n",
    "\n",
    "$\\bar V_t = \\sum_{i=0}^{\\infty} \\gamma^i r_{t+i} $\n",
    "\n",
    "where ${\\displaystyle 0\\leq \\gamma <1}  0 \\le \\gamma < 1 . $This formula can be expanded\n",
    "\n",
    "$ {\\displaystyle {\\bar {V}}_{t}=r_{t}+\\sum _{i=1}^{\\infty }\\gamma ^{i}r_{t+i}}  \\bar V_t = r_{t} + \\sum_{i=1}^{\\infty} \\gamma^i r_{t+i} $ by changing the index of i to start from 0.\n",
    "\n",
    "$ {\\displaystyle {\\bar {V}}_{t}=r_{t}+\\sum _{i=0}^{\\infty }\\gamma ^{i+1}r_{t+i+1}}  \\bar V_t = r_{t} + \\sum_{i=0}^{\\infty} \\gamma^{i+1} r_{t+i+1} $\n",
    "\n",
    "${\\displaystyle {\\bar {V}}_{t}=r_{t}+\\gamma \\sum _{i=0}^{\\infty }\\gamma ^{i}r_{t+i+1}}  \\bar V_t = r_{t} + \\gamma \\sum_{i=0}^{\\infty} \\gamma^{i} r_{t+i+1} $\n",
    "\n",
    "${\\displaystyle {\\bar {V}}_{t}=r_{t}+\\gamma {\\bar {V}}_{t+1}}  \\bar V_t = r_{t} + \\gamma \\bar V_{t+1} $\n",
    "\n",
    "Thus, the reward is the difference between the correct prediction and the current prediction.\n",
    "${\\displaystyle r_{t}={\\bar {V}}_{t}-\\gamma {\\bar {V}}_{t+1}}  r_{t} = \\bar V_{t} - \\gamma \\bar V_{t+1} $\n",
    "\n",
    "***\n",
    "\n",
    "But the true reward never gives us particularly much information. (?)\n",
    "\n",
    "\n",
    "\n",
    "We want to estimate $ Y_t=y_{t+1}+\\gamma y_{t+2}+\\gamma y_{t+3}+...=\\mathop{\\sum}_i^{\\infty} \\gamma^{iâˆ’1} y_{t+i}$. Where y could be, the bast policy, the value function/reward, the ?, ... ? (not just for RL).\n",
    "\n",
    "Still dont understand temporal discounting. We want to try to predict the future. But the further away that future is the less we care whether it is correct. That doesnt seem right... Of course we care whether it is correct, but we are just saying that \n",
    "\n",
    "Ok, I think the prediction part is good. But I dislike the .. ? should have explanations, hypotheses, ... and be testing them.\n",
    "\n",
    "$\\Delta V(s_i) = \\alpha ( \\mathop{\\sum_{j=1}^i} V(s_j) - V(s_{j-1}))$ is this right?\n",
    "\n",
    "### Definition\n",
    "\n",
    "So rather than just saying that;\n",
    "\n",
    "* given the reward, update all outputs proportional to their error (MC). $f(x_i) \\leftarrow f(x_i) + \\alpha (R - f(x_i)) = (1-\\alpha) f(x_i) + \\alpha R$. (hmm, so $\\alpha$ is just a bias towards the present or past.\n",
    "* given the reward, update all outputs proportional to ?? $f(x_i) \\leftarrow f(x_i) + \\frac{\\alpha}{i} \\sum_{j=1}^i R_j - f(x_j)$\n",
    "    * Case 1: $f(x_i) \\leftarrow f(x_i) + \\frac{\\alpha}{i} \\sum_{j=1}^i R_j + f(x_{j-1})- f(x_j)$\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "* How should I (learn to) update my predctions based on new information?\n",
    "* We are learning the rewards function by trying to predict it. (could backprop wrt to this? assuming its accuate?)\n",
    "* Why isn't temporal difference learning used in RNNs? Try to predict the next input/activations??\n",
    "* Has anyone tried to learn to predict the lambda coefficients? Aka to think about which bits of information are important for making predictions, and how you should learn from them.\n",
    "* What about using an attention mechanism for updating weights that uses suprise to guide it?\n",
    "* Is this just backprop?? for the case where we get one reward at then end of a sequence. \n",
    "* Reminds me of a resnet??\n",
    "\n",
    "### Thoughts\n",
    "\n",
    "Imagine we have a binary decision. And the truth is 9999:1 chance of win:loss. However, we are presented with an outcome, loss. Thus we update our model/parameters so that next time we will predict a loss. (aka statistics) What we really want is a way to say, I cannot explain that loss, don't update the model. (or do we?)\n",
    "\n",
    "Sometimes there is not a better prediction you could have made given the information you had. E.g. an election. You predict 50:50 at the start, however polls quickly start show that there is likely to be a clear winner. So you update you preduction 90:10. Was your initial prediction wrong, or in need of updating so it is closer to the truth next time? It depends on the method used to generate the prediction, not necessarily the statistics we have observed. Hmm is this true? I am putting explanations against statistics. But really they go together. (?)\n",
    "\n",
    "It kind of like a contstraint of being continuous. Each prediction should be connected to 'close' states. (but this isnt necessarily true...)\n",
    "\n",
    "### Resources\n",
    "\n",
    "* http://www.scholarpedia.org/article/Temporal_difference_learning\n",
    "* Good judgement project??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Simple example\n",
    "import numpy as np\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
