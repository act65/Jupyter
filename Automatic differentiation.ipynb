{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why?\n",
    "\n",
    "* Recursive chain rule means we dont have exploding complexity?\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What?\n",
    "\n",
    "\n",
    "Automatic gradients are not calculated symbolically. Neither are the calculated empirically. What?! How the hell are they computed then?\n",
    "\n",
    "Forward vs backward -- applications?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "\n",
    "Want - $f(x+\\epsilon x′)=f(x)+\\epsilon f′(x)x′$ - aka the chain rule?\n",
    "\n",
    "$$\n",
    "z_0 = f(x), z_1 = g(X) \\\\\n",
    "z_2 = h(z_0,z_1) \\\\\n",
    "\\frac{d z_2}{d x} = \\frac{\\partial z_2}{\\partial z_0} \\frac{d z_0}{d x} + \\frac{\\partial z_2}{\\partial z_1} \\frac{d z_1}{d x}\\\\\n",
    "$$\n",
    "\n",
    "I cant remember where I saw it.\n",
    "\n",
    "They use a different type of algebra. So;\n",
    "* $x + y = z \\rightarrow (x + dx) + (y + dy) = (x + y) + (dx + dy) = z + dz$.\n",
    "* $xy = z \\rightarrow (x + dx)(y + dy) = (xy) + (dxdy) = z + dz$.\n",
    "    * still not sure how this one works\n",
    "* $f(x) = y \\rightarrow ???$\n",
    "\n",
    "The cool thing is that this algebra encodes the idea of the chain rule, ... Its grammar ? enforces the chain rule, as well as the usual binary operations.\n",
    "\n",
    "### Matrix\n",
    "\n",
    "Reverse - trace?\n",
    "\n",
    "\n",
    "\n",
    "### Dual numbers\n",
    "\n",
    "Look into this abit? The ring of real numbers and $\\epsilon$. Symmetries?\n",
    "\n",
    "> To do forward mode AD on a program, do a nonstandard interpretation of the program, as follows. Define “dual numbers” as formal truncated Taylor series of the form $x+\\epsilon x′x+\\epsilon x′$. Define arithmetic on dual numbers by $\\epsilon 2=0 \\epsilon 2=0$, and by interpreting any non-dual number $yy$ as $y+\\epsilon 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\n",
    "$e^x = e^{x+\\epsilon} = ???$\n",
    "\n",
    "$log(x) = log(x+\\epsilon) = ??$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop\n",
    "\n",
    "Is a special case.\n",
    "\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How?\n",
    "\n",
    "### Source code transformation\n",
    "\n",
    "\n",
    "\n",
    "### Operator-overloading\n",
    "\n",
    "\n",
    "### New programming language...\n",
    "\n",
    "\n",
    "### ??? Ideas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "> _For n > 1 and m > 1 there is a golden mean, but finding the optimal way is probably an NP-hard problem_ [lec notes](http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf) \n",
    "\n",
    "Proof?!?\n",
    "\n",
    "\n",
    "> and where differentiating an approximation to $f$ produces much worse answers than explicitly approximating the (known) derivative of $f$. [alexey radul](http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/)\n",
    "\n",
    "E.g. numerical integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "### Automatic differentiation\n",
    "* [Great intro](http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/)\n",
    "* http://conal.net/blog/posts/what-is-automatic-differentiation-and-why-does-it-work\n",
    "* http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/\n",
    "* http://blog.sigfpe.com/2005/07/automatic-differentiation.html\n",
    "* [Haskell implementation](http://www.danielbrice.net/blog/10/)\n",
    "* [Julia implementation](http://int8.io/automatic-differentiation-machine-learning-julia/#Reverse_mode_automatic_differentiation_8211_basic_bits)\n",
    "\n",
    "### Backprop\n",
    "\n",
    "* http://int8.io/backpropagation-from-scratch-in-julia-part-ii-derivation-and-implementation/\n",
    "* http://colah.github.io/posts/2015-08-Backprop/\n",
    "\n",
    "### Other\n",
    "* https://en.wikipedia.org/wiki/Dual_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* What about integration?\n",
    "* How is this related to functional programming and transforms on data?\n",
    "* Is there a way nature could be doing this using complex numbers (in place of nilpotent duals)? ?!???\n",
    "* Relation to dynamic programming (colah mentioned this)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "version": "7.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
