{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient estimation\n",
    "\n",
    "* Why do we need to do this?\n",
    "* Why does backprop not work with stochastic functions?\n",
    "* \n",
    "\n",
    "\n",
    "* [MuProp: Unbiased Backpropagation for Stochastic Neural Networks](http://arxiv.org/abs/1511.05176)\n",
    "* [Gradient Estimation Using Stochastic Computation Graphs](http://arxiv.org/abs/1506.05254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Imagine a single layer feed forward neural network, with some Loss function = $f_L$, parameters = W, input = x, output = y.\n",
    "$$\n",
    "\\begin{align}\n",
    "&= \\frac{\\partial}{\\partial W} \\mathop{\\mathbb{E}}_{x\\sim X} \\big[ f_L(x,W) \\big] \\tag{definition}\\\\\n",
    "&=   \\mathop{\\mathbb{E}}_{x\\sim X} \\big[ \\frac{\\partial}{\\partial W} f_L(x,W) \\big]  \\tag{because x is not dependent on W} \\\\\n",
    "&= \\frac{1}{N} \\mathop{\\sum}_i \\big( \\frac{\\partial}{\\partial W} f_L(x_i,W) \\big)  \\tag{because X uniform} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which leads us to the traditional SGD. However, if we also want to learn which x values to sample from X, the we need those gradients as well. E.g. some form of active learning then we have \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&= \\frac{\\partial}{\\partial \\phi} \\mathop{\\mathbb{E}}_{\\phi \\sim \\Phi} \\big[ f_L(g(\\phi),W) \\big] \\tag{definition}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but we dont really have a distribution on $\\phi$/ it is just = 1 for a single element of $\\Phi$. So cant we get rid of the expectation??? Nah, how does the sampling work now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.random((1,784))\n",
    "W = np.random.random((784,10))\n",
    "T = np.random.random((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))\n",
    "\n",
    "def Deterministic(x,W):\n",
    "    return Softmax(np.dot(x,W))\n",
    "\n",
    "def Stochastic(x,W):\n",
    "    y = Deterministic(x,W)\n",
    "    z = np.random.choice([n for n in range(10)],p=y.squeeze())\n",
    "    onehot = np.zeros((1,10))\n",
    "    onehot[0,z] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.66403521e-01   1.41200475e-04   9.17651759e-07   6.53917560e-02\n",
      "    2.08619066e-02   2.35149785e-01   2.14904900e-03   1.49700438e-04\n",
      "    5.85828240e-03   3.89388216e-03]]\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Deterministic(x,W))\n",
    "print(Stochastic(x,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic works\n",
      "hmm...\n"
     ]
    }
   ],
   "source": [
    "def DeterLoss(W,x):\n",
    "    return np.sum(T-Deterministic(x,W))\n",
    "\n",
    "def StocLoss(W,x):\n",
    "    return np.sum(T-Stochastic(x,W))\n",
    "\n",
    "dL = autograd.grad(DeterLoss)\n",
    "sdL = autograd.grad(StocLoss)\n",
    "\n",
    "try:\n",
    "    y = dL(W,x)\n",
    "    print('Deterministic works')\n",
    "    stoc_y = sdL(W,x)\n",
    "    print('Stochastic works')\n",
    "except:\n",
    "    print('hmm...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so;\n",
    "* Autograd could (??) backprop through the `random.choice` function, but that doesnt seem like it solves anything.\n",
    "* Need some unbiased estimator? But now the framework wont be functional anymore? As we need states to remember things, so we can calculate a mean field? Or we just do a large amount of expensive MC simulations?\n",
    "    * Wait a minute. All we are trying to do with these MC simulations are trying to estimate the probability distribution. In many cases we already know this, like above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
