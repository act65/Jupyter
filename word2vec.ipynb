{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tr\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torch.sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 300\n",
    "n = 4096\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = '/Volumes/ExtraDiskSpace/Documents/testing_word_encodings/dorian-grad_encoded-bpe4096.txt'\n",
    "with open(fname, 'rt') as f:\n",
    "    raw = f.read()\n",
    "onehots = np.array(raw.replace('\\n', '').split(' ')).astype(np.int32)\n",
    "onehots = onehots[onehots<n]\n",
    "N = len(onehots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tr.from_numpy(onehots).type(tr.LongTensor)\n",
    "vec = Variable(torch.randn(n, d).type(tr.FloatTensor), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram training.\n",
    "\n",
    "There is a trade-off between efficient evaluation of the loss and the number of times we need to run the loss.\n",
    "\n",
    "Imagine we have `the quick brown fox jumps` with a skip-gram window of 2. Then the skip-grams are;\n",
    "\n",
    "* the - quick, brown\n",
    "* quick - the, brown, fox\n",
    "* brown - the, quick, fox, jumps\n",
    "* fox - quick, brown, jumps\n",
    "* jumps - brown, fox\n",
    "\n",
    "__>>>__ Already we have thrown away relative positions, is this important? We could use it to weight the loss?\n",
    "\n",
    "Next, we turn each of these bags of words into pairs for training (x - y).\n",
    "\n",
    "* the - quick, the - brown\n",
    "* quick - the, quick - brown, quick - fox\n",
    "* ...\n",
    "* jumps - brown, jumps - fox\n",
    "\n",
    "__>>>__ Why do we need to pair? Because we want to use onehot training, which can be evaluated efficiently? Bu, now we have to evaluate it again for each pairing. This seems weird as we are treating the output distrubution as a set conditionally independent variables, when they are really just indpenedent (ie sigmoid not softmax?).\n",
    "\n",
    "<!-- there is actually another stop of logic in here, project into some lower dim space-->\n",
    "\n",
    "Next we train to maximise the likelihood of each pairing using softmax to calculate the probability distribution over each input token, the likelihood of each token being in the skip-gram of the input token.\n",
    "\n",
    "$$p(z) = \\sigma(W^T Wx)$$\n",
    "\n",
    "__>>>__ Wait a minute. Is W not usual shared? I.e. We have $ABx$? And set B as the word vectors.\n",
    "\n",
    "$$Ïƒ_{ij} = \\frac{exp(w_i^T c_j ) }{1+exp(w_i^T c_j )}$$\n",
    "\n",
    "Jokes...\n",
    "> This update has been shown to be equivalent to the gradient of a factorization of a pointwise mutual information matrix. (Levy and Goldberg, 2014)\n",
    "\n",
    "__>>>__ What about orthogonal W? Then we dont have to project??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skip_gram(x, window):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: A sequence of tokens as integers\n",
    "        window: the size (int) of window to use when constucting skip-grams\n",
    "    Returns:\n",
    "        generator then yields pairs of word-context. (int, [int]*window)\n",
    "    \"\"\"\n",
    "    for i in range(window, len(x)- window):\n",
    "        yield x[i], tr.cat([x[i-window:i], x[i+1:i+window+1]], 0)\n",
    "        \n",
    "def batch(seq, batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq: a sequence of pairs in  (int, [int]*window)\n",
    "        batch_size: size of batch\n",
    "    Returns:\n",
    "        generator that yields \n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i, pair in enumerate(seq):\n",
    "        x, t = pair\n",
    "        inputs.append(tr.from_numpy(np.array(x).reshape((-1, 1))))\n",
    "        targets.append(t.view(1, -1))\n",
    "        if i % batch_size == batch_size-1:\n",
    "            yield tr.cat(inputs, 0), tr.cat(targets, 0)\n",
    "            inputs = []\n",
    "            targets = []\n",
    "            \n",
    "# TODO need some shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1]) torch.Size([128, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "window = 30  \n",
    "# it is quite cheap to increase window size in this formulation!??!!\n",
    "# just a few more non zero entries in the targets\n",
    "\n",
    "words, context = next(batch(skip_gram(tokens, window), batch_size))\n",
    "print(words.size(), context.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 300])\n"
     ]
    }
   ],
   "source": [
    "batch_x = vec[words.view(batch_size)].view((batch_size, d))\n",
    "\n",
    "# use a subset of all the vectors. negative sampling\n",
    "# positive_samples = vec[t.resize((batch_size*window*2))].view((batch_size, window*2, n))\n",
    "# n_negatives = 12\n",
    "# negative_samples = vec[rand_int(n_negatives)].view((batch_size, window*2*n_negatives, n))\n",
    "# samples = \n",
    "# print(positive_samples.size(), negative_samples.size()))\n",
    "\n",
    "print(batch_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these wont change over the iterations.\n",
    "batch_idx = tr.stack([tr.arange(0, batch_size) for _ in range(window*2)], 1).type(tr.LongTensor)\n",
    "v = tr.ones(batch_size, window*2).type(tr.FloatTensor)\n",
    "I = Variable(tr.eye(n).type(tr.FloatTensor), requires_grad=False)\n",
    "\n",
    "learning_rates = list(reversed(np.exp(np.linspace(np.log(0.00001), np.log(0.001), epochs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh2jRV0opH/L/TLHMPsnSU6AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10eca8358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gaussian_fn(mean, var):\n",
    "    d = np.sqrt(2*3.142*var**2)\n",
    "    n = -(mean - np.linspace(0, window*2-1, window*2))**2\n",
    "    return np.exp(n/(2*var**2))/d\n",
    "\n",
    "gaussian = gaussian_fn(window, window)\n",
    "gaussian = gaussian/np.max(gaussian)\n",
    "plt.plot(gaussian)\n",
    "gaussian = tr.Tensor(gaussian).type(tr.FloatTensor)\n",
    "# scale the context by distance from center\n",
    "v = v*gaussian  # can do this because the indices are still in order\n",
    "# TODO what if we see the same word multiple times\n",
    "# within the same contex? should additively increase\n",
    "# the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loss: 8033.40625562555"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b3560bfa2cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m### auto grad and optimise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExtraDiskSpace/Documents/testing_word_encodings/venv/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/ExtraDiskSpace/Documents/testing_word_encodings/venv/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for i, pair in enumerate(batch(skip_gram(tokens, window), batch_size)):\n",
    "        pair = words, context\n",
    "        ### pick out training word vectors\n",
    "        # could pick a subset of w to  compare against -- ie negative sampling\n",
    "        # doesnt seem like there is much to be gained?\n",
    "        batch_x = vec[words.view(batch_size)].view((batch_size, d))\n",
    "\n",
    "        ### do a similarity search to the rest of our vectors\n",
    "        # (batch_size x v_dims) x (v_dims x n_tokens)\n",
    "        y = tr.matmul(batch_x, tr.transpose(vec, 1, 0))  # shape = [batch_size x n_tokens]\n",
    "\n",
    "        ### construct targets\n",
    "        idx = tr.stack([batch_idx.view(-1), context.view(-1)])\n",
    "        onehots = torch.sparse.FloatTensor(idx, v.view(-1), torch.Size([batch_size,n])).to_dense()\n",
    "        onehots = Variable(onehots, requires_grad=False)\n",
    "        # by using ones as the targets we are training the vectors to be orthonormal??\n",
    "\n",
    "        ### caluclate loss\n",
    "        loss = tr.matmul(tr.transpose(y, 1, 0), onehots) - I\n",
    "        loss = tr.abs(loss).sum(dim=1).mean(dim=0)\n",
    "\n",
    "        ### auto grad and optimise\n",
    "        loss.backward()\n",
    "        vec.data -= learning_rates[e] * vec.grad.data\n",
    "\n",
    "        # print('\\r step: {}'.format(i), end='', flush=True)\n",
    "        print('\\r loss: {}'.format(loss.data.numpy()[0]), end='', flush=True)\n",
    "np.save('/Volumes/ExtraDiskSpace/Documents/testing_word_encodings/wordvectors.npy', \n",
    "        vec.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wait a minute, do I even need to train this? Can I just solve it?\n",
    "* Can we track the variance of the gradients to estimate whether there are multiple different meanings/contexts to a token?\n",
    "* Only learning correlations. How can we capture the ordering as well?\n",
    "    * Should we add a gaussian window over the skip-gram rather than uniform?\n",
    "    * Position embeddings? Grammar embeddings?\n",
    "    * Could we use asymetric (ordering matters) relation networks to capture the syntax/dependencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we really doing? \n",
    "\n",
    "Want a way to index each word in a meaningful way. Ie, we want to learn an ordering of the different word indexes (vectors, encodings, ...) so \n",
    "\n",
    "Could imagine just trying to shatter all the different vectors, i.e. making them orthogonal. Would require n dimensions. But, we can do better than this by ordering them in meaningful ways.\n",
    "\n",
    "Why not just use a binary hash of the 4096 tokens? This means we would only have 12 dimensions in our embeddings!?! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
