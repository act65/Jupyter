{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear relations: Derivatives and variance\n",
    "\n",
    "$$\n",
    "cov(Y, X) = \n",
    "\\begin{bmatrix}\n",
    "var(y_1, x_1) & \\dots & var(y_1, x_m)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "var(y_n, x_1) & \\dots & var(y_n, x_m)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_m}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\dots & \\frac{\\partial y_n}{\\partial x_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Best linear approximation is $y = \\mathcal J x$?\n",
    "\n",
    "\n",
    "#### Relation to derivatives\n",
    "\n",
    "Baldi et al. 2012. What do the weight matrices learn? In the case of $y = ABx, B = (A^*A)^{-1}A^* \\Sigma_{yx}\\Sigma_{xx}^{-1}$. Where $A(A^*A)^{-1}A^*$ is just some low rank projection, and in the case where rank of A and B is of the same rank as the data, then we get $A(A^*A)^{-1}A^* = I$, therefore $y = ABx =  \\Sigma_{yx}\\Sigma_{xx}^{-1}x$\n",
    "\n",
    "Therefore, $\\mathcal J = \\Sigma_{yx}\\Sigma_{xx}^{-1}$ ... in which cases?! And this makes perfect sense as $\\Sigma_{yx}\\Sigma_{xx}^{-1} = \\frac{rise}{run}$. An estimate for the rise over the run. Aka the gradient.\n",
    "\n",
    "***\n",
    "\n",
    "What is variance?\n",
    "\n",
    "$Var (X)=\\mathbb E [(X-\\mu)^2]$\n",
    "(what about moments?)\n",
    "\n",
    "In one dimension, $ \\Sigma_{yx}\\Sigma_{xx}^{-1}$ is equivalent to (using cov(y,x) = E[(x-m)(y-m)])\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_i (y_i- \\hat y)(x_i - \\hat x)}{\\sum_j (x_j - \\hat x)^2} \\\\\n",
    "\\frac{\\sum_i (y_i- \\hat y)}{\\sum_j (x_j - \\hat x)} \\\\\n",
    "\\sum_i \\frac{(y_i- \\hat y)}{(x_i - \\hat x)} \\\\\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "So this means that we are learning a factorisation of the jacobian?!?\n",
    "Framed differently, It seems clearer that we are learning a linear approximation.\n",
    "\n",
    "Pick a linear approx $y = Mx$. \n",
    "Calculate M via $ \\Sigma_{yx}\\Sigma_{xx}^{-1}$.\n",
    "How about matrix solvers? Under-determined solution.\n",
    "Why does the variance show up here?\n",
    "\n",
    "#### Hessians\n",
    "\n",
    "$$\n",
    "f \\circ g = h \\\\\n",
    "\\mathbb R^n \\mathop{\\rightarrow}^f \\mathbb R^m \\mathop{\\rightarrow}^g \\mathbb R^p \\\\\n",
    "\\mathbf H_h = (I_p \\otimes \\nabla f^T)\\cdot \\mathbf H_g \\cdot \\nabla f +  (\\nabla_g \\otimes I_n) \\cdot \\mathbf H_f \\\\\n",
    "$$\n",
    "\n",
    "derivatives are linearisations. are linear operators and can be represented in matrix form.\n",
    "\n",
    "approximating the hessian with lower order samples, and using their variance. that denoising - bengio paper?\n",
    "\n",
    "\n",
    "positive semidefinite structure on hessians can be important. what other structures are menaingful? so could we do a non-negative matrix factorisation?! or ..?\n",
    "\n",
    "#### nth order moments and derivatives\n",
    "\n",
    "#### Correlations of correlations. \n",
    "\n",
    "* Is this some sort of tensor product? \n",
    "* What relation to the hessian?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCA and mutual information?\n",
    "http://www.imt.liu.se/~magnus/cca/tutorial/tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring and grad descent!?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Scoring_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetry and compression\n",
    "\n",
    "Let $f: x \\rightarrow y$. If $f$ is invariant to $T(x)$ -- i.e. $f(T(x)) = f(x)$-- this implies that there is some sort of summetry in $\\nabla f$.\n",
    "\n",
    "Cool. \n",
    "\n",
    "> How does structure in the jacobian of $f$ relate to invariance and equivariance properties if $f$?\n",
    "\n",
    "\n",
    "If $\\frac{\\partial y}{\\partial x_i} = 0$ then we could say that $y$ is invariant to $x_i$.\n",
    "Want we really want is to learn a contractive function that gives us a homomorphic representation.\n",
    "A mapping that preserves all directions of change, but embedded in a lower dimension.\n",
    "\n",
    "Let $\\mathcal J(x)$ be the true, high dimensional jacobian.\n",
    "Want a mapping, with (is there such thing as a non-linear determinant?) determinant < 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rates of change, compression and contraction\n",
    "\n",
    "Is contraction equivalent to compression? \n",
    "\n",
    "<u>Definition</u>. A linear operator is contractive if $ \\forall i: \\parallel J_{ii} \\parallel < 1$ Not true. it's the eigen vectors?\n",
    "\n",
    "I.e the rate of change of $x$ w.r.t $y$ is decreasing. So if we had a neural networks, where every layer was contractive, then every layer would be reducing the effect $x$ has on $y$. (symmetries, invariance, want to direct this contraction along the right geometry, compress info...)\n",
    "\n",
    "\n",
    "What about;\n",
    "\n",
    "* entropy?\n",
    "* score? $\\nabla \\mathbb E [f(x)] = \\mathbb E[f(x) \\nabla log p(x)]$\n",
    "* also relation to denoising?! (what about local noise? only in a subspace, or locally correlated?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise, infomation and gradients\n",
    "\n",
    "Two perspectives to compression of info via adding noise;\n",
    "* entropy of ?!? signals stuff\n",
    "* adding noise == minimising the jacobian\n",
    "* \n",
    "\n",
    "#### Maximising info\n",
    "\n",
    "$\\mathrm {H} (X)=\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\,\\mathrm {I} (x_{i})}=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{b}\\mathrm {P} (x_{i})}$\n",
    "\n",
    "#### Invariance to pertubations -- Denosing\n",
    "\n",
    "Denoising autoencoder $\\equiv$ contractive autoencoder, in the limit of small variance on the noise.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal L &= \\parallel y - f(x + \\epsilon)\\parallel _2^2 \\\\\n",
    "&= \\parallel y - f(x) + f(\\epsilon)\\parallel _2^2 \\tag{if $f$ is linear} \\\\\n",
    "&\\dots \\\\\n",
    "\\frac{\\partial \\mathcal f(x)}{\\partial x} &= \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x + \\epsilon) - f(x)}{\\epsilon} \\tag{finite difference}\\\\\n",
    "&= \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x) + f(\\epsilon) - f(x)}{\\epsilon} \\tag{if $f$ is linear}\\\\\n",
    " &= \\lim_{\\epsilon \\rightarrow 0}\\frac{f(\\epsilon)}{\\epsilon} \\tag{if $f$ is linear} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "__ WHat if $f$ is nonlinear...__\n",
    "\n",
    "***\n",
    "\n",
    "So minimizing the values (no not values, but rank, yes??? no, the product of the singular values??) of J is equivalent to minimizing the description length?\n",
    "\n",
    "\n",
    "Which is cheaper/more effective?? noise or grads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
