{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dual():\n",
    "    def __init__(self,val,grad):\n",
    "        self.val = val\n",
    "        self.grad = grad\n",
    "        \n",
    "    def __add__(self,x):\n",
    "        return Dual(self.val + x.val,self.grad + x.grad)\n",
    "    \n",
    "    def __mul__(self,x):\n",
    "        return Dual(self.val*x.val,self.grad*x.val + self.val*x.grad)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '('+str(self.val) + ' , '+ str(self.grad)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9 , 15)\n"
     ]
    }
   ],
   "source": [
    "print(Dual(3,1) * Dual(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13 , (11 , 9))\n",
      "((30 , 0) , (47 , 34))\n"
     ]
    }
   ],
   "source": [
    "print(Dual(3,Dual(2,1)) + Dual(10,Dual(9,8)))\n",
    "print(Dual(Dual(3,0),Dual(2,1)) * Dual(Dual(10,0),Dual(9,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0(0(0)(1))(3))\n",
      "(0(0(2)(3))(2))\n",
      "(0(0(4)(1))(2))\n"
     ]
    }
   ],
   "source": [
    "class DS():\n",
    "    \"\"\"\n",
    "    A tree ??? \n",
    "    of derivatives\n",
    "    \"\"\"\n",
    "    def __init__(self,n,m,value=np.random.randint(0,5),lchild='',rchild=''):#n = vars, m = order\n",
    "        self.n = n; self.m=m\n",
    "\n",
    "        if n ==0 or m==0:\n",
    "            self.value = np.random.randint(0,5)\n",
    "            self.lchild = lchild\n",
    "            self.rchild = rchild\n",
    "        else:\n",
    "            self.value = value\n",
    "            self.lchild = DS(n-1,m,value) \n",
    "            self.rchild = DS(n,m-1,value)\n",
    "\n",
    "            \n",
    "    def __str__(self): #doesnt work well...\n",
    "        return '(' + str(self.value) + str(self.lchild) + str(self.rchild) + ')'\n",
    "    \n",
    "    def __add__(self,x):#can make this algorithm general for all operations!\n",
    "        #recursively add\n",
    "        if (self.n == 0 or self.m == 0) and (x.n == 0 or x.m == 0):\n",
    "            return self.value+x.value\n",
    "        else:\n",
    "            return DS(self.n,self.m,\n",
    "                          self.value+x.value,\n",
    "                          self.lchild+x.lchild,\n",
    "                          self.rchild+x.rchild)\n",
    "\n",
    "\n",
    "a = DS(2,1) #must be the same shape???\n",
    "b = DS(2,1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. so this is like the opposite of autograd? \n",
    "* autograd keeps track of the transforms/computational graph and then uses that knowledge to calculate derivatives later.\n",
    "* This algorithm calculates all the derivatives as it goes. Which might be ok in a lazy language, but..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
