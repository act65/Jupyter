{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a parameterised optimisation function (e.g. SGD) which you use to find good minima of other parameterised functions, e.g. neural networks. \n",
    "\n",
    "I want to know the best way to optimise a given (type of) function, e.g. CNNs. How can I do this? Optimise the optimiser? \n",
    "\n",
    "So this is also know as hyperparameter optimisation or meta-learning. The techniques used are ... grid/random searches ...??? \n",
    "\n",
    "Instead, could we use the optimiser we already have? Afterall, optimisation is just a function. We have a loss function, the number of steps required to get to a minima, or the ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verson 0.1\n",
    "\n",
    "For simplicity we consider SGD as a parameterised function (parameterised by the learning rate).\n",
    "\n",
    "```python\n",
    "#A simple function to be optimised\n",
    "def f(x):\n",
    "    return x*2\n",
    "\n",
    "#A simple convex optimiser with a single parameter\n",
    "def optimise(func,params):\n",
    "    while x[steps+1]-x[steps] > tolerance:\n",
    "        x[steps+1] = x[steps] - params * grad(func,x[steps])\n",
    "        steps +=1\n",
    "    return x,steps\n",
    "    \n",
    "#Recursively call the optimiser on itself\n",
    "def recusrive_optimiser(func,params,depth):\n",
    "    if depth == 1:\n",
    "        return optimise(f,params)\n",
    "    else:\n",
    "        return optimise(recursive_optimiser,params,depth-1)\n",
    "\n",
    "\n",
    "def g(x):\n",
    "    return x*4\n",
    "    \n",
    "min_f,params = recursive_optimiser(f,init,3)\n",
    "min_g,params = recursive_optimiser(g,params,3)\n",
    "```\n",
    "\n",
    "* Wont this result in all optimisers (with a call) having the same parameters?\n",
    "* So weight tying across optimisers, how does this make sense?\n",
    "* What should I be doing with the outputs of the optimisers, while deep in the recursion?\n",
    "* How is this related to data points and gradients evaluated at a point?\n",
    "* The point is that it optimises a function as well as itself (to optimise the function faster).\n",
    "* What does depth mean here? I get what depth two means, optimise the optimiser. But what does more depth do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verson 0.2\n",
    "\n",
    "Just taking one step in the right direction from data. So the optimier should look at how much the step taken reduced the loss function and seek to maximise the reduction? Could this lead to poor places?\n",
    "\n",
    "```python\n",
    "#A simple function to be optimised\n",
    "def f(x):\n",
    "    return x*2\n",
    "    \n",
    "#Recursively call the optimiser on itself\n",
    "def recusrive_optimiser(func,params,depth):\n",
    "    if depth == 1:\n",
    "        return x - params * grad(func,x[steps])\n",
    "    else:\n",
    "        return \n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
