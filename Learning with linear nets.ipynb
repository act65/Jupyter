{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, an AE is trying to maximise the variance each middle node sees across the dataset. But they also interefere with eachother, competitively? Why do they learn this??? More variance captures more 'information'? Easier to separate?\n",
    "\n",
    "### Questions\n",
    "\n",
    "* Does linear imply convexity?? Yes and no.(?)\n",
    "* So then the minimal loss comes from using the principle components.\n",
    "* How does the update rule imply we will learn the principle components?\n",
    "* If we initialised every weight to be the same, would it end up learning the same components?\n",
    "* Why does it learn perpendicular directions? Caputres more information, but how does it know that???\n",
    "* Without biases??\n",
    "* Multi layer nets? Doing PCA of PCA... if linear this doesnt make sense. But why does it make sense for non-linear layers?\n",
    "* Why does MSE equal $(x-y)^2$ rather than $(y-x)^2$??\n",
    "* Not having tied weights, aka having more parameters may help training as the problem will be convex wrt to each weight.\n",
    "* So for any linear network $y = ABx$ we can rewrite it as $y = Cx$ where $C = A\\cdot B$. So any linear network is really just a one layer net? Or do we actually get something from depth in linear networks?\n",
    "\n",
    "\n",
    "##### Let's go through a smple 2d case (weights tied).\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{c}   y_1\\\\ y_2 \\\\ \\end{array} \\right]\n",
    "= \\begin{bmatrix} \n",
    "W_1 \\\\ \n",
    "W_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "W_1 & W_2 \\\\ \n",
    "\\end{bmatrix} \\left[ \\begin{array}{c}  x_1\\\\ x_2\\\\ \\end{array} \\right] \\\\\n",
    "= \\left[ \\begin{array}{c}  W_1(W_1x_1 + W_2x_2)   \\\\ W_2(W_1x_1 + W_2x_2) \\\\ \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= (\\textbf{x}-\\textbf{y})^2\\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "(x_1 - W_1(W_1x_1 + W_2x_2) )^2  \\\\ \n",
    "(x_2 - W_2(W_1x_1 + W_2x_2) )^2 \\\\ \n",
    "\\end{array} \\right] \\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "(x_1 - W_1^2x_1 - W_1W_2x_2)^2  \\\\ \n",
    "(x_2 - W_2W_1x_1 - W_2^2x_2)^2 \\\\ \\end{array} \\right] \\\\\n",
    "&= \\left[ \\begin{array}{c}  \n",
    "  \\\\ \n",
    " \\\\ \\end{array} \\right] \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs and PCA ... Baldi et al.\n",
    "\n",
    "where does _\"without local minima\"_ come into it?\n",
    "\n",
    "### Proof\n",
    "\n",
    "(supposed) fact:  E(A,B) is convex in the coefficients of B and attains its minimum for B satisfying $A^T\\Sigma_{YX} = A^T A B \\Sigma_{XX}$\n",
    "\n",
    "##### Definitions\n",
    "$y_t$ is some target, A and B are matrices (of rank p??) and $x_t$ is the input. $E(A,B) = \\sum_t \\parallel y_t - ABx_t\\parallel^2 $\n",
    "\n",
    "\\begin{align}\n",
    "E(A,B) &= \\sum_t \\parallel y_t - ABx_t\\parallel^2 \\\\\n",
    "&=\\parallel Y - ABX\\parallel^2 \\tag{not sure about this. sum vs norm}\\\\\n",
    "&= (Y - ABX) (Y^T - (ABX)^T) \\tag{well... that depends on what type of norm}\\\\\n",
    "&= YY^T -ABX Y^T - Y(ABX)^T+ABX(ABX)^T \\tag{hmm. order matters, what is right?} \\\\\n",
    "\\frac{\\partial E(A,B)}{\\partial B} &= \\frac{\\partial }{\\partial B} \\parallel Y - ABX\\parallel^2 \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact solutions ... Saxe et al.\n",
    "Andrew\n",
    "\n",
    "> _Our fundamental goal is to understand the dynamics of learning as a function of the input statistics $\\Sigma_{XX}$ and input-output statistics $\\Sigma_{YX}$._\n",
    "\n",
    "Hmm. So this is the goal of all learning algorithms?\n",
    "\n",
    "> _To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where $\\Sigma_{XX} = I$. This assumption will hold exactly for whitened input data, a widely used preprocessing step._\n",
    "\n",
    "\n",
    "$\\Sigma_{YX} = U_{mxm}S_{mxn}V_{nxn}^T =\\sum^{n}_{i=1} s_iu_iv_i^T$\n",
    "\n",
    "But the indicies dont work out?? as there are m lots of $u_i$ vectors in U, but we are only looking at n of them. So either $n>m$ and $u_n$ doesnt exist, or $n<m$ and we have not used all of them. And u_i.v_i doesnt work?!? Oh... we are talking about the covariance matrix, so m = n... Wait are we?\n",
    "\n",
    "\n",
    "* $u_i$ reflect the independent modes of variation in the output,\n",
    "    * so U and $\\Sigma_{YX}$ are also decorrelated??\n",
    "* $v_i$ reflect the independent modes of variation in the input,\n",
    "* $s_i$ are the singular values -- which mean __?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random idea: Orthogonal weights and rotating updates\n",
    "\n",
    "Initialise the weights to be orthonormal, then only update the weights using rotations. Why? As we dont want to bother learning orthogonal weights, the principle components (and normalising is just nice). As orthogonal weights are optimal (in non-linear systems as well????)\n",
    "\n",
    "```python\n",
    "weights = random.init(shape,'orthonormal')\n",
    "\n",
    "for e in range(epochs):\n",
    "    grads = dL_dw(weights,batch)\n",
    "    \n",
    "    \n",
    "    weights = np.dot(weights,???) #must be matmul to rotate?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
