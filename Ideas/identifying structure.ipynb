{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding structure__ in dense tensors. \n",
    "\n",
    "* If we see that as a tensor is being trained it is tending towards (within some tolerance) a certain structure. \n",
    "* Then reinitialise tensor with that structure. (but many structures can end up creating the same output -- need more data? or better priors) \n",
    "* If that effects loss (how???) then change back to dense? \n",
    "\n",
    "You would start with a fully connected/dense network of tensors, and you could slowly convert each tensor into something more structured as they converge (if grads start to diverge then change back to dense). (reminds me of the story behind cognitive development -- many connections initially, then we specialise and cut down connections -- not sure how true this story is tho)\n",
    "\n",
    "(!!! this is a cool direction. finding structure and exploiting)\n",
    "\n",
    "How can we know what structure is present? \n",
    "The directions/grads that the tensor wants to/has changed. (must all fit within the same structure)\n",
    "Does this mean we would need to compare many different possible structures at every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structs = [Cholesky, HOSVD, Tucker, TT, CP]\n",
    "\n",
    "def isclose(x, y):\n",
    "    return True\n",
    "\n",
    "def optimise(loss, params):\n",
    "    while not converged:\n",
    "        dl = dloss(x, params)\n",
    "        \n",
    "        for struct in structs:\n",
    "            S = struct(params)  \n",
    "            # is there a faster way to approximately check their structure?\n",
    "            if isclose(params, S):\n",
    "                # reinitialise parameters in the decomposed structure\n",
    "                params = S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. So this transition is non differentiable. E.g. Fully connected to rank decomposition. \n",
    "\n",
    "\n",
    "Also, what if it was a spurious closeness? Or is close to two different structures? Maybe should be picked by voting?\n",
    "\n",
    "Would like a knob for how much accuracy we are ok with sacrificing. Maybe we want each decomposition to not lose much info from the original structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should test this out in a simple case?\n",
    "# how about 1 layer mnist?\n",
    "x = tf.placeholder()\n",
    "fc = Structs.FC(shape, dtype)\n",
    "y = tf.matmul(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Is there some more general/differentiable way of describing the structure of a matrix?__\n",
    "\n",
    "\n",
    "\n",
    "__Do we need to check against every structure everytime?__\n",
    "\n",
    "* Do they tell us about each other?\n",
    "* Are there other metrics we could use to decide which structure is present? Locality, rank, ?, ... When I look at a heatmap of a covar matrix I can guess what sorts of structure there are based upon...?!?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
