{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and intro\n",
    "\n",
    "> _In particular, we propose a method, called differential training, that can be used to obtain an approximation to cost-to-go differences rather than cost-to-go values ..._ [Bertsekas](http://www.mit.edu/~dimitrib/Diftrain.pdf)\n",
    "\n",
    "What is the motivation?\n",
    "Why do residuals work well in ResNets? And how does that apply here? ResNets make it easy to learn identity mappings. We want to make it easy to learn \\_\\_(?)\\_\\_ policies.\n",
    "\n",
    "> _We focus on the computation of a rollout policy, which is obtained by a single policy iteration starting from some known base policy and using some form of exact or approximate policy improvement._\n",
    "\n",
    "We can bias our policies to something more simple, like not taking actions. Like a prior on expected policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals\n",
    "\n",
    "Learning the difference between things may be easier for NNs? As we have some anchor to connect ideas to? Rather than having absolute value/distinct memory.\n",
    "\n",
    "This allows it to learn a more general class of features (?) as it is learning the relation between actions w.r.t to a base action. This means that as long as other settings have similar structure (ahh, also their relation to loss) between actions then our train algorithm should be able to __transfer__ its knowledge (???).\n",
    "\n",
    "\n",
    "\n",
    "### Residual Q-learning\n",
    "\n",
    "So instead of learning a function, $Q:S\\times A\\to {\\mathbb  {R}}$, that maps states and actions to a expected (absolute) value. Where we then pick the action with the highest value. (but what about the policy...) \n",
    "\n",
    "We want, $Q:S\\times A\\to {\\mathbb  {R}}$, that gives us the relative value compared to some default policy.\n",
    "\n",
    "What are good canidates for the default policy?\n",
    "* Same action the whole time.\n",
    "* Just initialise a random one.\n",
    "* \n",
    "\n",
    "$Q(s,a) = $\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
