{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs and PCA  (Baldi et al. 1989/2012)\n",
    "\n",
    "### Summary\n",
    "Goals: \n",
    "1. Show that each critical point of $E(A,B)$ is some combination of principle components of $\\Sigma$, which are the eigen vectors of the covariance matrix of our data. (is that true?)\n",
    "* Show that the minium loss is achieved when the largest eigenvalues/leading eigenvectors are chosen/learnt.\n",
    "\n",
    "***\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(A,B) = \\sum_t \\parallel y^t - ABx^t \\parallel \\tag{a} \\\\\n",
    "A^T(AB - \\Sigma_{YX}\\Sigma^{-1}_{XX}) = 0 \\\\\n",
    "S = AB - P_A \\Sigma_{YX}\\Sigma^{-1}_{XX} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "To show (1) they set A and solve for B given $E(A,B)$ is at a critical point (aka that the gradient of loss is zero) -- and vice versa. Using this result we find that AB is equal to the projection of the covariance YX and the inverse of covariance XX into a subspace of A (using a clever argument to show that $S = 0$, as S is both in the subspace of A and orthogonal to A). It follows that $\\Sigma$ is invariant to the columns of A, which is (roughly) the definition of an eigen vector. \n",
    "\n",
    "To show (2) we use a unitary change of coordinates, where the covariance XX is diagonal, to allow some nice rearranging. Because AB is a projection matrix we know that we can drop the aquared term, thus we can cancel we can simplity to the trace of $tr(\\Sigma) - tr(AB\\Sigma)$. Then using pertubation analysis of ...\n",
    "\n",
    "### Questions and notes\n",
    "\n",
    "* We did second order optimisation without computing the hessian.\n",
    "* What was the most important part of this paper? How significant is that that we could describe the loss surface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep linear nets (Kawaguchi)\n",
    "\n",
    "Using the necessary conditions of a positive semi definite matrix we can do a case analysis of potential network architectures to show that every local minima is the global minima. \n",
    "\n",
    "The main goal is to show that ... $C^T(CC^T)^-C$\n",
    "\n",
    "\n",
    "Schur compliment necessary conditions of $M \\ge 0$ where $M = \\begin{bmatrix} A & B \\\\ B^T & C \\end{bmatrix}$;\n",
    "* $A\\ge 0$     (first section of lemma 4.6)\n",
    "* $M/A \\ge 0$     (second section of lemma 4.6)\n",
    "* $R(B) \\subseteq R(A)$     (lemma 4.4)\n",
    "\n",
    "####  Questions and notes\n",
    "\n",
    "* What is all this fuss about schur compliments and generalised inverses? Why bother? What is hard about this problem? Why did Baldi ... do ??? \n",
    "* Alternate proof(s)? (that all mimima are global minima) Could show that for all minima, there is a symmetry and we can do a cts deformation between them. Therefore they are the same minima and is global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solutions (Saxe et al.)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Goals:\n",
    "1. Describe the dynamics/trajectories of parameter updates\n",
    "* Investigate orthogonal weight initialisations\n",
    "* Describe pathology in information loss\n",
    "\n",
    "Results:\n",
    "1. \n",
    "\n",
    "\n",
    "> _Our fundamental goal is to understand the dynamics of learning as a function of the input statistics $\\Sigma_{XX}$ and input-output statistics $\\Sigma_{YX}$._\n",
    "\n",
    "Hmm. So this is the goal of all learning algorithms?\n",
    "\n",
    "> _To begin, though, we further simplify the analysis by focusing on the case of orthogonal input representations where $\\Sigma_{XX} = I$. This assumption will hold exactly for whitened input data, a widely used preprocessing step._\n",
    "\n",
    "\n",
    "$\\Sigma_{YX} = U_{mxm}S_{mxn}V_{nxn}^T =\\sum^{n}_{i=1} s_iu_iv_i^T$\n",
    "\n",
    "But the indicies dont work out?? as there are m lots of $u_i$ vectors in U, but we are only looking at n of them. So either $n>m$ and $u_n$ doesnt exist, or $n<m$ and we have not used all of them. And u_i.v_i doesnt work?!? Oh... we are talking about the covariance matrix, so m = n... Wait are we?\n",
    "\n",
    "\n",
    "* $u_i$ reflect the independent modes of variation in the output,\n",
    "    * so U and $\\Sigma_{YX}$ are also decorrelated??\n",
    "* $v_i$ reflect the independent modes of variation in the input,\n",
    "* $s_i$ are the singular values -- which mean __?\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "* How can we transform to cts time?\n",
    "* How much work (path integral of trajectory in the vector field) is done for different initialisations?\n",
    "* $C = a^2 - b^2 \\implies a = \\pm b$. What does this mean? Solutions are symmetric?!? It's a circle?!?\n",
    "* $a^{\\alpha},b^{\\alpha}$. These represent the eigen vectors from the input layer and the output modes into the final layer. What happened to the middle layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training deep nets (Glorot et al.)\n",
    "\n",
    "Summary:\n",
    "\n",
    "If we initialise the columns of our weights matrices ($W^j$) independently, then we get $Var[z^i] = Var[x] \\prod_{j=0}^{i} n_j Var[W^j]$. As ???. Which is the forward propagation of variance. We can do the same to find the backward propagation of variance giving ... . Given these relations we would like the variance, of inputs propagated forward and gradients propagated backward, to remain constant (so we dont lose information).\n",
    "\n",
    "#### Questions\n",
    "\n",
    "* How does variance actually help? It doesnt actually constrain the values. We can still get var = 1 with vanishing/exploding gradients.\n",
    "* Why is saturating bad?!? No reason for it to be bad late in learning?\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
