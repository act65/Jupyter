{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd import grad\n",
    "from autograd.util import quick_grad_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    print(\"Loading training data...\")\n",
    "    import imp\n",
    "    partial_flatten = lambda x : np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n",
    "    one_hot = lambda x, K: np.array(x[:,None] == np.arange(K)[None, :], dtype=int)\n",
    "    source = '/var/folders/c8/nyrw36d161b9z7tngmrc3qym0000gn/T/tmpGHWkKL.py'\n",
    "    data = imp.load_source('data', source).mnist()\n",
    "    train_images, train_labels, test_images, test_labels = data\n",
    "    train_images = partial_flatten(train_images) / 255.0\n",
    "    test_images  = partial_flatten(test_images)  / 255.0\n",
    "    train_labels = one_hot(train_labels, 10)\n",
    "    test_labels = one_hot(test_labels, 10)\n",
    "    N_data = train_images.shape[0]\n",
    "    return N_data, train_images, train_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "(60000, 784)\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    N_data, train_images, train_labels, test_images, test_labels=load_mnist()\n",
    "    print(train_images.shape)\n",
    "    print(N_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_nn_funs(layer_sizes, L2_reg):\n",
    "    shapes = zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "    N = sum((m+1)*n for m, n in shapes)\n",
    "\n",
    "    def unpack_layers(W_vect):\n",
    "        for m, n in shapes:\n",
    "            yield W_vect[:m*n].reshape((m,n)), W_vect[m*n:m*n+n]\n",
    "            W_vect = W_vect[(m+1)*n:]\n",
    "\n",
    "    def predictions(W_vect, inputs):\n",
    "        for W, b in unpack_layers(W_vect):\n",
    "            outputs = np.dot(inputs, W) + b\n",
    "            inputs = np.tanh(outputs)\n",
    "        return outputs - logsumexp(outputs, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(W_vect, X, T):\n",
    "        log_prior = -L2_reg * np.dot(W_vect, W_vect)\n",
    "        log_lik = np.sum(predictions(W_vect, X) * T)\n",
    "        return - log_prior - log_lik\n",
    "\n",
    "    def frac_err(W_vect, X, T):\n",
    "        return np.mean(np.argmax(T, axis=1) != np.argmax(predictions(W_vect, X), axis=1))\n",
    "\n",
    "    return N, predictions, loss, frac_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178110\n",
      "<function loss at 0x104043230>\n",
      "<function frac_err at 0x104043398>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    layer_sizes = [784, 200, 100, 10]\n",
    "    L2_reg = 1.0\n",
    "    N, predictions, loss, frac_err = make_nn_funs(layer_sizes, L2_reg)\n",
    "    print(N)\n",
    "    print(loss)\n",
    "    print(frac_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_batches(N_data, batch_size):\n",
    "    return [slice(i, min(i+batch_size, N_data))\n",
    "            for i in range(0, N_data, batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "layer_sizes = [784, 200, 100, 10]\n",
    "L2_reg = 1.0\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.1\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "\n",
    "# Make neural net functions\n",
    "N_weights, pred_fun, loss_fun, frac_err = make_nn_funs(layer_sizes, L2_reg)\n",
    "loss_grad = grad(loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178110\n"
     ]
    }
   ],
   "source": [
    "print(N_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient of <function loss at 0x106668a28> at [ 0.0013496  -0.10268497  0.07023831 ..., -0.10388204  0.06244274\n",
      " -0.02446704]\n",
      "Gradient projection OK (numeric grad: -432.491936954, analytic grad: -432.491936705)\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "rs = npr.RandomState()\n",
    "W = rs.randn(N_weights) * param_scale\n",
    "\n",
    "# Check the gradients numerically, just to be safe\n",
    "quick_grad_check(loss_fun, W, (train_images, train_labels))\n",
    "\n",
    "\n",
    "\n",
    "def print_perf(epoch, W):\n",
    "    test_perf  = frac_err(W, test_images, test_labels)\n",
    "    train_perf = frac_err(W, train_images, train_labels)\n",
    "    print(\"{0:15}|{1:15}|{2:15}\".format(epoch, train_perf, test_perf))\n",
    "\n",
    "# Train with sgd\n",
    "batch_idxs = make_batches(train_images.shape[0], batch_size)\n",
    "cur_dir = np.zeros(N_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Checking gradient of <function loss at 0x106636f50> at [ 0.05768681 -0.03568799 -0.13128437 ..., -0.08752415 -0.06350238\n",
      "  0.07270739]\n",
      "Gradient projection OK (numeric grad: -199.372684874, analytic grad: -199.372684035)\n",
      "    Epoch      |    Train err  |   Test err  \n",
      "              0| 0.893533333333|         0.9018\n",
      "              1|0.0854833333333|         0.0796\n",
      "              2|0.0790833333333|         0.0747\n",
      "              3|        0.07545|         0.0726\n",
      "              4|0.0729833333333|         0.0708\n",
      "              5|        0.07075|         0.0687\n",
      "              6|0.0691833333333|         0.0671\n",
      "              7|0.0678666666667|         0.0666\n",
      "              8|0.0666666666667|          0.065\n",
      "              9|        0.06555|         0.0642\n",
      "             10|0.0648333333333|         0.0634\n",
      "             11|0.0641333333333|         0.0629\n",
      "             12|0.0635333333333|         0.0625\n",
      "             13|0.0630166666667|         0.0622\n",
      "             14|         0.0629|         0.0618\n",
      "             15|0.0624833333333|         0.0611\n",
      "             16|0.0621333333333|         0.0607\n",
      "             17|0.0616833333333|         0.0602\n",
      "             18|0.0611333333333|           0.06\n",
      "             19|0.0609166666667|           0.06\n",
      "             20|0.0607333333333|         0.0597\n",
      "             21|         0.0606|         0.0599\n",
      "             22|0.0604166666667|         0.0595\n",
      "             23|0.0602166666667|          0.059\n",
      "             24|0.0601333333333|         0.0586\n",
      "             25|0.0599166666667|         0.0583\n",
      "             26|         0.0598|         0.0581\n",
      "             27|0.0597666666667|         0.0579\n",
      "             28|0.0596333333333|         0.0578\n",
      "             29|         0.0596|         0.0577\n",
      "             30|0.0595166666667|         0.0575\n",
      "             31|         0.0595|         0.0573\n",
      "             32|        0.05945|         0.0572\n",
      "             33|0.0593833333333|         0.0573\n",
      "             34|0.0592666666667|         0.0572\n",
      "             35|0.0591833333333|         0.0573\n",
      "             36|        0.05915|         0.0573\n",
      "             37|        0.05915|         0.0574\n",
      "             38|0.0590666666667|         0.0571\n",
      "             39|0.0589833333333|         0.0569\n",
      "             40|0.0589333333333|          0.057\n",
      "             41|0.0588666666667|         0.0568\n",
      "             42|0.0588333333333|         0.0568\n",
      "             43|         0.0588|         0.0568\n",
      "             44|0.0587833333333|         0.0567\n",
      "             45|0.0588666666667|         0.0567\n",
      "             46|        0.05885|         0.0567\n",
      "             47|0.0588333333333|         0.0566\n",
      "             48|         0.0588|         0.0567\n",
      "             49|0.0587333333333|         0.0566\n"
     ]
    }
   ],
   "source": [
    "print(\"    Epoch      |    Train err  |   Test err  \")\n",
    "for epoch in range(num_epochs):\n",
    "    print_perf(epoch, W)\n",
    "    for idxs in batch_idxs:\n",
    "        grad_W = loss_grad(W, train_images[idxs], train_labels[idxs])\n",
    "        cur_dir = momentum * cur_dir + (1.0 - momentum) * grad_W\n",
    "        W -= learning_rate * cur_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
