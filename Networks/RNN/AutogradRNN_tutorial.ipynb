{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f5a91bc2df4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbuiltins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlstm_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWeightsParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import value_and_grad\n",
    "from autograd.util import quick_grad_check\n",
    "from scipy.optimize import minimize\n",
    "from os.path import dirname, join\n",
    "from builtins import range\n",
    "\n",
    "lstm_filename = join(dirname(__file__), 'lstm.py')\n",
    "\n",
    "class WeightsParser(object):\n",
    "    \"\"\"A helper class to index into a parameter vector.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.idxs_and_shapes = {}\n",
    "        self.num_weights = 0\n",
    "\n",
    "    def add_shape(self, name, shape):\n",
    "        start = self.num_weights\n",
    "        self.num_weights += np.prod(shape)\n",
    "        self.idxs_and_shapes[name] = (slice(start, self.num_weights), shape)\n",
    "\n",
    "    def get(self, vect, name):\n",
    "        idxs, shape = self.idxs_and_shapes[name]\n",
    "        return np.reshape(vect[idxs], shape)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1.0)   # Output ranges from 0 to 1.\n",
    "\n",
    "def activations(weights, *args):\n",
    "    cat_state = np.concatenate(args + (np.ones((args[0].shape[0],1)),), axis=1)\n",
    "    return np.dot(cat_state, weights)\n",
    "\n",
    "def logsumexp(X, axis=1):\n",
    "    max_X = np.max(X)\n",
    "    return max_X + np.log(np.sum(np.exp(X - max_X), axis=axis, keepdims=True))\n",
    "\n",
    "def build_rnn(input_size, state_size, output_size):\n",
    "    \"\"\"Builds functions to compute the output of an RNN.\"\"\"\n",
    "    parser = WeightsParser()\n",
    "    parser.add_shape('init_hiddens', (1, state_size))\n",
    "    parser.add_shape('change',  (input_size + state_size + 1, state_size))\n",
    "    parser.add_shape('predict', (state_size + 1, output_size))\n",
    "\n",
    "    def update(input, hiddens, change_weights):\n",
    "        return np.tanh(activations(change_weights, input, hiddens))\n",
    "\n",
    "    def hiddens_to_output_probs(predict_weights, hiddens):\n",
    "        output = activations(predict_weights, hiddens)\n",
    "        return output - logsumexp(output)     # Normalize log-probs.\n",
    "\n",
    "    def outputs(weights, inputs):\n",
    "        \"\"\"Goes from right to left, updating the state.\"\"\"\n",
    "        num_sequences = inputs.shape[1]\n",
    "        hiddens = np.repeat(parser.get(weights, 'init_hiddens'), num_sequences, axis=0)\n",
    "        change_weights    = parser.get(weights, 'change')\n",
    "        predict_weights   = parser.get(weights, 'predict')\n",
    "\n",
    "        output = [hiddens_to_output_probs(predict_weights, hiddens)]\n",
    "        for input in inputs:  # Iterate over time steps.\n",
    "            hiddens = update(input, hiddens, change_weights)\n",
    "            output.append(hiddens_to_output_probs(predict_weights, hiddens))\n",
    "        return output\n",
    "\n",
    "    def log_likelihood(weights, inputs, targets):\n",
    "        logprobs = outputs(weights, inputs)\n",
    "        loglik = 0.0\n",
    "        num_time_steps, num_examples, _ = inputs.shape\n",
    "        for t in range(num_time_steps):\n",
    "            loglik += np.sum(logprobs[t] * targets[t])\n",
    "        return loglik / (num_time_steps * num_examples)\n",
    "\n",
    "    return outputs, log_likelihood, parser.num_weights\n",
    "\n",
    "def string_to_one_hot(string, maxchar):\n",
    "    \"\"\"Converts an ASCII string to a one-of-k encoding.\"\"\"\n",
    "    ascii = np.array([ord(c) for c in string]).T\n",
    "    return np.array(ascii[:,None] == np.arange(maxchar)[None, :], dtype=int)\n",
    "\n",
    "def one_hot_to_string(one_hot_matrix):\n",
    "    return \"\".join([chr(np.argmax(c)) for c in one_hot_matrix])\n",
    "\n",
    "def build_dataset(filename, sequence_length, alphabet_size, max_lines=-1):\n",
    "    \"\"\"Loads a text file, and turns each line into an encoded sequence.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    content = content[:max_lines]\n",
    "    content = [line for line in content if len(line) > 2]   # Remove blank lines\n",
    "    seqs = np.zeros((sequence_length, len(content), alphabet_size))\n",
    "    for ix, line in enumerate(content):\n",
    "        padded_line = (line + \" \" * sequence_length)[:sequence_length]\n",
    "        seqs[:, ix, :] = string_to_one_hot(padded_line, alphabet_size)\n",
    "    return seqs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    npr.seed(1)\n",
    "    input_size = output_size = 128   # The first 128 ASCII characters are the common ones.\n",
    "    state_size = 40\n",
    "    seq_length = 30\n",
    "    param_scale = 0.01\n",
    "    train_iters = 100\n",
    "\n",
    "    # Learn to predict our own source code.\n",
    "    train_inputs = build_dataset(lstm_filename, seq_length, input_size, max_lines=60)\n",
    "\n",
    "    pred_fun, loglike_fun, num_weights = build_rnn(input_size, state_size, output_size)\n",
    "\n",
    "    def print_training_prediction(weights):\n",
    "        print(\"Training text                         Predicted text\")\n",
    "        logprobs = np.asarray(pred_fun(weights, train_inputs))\n",
    "        for t in range(logprobs.shape[1]):\n",
    "            training_text  = one_hot_to_string(train_inputs[:,t,:])\n",
    "            predicted_text = one_hot_to_string(logprobs[:,t,:])\n",
    "            print(training_text.replace('\\n', ' ') + \"|\" + predicted_text.replace('\\n', ' '))\n",
    "\n",
    "    # Wrap function to only have one argument, for scipy.minimize.\n",
    "    def training_loss(weights):\n",
    "        return -loglike_fun(weights, train_inputs, train_inputs)\n",
    "\n",
    "    def callback(weights):\n",
    "        print(\"Train loss:\", training_loss(weights))\n",
    "        print_training_prediction(weights)\n",
    "\n",
    "   # Build gradient of loss function using autograd.\n",
    "    training_loss_and_grad = value_and_grad(training_loss)\n",
    "\n",
    "    init_weights = npr.randn(num_weights) * param_scale\n",
    "    # Check the gradients numerically, just to be safe\n",
    "    quick_grad_check(training_loss, init_weights)\n",
    "\n",
    "    print(\"Training LSTM...\")\n",
    "    result = minimize(training_loss_and_grad, init_weights, jac=True, method='CG',\n",
    "                      options={'maxiter':train_iters}, callback=callback)\n",
    "    trained_weights = result.x\n",
    "\n",
    "    print()\n",
    "    print(\"Generating text from RNN...\")\n",
    "    num_letters = 30\n",
    "    for t in range(20):\n",
    "        text = \"\"\n",
    "        for i in range(num_letters):\n",
    "            seqs = string_to_one_hot(text, output_size)[:, np.newaxis, :]\n",
    "            logprobs = pred_fun(trained_weights, seqs)[-1].ravel()\n",
    "            text += chr(npr.choice(len(logprobs), p=np.exp(logprobs)))\n",
    "        print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
