{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives and variance\n",
    "\n",
    "$$\n",
    "cov(y, x) = \n",
    "\\begin{bmatrix}\n",
    "var(y_1, x_1) & \\dots & var(y_1, x_m)\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "var(y_n, x_1) & \\dots & var(y_n, x_m)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_m}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\dots & \\frac{\\partial y_n}{\\partial x_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Best linear approximation is $y = \\mathcal J x$?\n",
    "\n",
    "\n",
    "#### Relation to derivatives\n",
    "\n",
    "Baldi et al. 2012. What do the weight matrices learn? In the case of $y = ABx, B = (A^*A)^{-1}A^* \\Sigma_{yx}\\Sigma_{xx}^{-1}$. Where $A(A^*A)^{-1}A^*$ is just some low rank projection, and in the case where rank of A and B is of the same rank as the data, then we get $A(A^*A)^{-1}A^* = I$, therefore $y = ABx =  \\Sigma_{yx}\\Sigma_{xx}^{-1}x$\n",
    "\n",
    "Therefore, $\\mathcal J = \\Sigma_{yx}\\Sigma_{xx}^{-1}$ ... in which cases?! And this makes perfect sense as $\\Sigma_{yx}\\Sigma_{xx}^{-1} = \\frac{rise}{run}$. An estimate for the rise over the run. Aka the gradient.\n",
    "\n",
    "So this means that we are learning a factorisation of the jacobian?!?\n",
    "\n",
    "\n",
    "#### Hessians\n",
    "\n",
    "$$\n",
    "f \\circ g = h \\\\\n",
    "\\mathbb R^n \\mathop{\\rightarrow}^f \\mathbb R^m \\mathop{\\rightarrow}^g \\mathbb R^p \\\\\n",
    "\\mathbf H_h = (I_p \\otimes \\nabla f^T)\\cdot \\mathbf H_g \\cdot \\nabla f +  (\\nabla_g \\otimes I_n) \\cdot \\mathbf H_f \\\\\n",
    "$$\n",
    "\n",
    "derivatives are linearisations. are linear operators and can be represented in matrix form.\n",
    "\n",
    "approximating the hessian with lower order samples, and using their variance. that denoising - bengio paper?\n",
    "\n",
    "\n",
    "positive semidefinite structure on hessians can be important. what other structures are menaingful? so could we do a non-negative matrix factorisation?! or ..?\n",
    "\n",
    "#### nth order moments and derivatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rates of change, compression and contraction\n",
    "\n",
    "Is contraction equivalent to compression? \n",
    "\n",
    "<u>Definition</u>. A linear operator is contractive if $ \\forall i: \\parallel J_{ii} \\parallel < 1$ Not true. it's the eigen vectors?\n",
    "\n",
    "I.e the rate of change of $x$ w.r.t $y$ is decreasing. So if we had a neural networks, where every layer was contractive, then every layer would be reducing the effect $x$ has on $y$. (symmetries, invariance, want to direct this contraction along the right geometry, compress info...)\n",
    "\n",
    "\n",
    "What about;\n",
    "\n",
    "* entropy?\n",
    "* score? $\\nabla \\mathbb E [f(x)] = \\mathbb E[f(x) \\nabla log p(x)]$\n",
    "* also relation to denoising?! (what about local noise? only in a subspace, or locally correlated?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise, infomation and gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
