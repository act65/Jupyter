{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont know what i think so far. I am adding more parameters, but now the can learn different non-linearities. Adding more parameters will lead to over fitting? But i can just reduce the nets size?\n",
    "\n",
    "I could tie weights across\n",
    "* neurons?\n",
    "* read/write????\n",
    "\n",
    "Based on \n",
    "* Balduzzi - Strongly typed RNNs\n",
    "* and Neural Turing Machines.\n",
    "\n",
    "\n",
    "Permutations\n",
    "* Additive, multiplication\n",
    "* Walks through weight space, aka the environment\n",
    "\n",
    "Analogy\n",
    "* Memory = environment\n",
    "* Weights = position in environment\n",
    "* Neurons = ants, particles, ??\n",
    "* \n",
    "\n",
    "Other questions/problems\n",
    "* Make the number of time steps a variable and add it as a regulariser in loss?\n",
    "* Unstable... how can you make more stable systems? Negative feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import MNIST\n",
    "mnist = MNIST(('train',))\n",
    "state = mnist.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "n_inputs = 28\n",
    "n_hidden = 0\n",
    "n_outputs = 10\n",
    "m = n_inputs + n_hidden + n_outputs #number of memory cells\n",
    "\n",
    "n = 10#number of neurons\n",
    "\n",
    "time_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neurons that dont have a state\n",
    "class Neurons():\n",
    "    def __init__(self,n):\n",
    "        self.alphas = tf.Variable(tf.random_normal((n,1),dtype=np.float32),'alpha')\n",
    "        self.betas = tf.Variable(np.ones((n,1),dtype=np.float32),'beta')\n",
    "        \n",
    "    def forward(self,phi):\n",
    "        #normalise to stop exploding/diminishing state values\n",
    "        m = tf.reduce_mean(phi)\n",
    "        v = tf.reduce_mean(tf.square(phi-m))\n",
    "        return  tf.nn.elu(self.alphas + (phi-m)*self.betas/v) #tf.exp(phi*self.betas) *self.alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape = [n_inputs,n_inputs])\n",
    "L = tf.placeholder(tf.int64, shape = [1,1])\n",
    "y_ = tf.reshape(tf.one_hot(L,10,1.0,0.0),[n_outputs,1])\n",
    "\n",
    "#The environment where signals interact.\n",
    "memory = tf.Variable(np.ones((m,1),dtype=np.float32),'Memory') \n",
    "\n",
    "#The weights. Do we want these to be symmetric? Do we want to learn these?\n",
    "read = tf.Variable(tf.random_normal((n,m)),'ReadConnectionWeights')\n",
    "write = tf.Variable(tf.random_normal((m,n)),'WriteConnectionWeights')\n",
    "biases = tf.Variable(-np.ones((n,1),dtype=np.float32),'Biases')\n",
    "\n",
    "neurons = Neurons(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cell(inputs, memory):\n",
    "    #with tf.VariableScope('Basic'):\n",
    "    # Inputs\n",
    "    i = tf.slice(memory, [0,0],[n_inputs,1])\n",
    "    j = tf.slice(memory, [n_inputs,0], [m-n_inputs,1])\n",
    "    i = i + inputs\n",
    "    mem = tf.concat(0,[i,j])\n",
    "\n",
    "    ### Dynamics\n",
    "    #the inputs to our neurons. \n",
    "    phi = tf.matmul( read , mem ) + biases #nxm x mx1 = nx1\n",
    "    #the inputs to our memory cells. \n",
    "    psi = tf.matmul( write , neurons.forward(phi) ) #mxn x nx1 = mx1\n",
    "\n",
    "    return psi*memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since I have rolled this operation up with in the tf graph, it is hard to query it for values\n",
    "with tf.variable_scope(\"RNN\"):\n",
    "    for t in range(time_steps):\n",
    "        if time_steps > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        memory = cell(tf.slice(x, [0,t],[-1,1]), memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Classification\n",
    "s = tf.slice(memory, [m-n_outputs,0],[n_outputs,1]) #take the outputs\n",
    "W_soft = tf.Variable(tf.random_normal((n_outputs,n_outputs)))\n",
    "b_soft = tf.Variable(np.zeros((n_outputs),dtype = np.float32))\n",
    "\n",
    "soft = tf.reshape(tf.matmul(W_soft,s),[1,n_outputs]) + b_soft\n",
    "m = tf.reduce_mean(soft)\n",
    "v = tf.reduce_mean(tf.square(soft-m))\n",
    "softin = tf.abs((soft - m)/v)\n",
    "y = tf.nn.softmax(softin)\n",
    "#hmm. have to do some nasty hacks to get some decent output values. positive, and small (ish)\n",
    "#how do conv nets manage this??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Loss and optimisation\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[0]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,0), tf.argmax(y_,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 1)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "ans = sess.run([memory,y,soft,softin],feed_dict={x : np.ones((n_inputs,n_inputs),dtype = np.float32),L : np.ones((1,1),dtype = np.int64 )})\n",
    "for a in ans:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.05607948e+01]\n",
      " [  1.45409012e-03]\n",
      " [ -4.39483490e+02]\n",
      " [  1.54031052e+02]\n",
      " [ -6.07794714e+00]\n",
      " [  6.68625608e-02]\n",
      " [  1.51745346e+02]\n",
      " [  8.96846313e+01]\n",
      " [  5.39112427e+02]\n",
      " [ -2.07207661e+01]\n",
      " [  1.22115060e-04]\n",
      " [ -1.33262050e+00]\n",
      " [  8.22290726e+01]\n",
      " [  6.56104851e+00]\n",
      " [ -4.54012756e+01]\n",
      " [ -8.60629517e+02]\n",
      " [  7.24232788e+01]\n",
      " [  1.26268864e+01]\n",
      " [ -3.63664702e-02]\n",
      " [  2.59771149e+02]\n",
      " [  2.33434830e+02]\n",
      " [ -1.52324271e+00]\n",
      " [ -2.18608008e+03]\n",
      " [  8.38435974e+01]\n",
      " [  3.88211831e-02]\n",
      " [  2.36482956e+02]\n",
      " [ -3.42604704e-02]\n",
      " [ -5.08655071e-01]\n",
      " [ -1.04268627e+01]\n",
      " [  8.91410828e+00]\n",
      " [  1.45696020e+01]\n",
      " [ -2.12244614e+02]\n",
      " [  3.69294930e+00]\n",
      " [  6.27843689e+02]\n",
      " [  2.41797066e+01]\n",
      " [  3.51288989e+03]\n",
      " [  3.05528736e+01]\n",
      " [  3.71107758e+02]]\n",
      "[[ 0.10000157  0.0999779   0.10000884  0.10000444  0.09998711  0.09998541\n",
      "   0.10002433  0.10001744  0.10001078  0.09998216]]\n",
      "[[-1605.20715332   874.67248535  5127.13867188  4665.79492188\n",
      "   2850.62866211    87.9720459  -3987.88769531  6027.55126953\n",
      "  -2569.25537109  2331.75805664]]\n",
      "[[  2.85006536e-04   4.82702089e-05   3.57682235e-04   3.13641067e-04\n",
      "    1.40360149e-04   1.23370861e-04   5.12463914e-04   4.43638157e-04\n",
      "    3.77037271e-04   9.08272923e-05]]\n"
     ]
    }
   ],
   "source": [
    "for a in ans:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "10 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "20 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "30 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "40 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "50 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "60 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "70 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "80 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "90 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "100 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "110 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "120 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "130 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "140 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "150 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "160 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "170 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "180 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "190 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "200 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "210 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "220 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "230 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "240 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "250 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "260 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "270 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "280 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "290 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "300 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "310 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "320 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "330 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "340 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "350 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "360 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "370 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "380 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "390 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "400 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "410 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "420 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "430 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "440 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "450 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "460 : Accuracy =  1.0 : Cross entropy =  2.30259\n",
      "470 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "480 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "490 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "500 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "510 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "520 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "530 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "540 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "550 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "560 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "570 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "580 : Accuracy =  0.0 : Cross entropy =  2.30259\n",
      "590 : Accuracy =  0.0 : Cross entropy =  2.30259\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "from fuel.schemes import ShuffledScheme\n",
    "scheme = ShuffledScheme(examples=int(mnist.num_examples/100), batch_size=batch_size)\n",
    "for i,request in enumerate(scheme.get_request_iterator()):\n",
    "    data,labels = mnist.get_data(state=state, request=request)\n",
    "    feed = feed_dict= {x:data.reshape(28,28),L:labels}\n",
    "    sess.run(train_step, feed)\n",
    "    if i%10 == 0:\n",
    "        acc,cross=sess.run([accuracy,cross_entropy], feed)\n",
    "        print(i,': Accuracy = ',acc,': Cross entropy = ',cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
