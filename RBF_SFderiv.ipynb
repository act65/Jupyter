{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: some writing\n",
    "\n",
    "## Neural nets (1)\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= \\sigma(W\\cdot x + b) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```python\n",
    "x,T = get_data()\n",
    "L = Loss(y,T)\n",
    "while training:\n",
    "    y = NN(x)\n",
    "    #use gradients to descend loss surface\n",
    "    NN.weights -= learning_rate * dLdW(y,T)\n",
    "    NN.biases -= learning_rate * dLdb(y,T)\n",
    "    \n",
    "```\n",
    "## Radial basis functions (2)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= W\\cdot\\phi( \\parallel x - c\\parallel)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "x,T = get_data()\n",
    "#pick centers via clustering\n",
    "RBF.centers = kmeans(x)\n",
    "while training:\n",
    "    y = rbf(x)\n",
    "    #fit weights with linear least squares\n",
    "    RBF.weights -= Learning_rate * RBF.weights * (y-T)\n",
    "```\n",
    "\n",
    "\n",
    "### Similarities\n",
    "\n",
    "Clearly some of the elements of each equation has simply been renamed and/or moved. \n",
    "\n",
    "* The biases in (1) do the same job as the centers in (2).\n",
    "* The activation finction in (1) is the same as the radial radial basis function in (2). (in the sense that they are non-linearities).\n",
    "* The dot products in both (1) and (2) aggregate a row of weights with ____.\n",
    "* The weights themselves act as a transform on the data, rotating, shifting, inverting, ...\n",
    "\n",
    "### Disimilarities\n",
    "\n",
    "* We have added a norm to (2). \n",
    "* The weights are applied outside the non-linearity.\n",
    "* The activation functions used in (1) and the radial basis functions used in (2) are generally quite different.\n",
    "* When training, the centers of (2) are generally selected using some clustering algorithm ([3 learning phases for rbfs](http://www.sciencedirect.com/science/article/pii/S0893608001000272))\n",
    "\n",
    "## Discussion\n",
    "\n",
    "This makes all the information pass through a single scalar value, before being mapped back out into some vector.\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textbf{NN} \\\\\n",
    "\\sigma (z) &= \\frac{1}{1+e^{-z}}  \\tag{sigmoid} \\\\\n",
    "\\rho (z) &= max(z,0)   \\tag{relu}\\\\\n",
    "&\\textbf{RBF} \\\\\n",
    "\\phi (r)&=e^{-(\\varepsilon r)^{2}}\\\\\n",
    "\\phi (r)&={\\sqrt {1+(\\varepsilon r)^{2}}} \\tag{Multiquadric}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Squared term to remove negative values, \n",
    "\n",
    "### \n",
    "\n",
    "The job of the weights it to (generally) map into some space where the ..., like the principle components if the covariance. This allows use to \n",
    "\n",
    "### Directional invariance\n",
    "\n",
    "What does the radial distance invariance buy us?\n",
    "Effectively, we are throwing out the information about the direction of our inputs.\n",
    "In what instances would this be advisable?\n",
    "\n",
    "### Training\n",
    "\n",
    "RBF training costs more up front, but linear least squares is cheaper than backprop while training. Given that ... better init.\n",
    "\n",
    "http://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: some math\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textbf{Definitions} \\\\\n",
    "E(T,Y) &= \\sum_t \\textbf t^t \\cdot log(\\textbf y^t) \\\\\n",
    "Y &= \\frac{e^{z}}{\\sum_j e^{z_j}} \\\\\n",
    "z &= Wx \\\\\n",
    "&\\textbf{Partial derivatives} \\\\\n",
    "\\frac{\\partial E}{\\partial Y} &= \\sum_t T^t \\cdot \\frac{1}{Y^t} \\\\\n",
    "\\frac{\\partial Y^t}{\\partial z^t} &= Y^t(\\delta_{ij}-Y^t) \\tag{see derivation below}\\\\\n",
    "\\frac{\\partial z^t}{\\partial W^t} &= x^t \\\\\n",
    "&\\textbf{Chain rule} \\\\\n",
    "\\frac{\\partial E}{\\partial Y} \\frac{\\partial Y^t}{\\partial z^t} \\frac{\\partial z^t}{\\partial W^t}  &= \\sum_t T^t \\frac{1}{Y^t} \\quad Y^t(\\delta_{ij}-Y^t) \\quad  x^t \\\\\n",
    "\\frac{\\partial E}{\\partial W} &= \\sum_t T^t (\\delta_{ij}-Y^t) x^t \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\textbf{Softmax derivation} \\\\\n",
    " \\\\\n",
    "&\\textbf{Case: $i = k$} \\\\\n",
    "\\frac{\\partial y_k}{\\partial z_i} &=\\frac{\\partial}{\\partial z_i} \\Big( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\Big) \\\\\n",
    "&= \\frac{e^{z_i}\\big(\\sum_j e^{z_j}\\big) - e^{z_i}e^{z_i}}{\\big(\\sum_j e^{z_j}\\big)^2} \\tag{Quotient rule} \\\\\n",
    "&= \\frac{e^{z_i}}{\\sum_j e^{z_j}}  - \\frac{e^{z_i}e^{z_i}}{\\big(\\sum_j e^{z_j}\\big)^2} \\\\\n",
    "&= \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\Big(- \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\Big) \\\\\n",
    "&= y_i (1 - y_i) \\\\\n",
    " \\\\\n",
    "&\\textbf{Case: $i \\neq k$} \\\\\n",
    "\\frac{\\partial y_k}{\\partial z_i} &=\\frac{\\partial}{\\partial z_i} \\Big( \\frac{e^{z_k}}{\\sum_j e^{z_j}} \\Big) \\\\\n",
    "&= e^{z_k} (-1)\\big(\\frac{1}{\\sum_j e^{z_j}}\\big)^2 e^{z_i} \\tag{chain rule}\\\\\n",
    "&= -y_i y_k \\\\\n",
    " \\\\\n",
    " \\\\\n",
    "\\frac{\\partial y_k}{\\partial z_i} &= y_i(\\delta_{ik}-y_k) \\tag{kroneker delta}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
